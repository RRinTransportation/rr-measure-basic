{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Structure of the Dataset Folder\n",
    "```\n",
    "├── journal-meta/\n",
    "│   ├── 0968-090X.csv         # TRC\n",
    "│   ├── mini-dataset.csv      # A mini dataset for tutorial, extracted from TRC\n",
    "│   ├── journal-meta-dataset.csv   # the combined dataset for all journals\n",
    "│   ├── github_data.json      # the links for the GitHub repository\n",
    "│   ├── url_data.json         # the links for the data avaiablity urls\n",
    "│   └── ... (other journal CSV files)\n",
    "├── journal-full-text/\n",
    "│   ├── 0968-090X/\n",
    "│   │   └── 10.1016_j.trc.2023.104311.xml\n",
    "│   └── ... (other DOI folders)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "# replace it with your downloaded folder path\n",
    "full_data_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-full-text'\n",
    "meta_data_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(meta_data_folder, 'full-meta-dataset.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_github'] = 0\n",
    "data['num_of_github_urls'] = 0\n",
    "# create a new json file to store the github data\n",
    "github_data = []\n",
    "for i in range(len(data)):\n",
    "# # In case for the mini test to debug the code\n",
    "# for i in tqdm(range(1000)):\n",
    "    github_urls = []\n",
    "    github_full_urls = []\n",
    "    journal_path = os.path.join(full_data_folder, data['issn'][i])  # Path to the journal folder\n",
    "    paper_path = os.path.join(journal_path, data['unique_id'][i] + '.xml')  # Path to the paper folder\n",
    "    sections = extract_sections_and_text_from_xml(paper_path)\n",
    "    reorganized_sections = postprocess_sections(sections)\n",
    "    for section in reorganized_sections:\n",
    "        urls = extract_github_urls(cleanup(section['text']))\n",
    "        full_urls = extract_full_github_urls(cleanup(section['text']))\n",
    "        if urls:\n",
    "            github_urls.extend(urls)\n",
    "        if full_urls:\n",
    "            github_full_urls.extend(full_urls)\n",
    "        for subsection in section['subsections']:\n",
    "            urls = extract_github_urls(cleanup(subsection['text']))\n",
    "            full_urls = extract_full_github_urls(cleanup(subsection['text']))\n",
    "            if urls:\n",
    "                github_urls.extend(urls)\n",
    "            if full_urls:\n",
    "                github_full_urls.extend(full_urls)\n",
    "            for subsubsection in subsection['subsubsections']:\n",
    "                urls = extract_github_urls(cleanup(subsubsection['text']))\n",
    "                full_urls = extract_full_github_urls(cleanup(subsubsection['text']))\n",
    "                if urls:\n",
    "                    github_urls.extend(urls)\n",
    "                if full_urls:\n",
    "                    github_full_urls.extend(full_urls)\n",
    "    if github_urls:\n",
    "        # Mark as GitHub present\n",
    "        data.loc[i, 'is_github'] = 1\n",
    "        # Remove duplicate URLs and count unique URLs\n",
    "        unique_github_urls = set(github_urls)\n",
    "        print('GitHub URLs found:', unique_github_urls)\n",
    "        unique_github_full_urls = set(github_full_urls)\n",
    "        data.loc[i, 'num_of_github_urls'] = len(unique_github_urls)\n",
    "        github_data.append({\n",
    "            'issn': data['issn'][i],\n",
    "            'unique_id': data['unique_id'][i],\n",
    "            'title': data['title'][i],\n",
    "            'github_urls': list(unique_github_urls),\n",
    "            'github_full_urls': list(unique_github_full_urls)\n",
    "        })\n",
    "# save the json file github_data\n",
    "save_json(github_data, os.path.join(meta_data_folder, 'github_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(meta_data_folder, 'full-meta-dataset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_counts = data.groupby('journal_name')['is_github'].value_counts()\n",
    "# grouped_counts\n",
    "grouped_counts = grouped_counts.reset_index(name='count')\n",
    "# Group by journal_name and calculate the sum of counts for each group\n",
    "grouped_counts['percentage'] = (\n",
    "    grouped_counts['count'] / grouped_counts.groupby('journal_name')['count'].transform('sum')\n",
    ") * 100\n",
    "# Display the updated DataFrame\n",
    "grouped_counts[grouped_counts['is_github'] == 1].sort_values('percentage', ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RR-measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
