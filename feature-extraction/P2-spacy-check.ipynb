{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Structure of the Dataset Folder\n",
    "```\n",
    "├── journal-meta/\n",
    "│   ├── 0968-090X.csv         # TRC\n",
    "│   ├── mini-dataset.csv      # A mini dataset for tutorial, extracted from TRC\n",
    "│   └── ... (other journal CSV files)\n",
    "├── journal-full-text/\n",
    "│   ├── 0968-090X/\n",
    "│   │   └── 10.1016_j.trc.2023.104311.xml\n",
    "│   └── ... (other DOI folders)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "full_data_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-full-text'\n",
    "meta_data_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor journal in journals:\\n    if journal == 'mini-dataset.csv': # to take the mini dataset for tutorial\\n        # connect the journal name with the issn from the journal_issn_df\\n        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\\n        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\\n        journal_meta['issn'] = journal_issn\\n        journal_meta['abstract'] = journal_meta['abstract'].apply(cleanup_abstract) # to clean up the abstract\\n        journal_meta.to_csv(os.path.join(meta_folder, journal), index=False) # at the end, save the cleaned dataset\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This section demonstrates how to work with the dataset, \n",
    "# utilizing the dataframe's apply method for efficient iteration in a loop.\n",
    "# An example here is to clean up the abstract.\n",
    "def cleanup_abstract(abstract):\n",
    "    \"\"\"\n",
    "    Cleans up an abstract string by standardizing spacing.\n",
    "\n",
    "    Args:\n",
    "        abstract (str): The abstract of a journal article, which may contain irregular spacing,\n",
    "                        including multiple spaces, leading spaces, or trailing spaces.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned string where all excessive spaces are replaced with a single space,\n",
    "             and any leading or trailing spaces are removed. This is essential for preparing\n",
    "             text data for further analysis or display, ensuring uniformity in the formatting\n",
    "             of abstracts.\n",
    "\n",
    "    Example:\n",
    "        >>> cleanup_abstract(\"  This  is   an example   abstract.  \")\n",
    "        'This is an example abstract.'\n",
    "    \"\"\"\n",
    "    # Check if the input is a string\n",
    "    if not isinstance(abstract, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    \n",
    "    return re.sub(r'\\s+', ' ', abstract).strip()\n",
    "'''\n",
    "for journal in journals:\n",
    "    if journal == 'mini-dataset.csv': # to take the mini dataset for tutorial\n",
    "        # connect the journal name with the issn from the journal_issn_df\n",
    "        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['issn'] = journal_issn\n",
    "        journal_meta['abstract'] = journal_meta['abstract'].apply(cleanup_abstract) # to clean up the abstract\n",
    "        journal_meta.to_csv(os.path.join(meta_folder, journal), index=False) # at the end, save the cleaned dataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section demostrates how to link the doi with the full text\n",
    "def doi_to_unique_id(doi):\n",
    "    \"\"\"\n",
    "    Converts a DOI to a unique identifier by replacing slashes with underscores.\n",
    "\n",
    "    Args:\n",
    "        doi (str): The DOI of a journal article.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique identifier where slashes are replaced with underscores.\n",
    "\n",
    "    Example:\n",
    "        >>> doi_to_unique_id(\"10.1016/j.trc.2023.104311\")\n",
    "        \"10.1016_j.trc.2023_104311\"\n",
    "    \"\"\"\n",
    "    return doi.replace('/', '_')\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_sections_and_text_from_xml(file_path):\n",
    "    \"\"\"\n",
    "    Extracts sections and text from an XML file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "    \n",
    "    Example:\n",
    "        >>> extract_sections_and_text_from_xml('/path/to/file.xml')\n",
    "        [{'label': '1', 'title': 'Introduction', 'text': 'This is the introduction...', 'subsections': []}]\n",
    "    \"\"\"\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Namespace to handle XML namespaces\n",
    "    namespaces = {\n",
    "        'xocs': 'http://www.elsevier.com/xml/xocs/dtd',\n",
    "        'ce': 'http://www.elsevier.com/xml/common/dtd',\n",
    "        'ja': 'http://www.elsevier.com/xml/ja/dtd',\n",
    "        'mml': 'http://www.w3.org/1998/Math/MathML'\n",
    "    }\n",
    "\n",
    "    # Extracting the sections using the item-toc element\n",
    "    sections = []\n",
    "    for item in root.findall('.//xocs:item-toc-entry', namespaces):\n",
    "        section_title = item.find('xocs:item-toc-section-title', namespaces)\n",
    "        section_label = item.find('xocs:item-toc-label', namespaces)\n",
    "        section_text = []\n",
    "        \n",
    "        # Use the section label to find the corresponding section id in <ce:section>\n",
    "        if section_label is not None and section_title is not None:\n",
    "            label_text = section_label.text.strip()\n",
    "            section_elem = root.find(f\".//ce:section[ce:label='{label_text}']\", namespaces)\n",
    "            if section_elem is not None:\n",
    "                # Get all text under the section element, including paragraphs and other texts\n",
    "                section_text_parts = []\n",
    "                subsections = []\n",
    "                before_subsection_text = True\n",
    "\n",
    "                # Iterate over all elements within the section\n",
    "                for elem in section_elem:\n",
    "                    # Check if this element is a subsection\n",
    "                    if elem.tag == f\"{{{namespaces['ce']}}}section\":\n",
    "                        # This is a subsection, process it\n",
    "                        subsection_title_elem = elem.find(f\"ce:section-title\", namespaces)\n",
    "                        if subsection_title_elem is not None:\n",
    "                            subsection_title = subsection_title_elem.text\n",
    "                            subsection_paragraphs = []\n",
    "                            subsubsections = []\n",
    "                            \n",
    "                            for sub_elem in elem:\n",
    "                                # If this is a paragraph, append text\n",
    "                                if sub_elem.tag == f\"{{{namespaces['ce']}}}para\":\n",
    "                                    paragraph_text = ''.join(sub_elem.itertext())\n",
    "                                    subsection_paragraphs.append(paragraph_text)\n",
    "                                \n",
    "                                # If this is a sub-subsection, process it\n",
    "                                elif sub_elem.tag == f\"{{{namespaces['ce']}}}section\":\n",
    "                                    subsubsection_title_elem = sub_elem.find(f\"ce:section-title\", namespaces)\n",
    "                                    if subsubsection_title_elem is not None:\n",
    "                                        subsubsection_title = subsubsection_title_elem.text\n",
    "                                        subsubsection_paragraphs = []\n",
    "                                        for subsub_elem in sub_elem.findall('ce:para', namespaces=namespaces):\n",
    "                                            paragraph_text = ''.join(subsub_elem.itertext())\n",
    "                                            subsubsection_paragraphs.append(paragraph_text)\n",
    "                                        subsubsection_text = ' '.join(subsubsection_paragraphs)\n",
    "                                        subsubsections.append({\n",
    "                                            \"label\": sub_elem.find(f\"ce:label\", namespaces).text if sub_elem.find(f\"ce:label\", namespaces) is not None else \"\",\n",
    "                                            \"title\": subsubsection_title,\n",
    "                                            \"text\": subsubsection_text\n",
    "                                        })\n",
    "                            \n",
    "                            subsection_text = ' '.join(subsection_paragraphs)\n",
    "                            subsections.append({\n",
    "                                \"label\": elem.find(f\"ce:label\", namespaces).text if elem.find(f\"ce:label\", namespaces) is not None else \"\",\n",
    "                                \"title\": subsection_title,\n",
    "                                \"text\": subsection_text,\n",
    "                                \"subsubsections\": subsubsections\n",
    "                            })\n",
    "                    else:\n",
    "                        # Collect text before any subsection starts\n",
    "                        if before_subsection_text and elem.tag == f\"{{{namespaces['ce']}}}para\":\n",
    "                            paragraph_text = ''.join(elem.itertext())\n",
    "                            section_text_parts.append(paragraph_text)\n",
    "\n",
    "                section_text = ' '.join(section_text_parts)\n",
    "                \n",
    "                sections.append({\n",
    "                    \"label\": section_label.text,\n",
    "                    \"title\": section_title.text,\n",
    "                    \"text\": section_text,\n",
    "                    \"subsections\": subsections\n",
    "                })\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Function to postprocess sections, subsections, and subsubsections\n",
    "def postprocess_sections(data):\n",
    "    \"\"\"\n",
    "    Postprocesses sections, subsections, and subsubsections by removing duplicate labels and ensuring unique content.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "\n",
    "    Example:\n",
    "        >>> reorganized_sections = postprocess_sections(sections)\n",
    "        # Save the reorganized sections to a JSON file\n",
    "        import json\n",
    "        # Define the file path for the output\n",
    "        output_file_path = '../example.json'\n",
    "        \n",
    "        # Open the file in write mode and dump the data\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            json.dump(reorganized_sections, file, indent=4)  # Added indentation for better readability\n",
    "        \n",
    "        for section in reorganized_sections:\n",
    "            print(section['label'], section['title'])\n",
    "            for subsection in section['subsections']:\n",
    "                print(\"    \", subsection['label'], subsection['title'])\n",
    "                for subsubsection in subsection['subsubsections']:\n",
    "                    print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "        # if you want to see the original sections, subsections, and subsubsections, you can use the following code\n",
    "        # for section in sections:\n",
    "        #     print(section['label'], section['title'])\n",
    "        #     for subsection in section['subsections']:\n",
    "        #         print(\"    \", subsection['label'], subsection['title'])\n",
    "        #         for subsubsection in subsection['subsubsections']:\n",
    "        #             print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "    \"\"\"\n",
    "    reorganized_data = []\n",
    "    \n",
    "    labels_to_remove = set()\n",
    "    \n",
    "    for section in data:\n",
    "        # Skip if the section is marked for removal\n",
    "        if section[\"label\"] in labels_to_remove:\n",
    "            continue\n",
    "        \n",
    "        new_section = {\n",
    "            \"label\": section[\"label\"],\n",
    "            \"title\": section[\"title\"],\n",
    "            \"text\": section[\"text\"],\n",
    "            \"subsections\": []\n",
    "        }\n",
    "        \n",
    "        # Iterate through subsections to reorganize them\n",
    "        for subsection in data:\n",
    "            # Check if the subsection label starts with the section label and follows the x.x format\n",
    "            if subsection[\"label\"].startswith(section[\"label\"] + \".\") and len(subsection[\"label\"].split('.')) == 2:\n",
    "                new_subsection = {\n",
    "                    \"label\": subsection[\"label\"],\n",
    "                    \"title\": subsection[\"title\"],\n",
    "                    \"text\": subsection[\"text\"],\n",
    "                    \"subsubsections\": []\n",
    "                }\n",
    "                labels_to_remove.add(subsection[\"label\"])\n",
    "                \n",
    "                # Iterate through subsubsections to reorganize them under the appropriate subsection\n",
    "                for subsubsection in data:\n",
    "                    if subsubsection[\"label\"].startswith(new_subsection[\"label\"] + \".\"):\n",
    "                        new_subsubsection = {\n",
    "                            \"label\": subsubsection[\"label\"],\n",
    "                            \"title\": subsubsection[\"title\"],\n",
    "                            \"text\": subsubsection[\"text\"]\n",
    "                        }\n",
    "                        labels_to_remove.add(subsubsection[\"label\"])\n",
    "                        new_subsection[\"subsubsections\"].append(new_subsubsection)\n",
    "                \n",
    "                # Add the subsection only if it is unique or has no subsubsections\n",
    "                if new_subsection[\"subsubsections\"]:\n",
    "                    # If subsubsections exist, avoid duplicate content\n",
    "                    new_subsection[\"text\"] = \"\"\n",
    "                new_section[\"subsections\"].append(new_subsection)\n",
    "        \n",
    "        reorganized_data.append(new_section)\n",
    "    \n",
    "    return reorganized_data\n",
    "\n",
    "# search the \"github.com\" across all the text in all the sections, subsections, and subsubsections\n",
    "# and extract the full github url, like https://github.com/username/repository\n",
    "def extract_github_urls(text):\n",
    "    # Regular expression to match GitHub URLs exactly without trailing directories\n",
    "    github_url_pattern = r\"https?://github\\.com/[\\w-]+/[\\w-]+(?!/\\S)\"\n",
    "    \n",
    "    # Find all matching GitHub URLs in the text\n",
    "    github_urls = re.findall(github_url_pattern, text)\n",
    "    \n",
    "    return github_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reorganized_sections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 31\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mreorganized_sections\u001b[49m, file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reorganized_sections' is not defined"
     ]
    }
   ],
   "source": [
    "for journal in journals:\n",
    "    if journal == 'mini-dataset.csv': # to take the mini dataset for tutorial\n",
    "        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['unique_id'] = journal_meta['doi'].apply(doi_to_unique_id) # to convert the doi to a unique id\n",
    "        # here we use the for loop to help understand how it works, it can be done in one line of code later\n",
    "        for i in range(len(journal_meta)):\n",
    "            github_urls = []\n",
    "            fulltext_path = os.path.join(full_text_folder, journal_issn, journal_meta.iloc[i]['unique_id'] + '.xml')\n",
    "            sections = extract_sections_and_text_from_xml(fulltext_path)\n",
    "            reorganized_sections = postprocess_sections(sections)\n",
    "            for section in reorganized_sections:\n",
    "                # add a preprcessing for the text here to make it more readable\n",
    "                urls  = extract_github_urls(cleanup_abstract(section['text']))\n",
    "                if urls:\n",
    "                    github_urls.extend(urls)\n",
    "                for subsection in section['subsections']:\n",
    "                    urls = github_urls.extend(extract_github_urls(cleanup_abstract(subsection['text'])))\n",
    "                    if urls:\n",
    "                        github_urls.extend(urls)\n",
    "                    for subsubsection in subsection['subsubsections']:\n",
    "                        urls = extract_github_urls(cleanup_abstract(subsubsection['text']))\n",
    "                        if urls:\n",
    "                            github_urls.extend(urls)\n",
    "            if github_urls:\n",
    "                print(github_urls)\n",
    "                \n",
    "# write the reorganized sections to a json file\n",
    "import json\n",
    "with open('example.json', 'w') as file:\n",
    "    json.dump(reorganized_sections, file, indent=4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "2 Related studies\n",
      "     2.1 Data-driven traffic prediction\n",
      "     2.2 Model-based traffic prediction\n",
      "     2.3 Hybrid traffic prediction\n",
      "3 The proposed model\n",
      "     3.1 Model framework\n",
      "     3.2 Data generation module\n",
      "     3.3 Event detection module\n",
      "         3.3.1 Global standard error range\n",
      "     3.4 Traffic prediction module\n",
      "         3.4.1 Sub-module 1: Base predictor using historical data\n",
      "4 Experiments\n",
      "     4.1 Study area\n",
      "     4.2 Dataset construction for traffic prediction\n",
      "         4.2.1 Historical traffic data\n",
      "     4.3 Prediction setting\n",
      "         4.3.1 Base predictor of sub-module 1\n",
      "5 Experimental results\n",
      "     5.1 Evaluation measurements\n",
      "     5.2 Experiment analysis\n",
      "6 Conclusion\n"
     ]
    }
   ],
   "source": [
    "for section in reorganized_sections:\n",
    "    print(section['label'], section['title'])\n",
    "    # print(cleanup_abstract(section['text']))\n",
    "    # print('--------------------------------')\n",
    "    for subsection in section['subsections']:\n",
    "        print(\"    \", subsection['label'], subsection['title'])\n",
    "        # print(cleanup_abstract(subsection['text']))\n",
    "        # print('--------------------------------')\n",
    "        for subsubsection in subsection['subsubsections']:\n",
    "            print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "            # print(cleanup_abstract(subsubsection['text']))\n",
    "            # print('--------------------------------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for journal in journals:\n",
    "    if journal == '0968-090X.csv': # to take the trc dataset for test\n",
    "        journal_issn = journal.replace('.csv', '')\n",
    "        # journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['unique_id'] = journal_meta['doi'].apply(doi_to_unique_id) # to convert the doi to a unique id\n",
    "        # here we use the for loop to help understand how it works, it can be done in one line of code later\n",
    "        count = 0\n",
    "        for i in range(len(journal_meta)):\n",
    "            github_urls = []\n",
    "            fulltext_path = os.path.join(full_text_folder, journal_issn, journal_meta.iloc[i]['unique_id'] + '.xml')\n",
    "            sections = extract_sections_and_text_from_xml(fulltext_path)\n",
    "            reorganized_sections = postprocess_sections(sections)\n",
    "            for section in reorganized_sections:\n",
    "                # add a preprcessing for the text here to make it more readable\n",
    "                urls  = extract_github_urls(cleanup_abstract(section['text']))\n",
    "                if urls:\n",
    "                    github_urls.extend(urls)\n",
    "                for subsection in section['subsections']:\n",
    "                    urls = github_urls.extend(extract_github_urls(cleanup_abstract(subsection['text'])))\n",
    "                    if urls:\n",
    "                        github_urls.extend(urls)\n",
    "                    for subsubsection in subsection['subsubsections']:\n",
    "                        urls = extract_github_urls(cleanup_abstract(subsubsection['text']))\n",
    "                        if urls:\n",
    "                            github_urls.extend(urls)\n",
    "            if github_urls:\n",
    "                count += 1\n",
    "                print(github_urls)\n",
    "        print(f'{count}/{len(journal_meta)}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BELKESSA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\BELKESSA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"attribute_ruler\"])\n",
    "\n",
    "\n",
    "def doi_to_unique_id(doi):\n",
    "    \"\"\"Convert DOI to a unique identifier.\"\"\"\n",
    "    return doi.replace('/', '_')\n",
    "\n",
    "\n",
    "# Function to classify data acquisition type\n",
    "def classify_acquisition_type(text):\n",
    "    patterns = {\n",
    "        \"simulated\": r\"(simulation|simulated|model-based|virtual|synthetic data|emulated|agent-based modeling|\"\n",
    "                     r\"traffic simulation|transport simulation|micro-simulation|macro-simulation|meso-simulation|\"\n",
    "                     r\"behavioral modeling|route choice modeling|demand forecasting simulation|synthetic scenarios|GAN-generated)\",\n",
    "        \"generated\": r\"(generated|created|constructed|fabricated|synthesized|computer-generated|\"\n",
    "                     r\"algorithm-generated|machine-generated|data augmentation|automatically produced|\"\n",
    "                     r\"stochastic modeling|synthetic generation|trained on synthetic|augmented data)\",\n",
    "        \"existing\": r\"(existing|archived|previously collected|third-party|external|publicly available|real-world|\"\n",
    "                    r\"historical data|field data|empirical data|open datasets|government data|observational data|\"\n",
    "                    r\"collected from sensors|IoT data|crowdsourced data|published data|survey results|\"\n",
    "                    r\"remote sensing|satellite data|real-time data)\"\n",
    "    }\n",
    "    for acquisition_type, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return acquisition_type\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Function to classify data access type\n",
    "def classify_access_type(text):\n",
    "    patterns = {\n",
    "        \"open source\": r\"(open\\s*source|publicly\\s*available|shared\\s*freely|free\\s*access|freely\\s*accessible|\"\n",
    "                       r\"unrestricted access|hosted on GitHub|open repository|downloadable datasets)\",\n",
    "        \"proprietary\": r\"(proprietary|restricted|commercial|license required|not open|private|subscription|confidential|\"\n",
    "                       r\"requires authorization|data agreement|exclusive access)\",\n",
    "        \"third-party\": r\"(third-party|external source|external provider|sourced externally|purchased data|licensed from|\"\n",
    "                        r\"collaborative data|shared by partners|data obtained from agencies|\"\n",
    "                        r\"industry-provided data|crowdsourced)\"\n",
    "    }\n",
    "    for access_type, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return access_type\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Function to classify data type\n",
    "def classify_data_type(text):\n",
    "    patterns = {\n",
    "        \"open source\": r\"(open\\s*source|publicly\\s*available|shared\\s*freely|free\\s*access|freely\\s*accessible|\"\n",
    "                       r\"open\\s*data|free\\s*to\\s*use|government\\s*open\\s*data|unrestricted\\s*access|\"\n",
    "                       r\"hosted\\s*on\\s*platforms\\s*like\\s*GitHub|open\\s*repository|\"\n",
    "                       r\"downloadable\\s*without\\s*restrictions|free\\s*and\\s*open\\s*datasets)\",\n",
    "        \"proprietary\": r\"(proprietary|restricted|commercial|license\\s*required|not\\s*open|private|confidential|\"\n",
    "                       r\"internal\\s*use\\s*only|data\\s*for\\s*purchase|requires\\s*payment|subscription\\s*required|\"\n",
    "                       r\"restricted\\s*access|non-disclosure\\s*agreement|paid\\s*dataset|for\\s*institutional\\s*use)\",\n",
    "        \"third-party\": r\"(third-party|external\\s*source|external\\s*provider|sourced\\s*externally|\"\n",
    "                        r\"licensed\\s*from\\s*another\\s*organization|collaborative\\s*data|\"\n",
    "                        r\"acquired\\s*from\\s*partners|data\\s*from\\s*vendors|industry-provided\\s*data|\"\n",
    "                        r\"data\\s*obtained\\s*from\\s*other\\s*agencies|crowdsourced\\s*data|shared\\s*by\\s*research\\s*partners)\"\n",
    "    }\n",
    "    for data_type, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return data_type\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to classify license type\n",
    "def classify_license_type(text):\n",
    "    patterns = {\n",
    "        \"Creative Commons\": r\"(Creative\\s*Commons|CC\\s*BY|CC\\s*BY-SA|CC\\s*BY-NC|CC\\s*BY-ND|CC\\s*BY-NC-SA|\"\n",
    "                            r\"CC\\s*BY-NC-ND|Creative\\s*Commons\\s*Zero|CC0|CC\\s*Public\\s*Domain\\s*Dedication)\",\n",
    "        \"MIT\": r\"(MIT\\s*license|Massachusetts\\s*Institute\\s*of\\s*Technology\\s*license)\",\n",
    "        \"Apache\": r\"(Apache\\s*license|Apache\\s*2\\.0|Apache\\s*Software\\s*License)\",\n",
    "        \"GNU\": r\"(GNU\\s*GPL|General\\s*Public\\s*License|LGPL|Affero\\s*GPL|GNU\\s*AGPL|GPLv[23]|GNU\\s*Lesser\\s*GPL)\",\n",
    "        \"BSD\": r\"(BSD\\s*license|Berkeley\\s*Software\\s*Distribution|BSD-2-Clause|BSD-3-Clause|BSD\\s*Zero-Clause)\",\n",
    "        \"public domain\": r\"(public\\s*domain|no\\s*restrictions|publicly\\s*available|dedicated\\s*to\\s*the\\s*public\\s*domain|\"\n",
    "                          r\"free\\s*for\\s*use|no\\s*rights\\s*reserved|unrestricted\\s*use)\",\n",
    "        \"proprietary\": r\"(proprietary|restricted|all\\s*rights\\s*reserved|exclusive\\s*rights|not\\s*for\\s*redistribution|\"\n",
    "                       r\"internal\\s*use\\s*only|closed-source|private\\s*license|requires\\s*authorization|limited\\s*license)\",\n",
    "        \"custom\": r\"(custom\\s*license|tailored\\s*license|unique\\s*licensing\\s*terms|organization-specific\\s*license|\"\n",
    "                  r\"bespoke\\s*license\\s*terms|institutional\\s*license)\"\n",
    "    }\n",
    "    for license_type, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return license_type\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Function to check relevant sections\n",
    "def is_relevant_section(title):\n",
    "    \"\"\"\n",
    "    Determine if a section is relevant based on its title.\n",
    "    \"\"\"\n",
    "    if not title:  # Handle None or empty title\n",
    "        return False\n",
    "\n",
    "    relevant_keywords = [\n",
    "        \"abstract\", \"introduction\", \"background\", \"overview\",\n",
    "        \"data\", \"dataset\", \"data source\", \"data description\", \"data collection\",\n",
    "        \"method\", \"methodology\", \"approach\", \"experimental design\", \"procedures\",\n",
    "        \"results\", \"findings\", \"outcomes\", \"analysis\", \"discussion\",\n",
    "        \"license\", \"licensing\", \"copyright\", \"terms of use\", \"data policy\",\n",
    "        \"simulation\", \"modeling\", \"synthetic data\", \"generated data\",\n",
    "        \"reproducibility\", \"replication\", \"validation\", \"verification\",\n",
    "        \"tools\", \"frameworks\", \"software\", \"code\", \"implementation\",\n",
    "        \"availability\", \"resources\", \"access\", \"sharing\", \"open source\",\n",
    "        \"conclusion\", \"future work\", \"limitations\", \"summary\"\n",
    "    ]\n",
    "    return any(keyword in title.lower() for keyword in relevant_keywords)\n",
    "\n",
    "def extract_dataset_links(text):\n",
    "    \"\"\"\n",
    "    Extract dataset links from text using regex.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text content to process.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique dataset links.\n",
    "    \"\"\"\n",
    "    dataset_link_pattern = r\"(https?://(?:www\\.)?(?:[\\w-]+\\.)+(?:com|org|edu|io|gov|net)/[^\\s]+(?:\\.csv|\\.json|\\.xlsx|\\.txt|\\.zip|\\.tar|\\.h5|))\"\n",
    "    dataset_links = re.findall(dataset_link_pattern, text, re.IGNORECASE)\n",
    "    return list(set(dataset_links))  # Remove duplicates\n",
    "\n",
    "\n",
    "def extract_dataset_names(text):\n",
    "    \"\"\"\n",
    "    Extract dataset names from text using regex and spaCy NER.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text content to process.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique dataset names.\n",
    "    \"\"\"\n",
    "    dataset_name_patterns = [\n",
    "        r\"dataset\\s*(?:named|called|known as|referred to as|is titled|entitled|used in this study as|termed)\\s*[:\\-]?\\s*\\\"?(.*?)\\\"?\\.?\",\n",
    "        r\"(?:named|titled|referred to as|termed|entitled|used as)\\s*\\\"(.*?)\\\"\",\n",
    "        r\"(?:the dataset|data)\\s*(?:is|was|has been)\\s*(?:called|named|referred to as|known as|entitled|termed)\\s*\\\"?(.*?)\\\"?\\.?\",\n",
    "        r\"(?:dataset|data)\\s*(?:from|of|for)\\s*(.*?)\\s*(?:was|is|were|has been|collected)\",\n",
    "        r\"datasets?\\s*(?:used|analyzed|provided|sourced from|developed)\\s*(.*?)\\.\"\n",
    "    ]\n",
    "\n",
    "    dataset_names = []\n",
    "    for pattern in dataset_name_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        dataset_names.extend(matches)\n",
    "\n",
    "    # Use spaCy NER for additional extraction\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PRODUCT\", \"WORK_OF_ART\", \"ORG\"} and \"dataset\" in ent.text.lower():\n",
    "            dataset_names.append(ent.text.strip())\n",
    "\n",
    "    # Remove duplicates and clean trailing punctuations\n",
    "    return list(set([name.strip().rstrip(\".\") for name in dataset_names]))\n",
    "\n",
    "\n",
    "\n",
    "# Main extraction function\n",
    "def extract_data_details_with_spacy(text):\n",
    "    \"\"\"Extract data-related information from text.\"\"\"\n",
    "    details = {\n",
    "        \"acquisition_type\": classify_acquisition_type(text),\n",
    "        \"access_type\": classify_access_type(text),\n",
    "        \"data_type\": classify_data_type(text),\n",
    "        \"license\": classify_license_type(text),\n",
    "    }\n",
    "\n",
    "    # Regex for dataset link and name\n",
    "    dataset_link_pattern = r\"(https?://[^\\s]+|www\\.[^\\s]+|ftp://[^\\s]+|doi\\.org/[^\\s]+)\"\n",
    "    dataset_name_pattern = r\"dataset\\s*(?:named|called|known as|referred to as)\\s*[:\\-]?\\s*\\\"?(.*?)\\\"?\\.\"\n",
    "\n",
    "    # Extract dataset link\n",
    "    dataset_link_match = re.search(dataset_link_pattern, text, re.IGNORECASE)\n",
    "    if dataset_link_match:\n",
    "        details[\"dataset_link\"] = dataset_link_match.group(1)\n",
    "\n",
    "    # Extract dataset name\n",
    "    dataset_name_match = re.search(dataset_name_pattern, text, re.IGNORECASE)\n",
    "    if dataset_name_match:\n",
    "        details[\"dataset_name\"] = dataset_name_match.group(1)\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "# Process the mini-dataset\n",
    "journals = ['mini-dataset.csv']  # Example journal\n",
    "meta_folder = '../'\n",
    "full_text_folder = '../journal-full-text'\n",
    "journal_issn_df = pd.DataFrame({'journal': ['mini-dataset'], 'issn': ['0968-090X']})  # Example ISSN DataFrame\n",
    "\n",
    "detailed_info = []\n",
    "\n",
    "for journal in journals:\n",
    "    if journal == 'mini-dataset.csv':  # Adjust for larger datasets\n",
    "        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['unique_id'] = journal_meta['doi'].apply(doi_to_unique_id)\n",
    "\n",
    "        for i in range(len(journal_meta)):\n",
    "            fulltext_path = os.path.join(full_text_folder, journal_issn, journal_meta.iloc[i]['unique_id'] + '.xml')\n",
    "            if not os.path.exists(fulltext_path):\n",
    "                continue\n",
    "\n",
    "            sections = extract_sections_and_text_from_xml(fulltext_path)\n",
    "            reorganized_sections = postprocess_sections(sections)\n",
    "\n",
    "            paper_info = {\"doi\": journal_meta.iloc[i]['doi']}\n",
    "            for section in reorganized_sections:\n",
    "                if is_relevant_section(section.get('title', '')):\n",
    "                    data_details = extract_data_details_with_spacy(section['text'])\n",
    "                    paper_info.update(data_details)\n",
    "\n",
    "            detailed_info.append(paper_info)\n",
    "\n",
    "# Save results\n",
    "detailed_info_df = pd.DataFrame(detailed_info)\n",
    "detailed_info_df.to_csv('data_details.csv', index=False)\n",
    "\n",
    "# Summarize results\n",
    "def summarize_results(df):\n",
    "    summary = {}\n",
    "    total_papers = len(df)\n",
    "    for column in df.columns:\n",
    "        if column == \"doi\":\n",
    "            continue\n",
    "        value_counts = df[column].value_counts()\n",
    "        percentages = (value_counts / total_papers * 100).round(2)\n",
    "        summary[column] = pd.DataFrame({\"Count\": value_counts, \"Percentage (%)\": percentages})\n",
    "    return summary\n",
    "\n",
    "\n",
    "summary_tables = summarize_results(detailed_info_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>acquisition_type</th>\n",
       "      <th>access_type</th>\n",
       "      <th>data_type</th>\n",
       "      <th>license</th>\n",
       "      <th>dataset_link</th>\n",
       "      <th>dataset_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1016/j.trc.2023.104451</td>\n",
       "      <td>simulated</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.trc.2023.104349</td>\n",
       "      <td>simulated</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.trc.2023.104427</td>\n",
       "      <td>existing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1016/j.trc.2023.104453</td>\n",
       "      <td>simulated</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.trc.2023.104459</td>\n",
       "      <td>generated</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10.1016/j.trc.2023.104458</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10.1016/j.trc.2024.104499</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10.1016/j.trc.2024.104496</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://doi.org/10.1016/j.trc.2024.104496.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10.1016/j.trc.2024.104494</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10.1016/j.trc.2024.104490</td>\n",
       "      <td>simulated</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          doi acquisition_type access_type data_type license  \\\n",
       "0   10.1016/j.trc.2023.104451        simulated        None      None    None   \n",
       "1   10.1016/j.trc.2023.104349        simulated        None      None    None   \n",
       "2   10.1016/j.trc.2023.104427         existing        None      None    None   \n",
       "3   10.1016/j.trc.2023.104453        simulated        None      None    None   \n",
       "4   10.1016/j.trc.2023.104459        generated        None      None    None   \n",
       "..                        ...              ...         ...       ...     ...   \n",
       "95  10.1016/j.trc.2023.104458             None        None      None    None   \n",
       "96  10.1016/j.trc.2024.104499             None        None      None    None   \n",
       "97  10.1016/j.trc.2024.104496             None        None      None    None   \n",
       "98  10.1016/j.trc.2024.104494             None        None      None    None   \n",
       "99  10.1016/j.trc.2024.104490        simulated        None      None    None   \n",
       "\n",
       "                                  dataset_link dataset_name  \n",
       "0                                          NaN          NaN  \n",
       "1                                          NaN          NaN  \n",
       "2                                          NaN          NaN  \n",
       "3                                          NaN          NaN  \n",
       "4                                          NaN          NaN  \n",
       "..                                         ...          ...  \n",
       "95                                         NaN          NaN  \n",
       "96                                         NaN          NaN  \n",
       "97  https://doi.org/10.1016/j.trc.2024.104496.          NaN  \n",
       "98                                         NaN          NaN  \n",
       "99                                         NaN          NaN  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for acquisition_type:\n",
      "                  Count  Percentage (%)\n",
      "acquisition_type                       \n",
      "existing             29            29.0\n",
      "simulated            21            21.0\n",
      "generated            12            12.0\n",
      "\n",
      "Summary for access_type:\n",
      "             Count  Percentage (%)\n",
      "access_type                       \n",
      "proprietary     11            11.0\n",
      "third-party      2             2.0\n",
      "open source      1             1.0\n",
      "\n",
      "Summary for data_type:\n",
      "             Count  Percentage (%)\n",
      "data_type                         \n",
      "proprietary     11            11.0\n",
      "open source      1             1.0\n",
      "third-party      1             1.0\n",
      "\n",
      "Summary for license:\n",
      "               Count  Percentage (%)\n",
      "license                             \n",
      "proprietary        2             2.0\n",
      "public domain      1             1.0\n",
      "\n",
      "Summary for dataset_link:\n",
      "                                                    Count  Percentage (%)\n",
      "dataset_link                                                             \n",
      "https://en.wikipedia.org/wiki/Seoul_Halloween_c...      1             1.0\n",
      "https://github.com/pabloguarda/pesuelogit.              1             1.0\n",
      "https://developers.google.com/optimization/)            1             1.0\n",
      "https://github.com/zhandongxu/GP_RTAP.                  1             1.0\n",
      "https://doi.org/10.1016/j.trc.2023.104470.              1             1.0\n",
      "https://doi.org/10.1016/j.trc.2023.104479.              1             1.0\n",
      "https://arb.ca.gov/emfac/.                              1             1.0\n",
      "https://www.sae.org/standards                           1             1.0\n",
      "https://doi.org/10.1016/j.trc.2024.104528.              1             1.0\n",
      "https://github.com/HDDL/DPRDDM.                         1             1.0\n",
      "https://doi.org/10.1016/j.trc.2024.104496.              1             1.0\n",
      "\n",
      "Summary for dataset_name:\n",
      "                                                    Count  Percentage (%)\n",
      "dataset_name                                                             \n",
      "pNeuma, which is a high-resolution vehicle traj...      1             1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column, summary_df in summary_tables.items():\n",
    "    print(f\"Summary for {column}:\\n{summary_df}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RR-measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
