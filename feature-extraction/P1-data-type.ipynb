{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Structure of the Dataset Folder\n",
    "```\n",
    "├── journal-meta/\n",
    "│   ├── 0968-090X.csv         # TRC\n",
    "│   ├── mini-dataset.csv      # A mini dataset for tutorial, extracted from TRC\n",
    "│   ├── journal-meta-dataset.csv   # the combined dataset for all journals\n",
    "│   ├── github_data.json      # the links for the GitHub repository\n",
    "│   ├── url_data.json         # the links for the data avaiablity urls\n",
    "│   └── ... (other journal CSV files)\n",
    "├── journal-full-text/\n",
    "│   ├── 0968-090X/\n",
    "│   │   └── 10.1016_j.trc.2023.104311.xml\n",
    "│   └── ... (other DOI folders)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "# replace it with your downloaded folder path\n",
    "full_data_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-full-text'\n",
    "meta_data_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-meta'\n",
    "result_data_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'doi', 'volume', 'date', 'year', 'month', 'abstract', 'issn',\n",
       "       'journal_name', 'unique_id', 'is_github', 'num_of_github_urls',\n",
       "       'is_availablity_statement', 'is_data_mentioned_in_section_title',\n",
       "       'is_experiment_mentioned_in_section_title',\n",
       "       'is_link_in_avaiablity_statement',\n",
       "       'num_of_links_in_avaiablity_statement'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(meta_data_folder, 'full-meta-dataset.csv'))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_availability_statement'] = 0\n",
    "for i in range(len(data)):\n",
    "# # In case for the mini test to debug the code\n",
    "# for i in tqdm(range(1000)):\n",
    "    journal_path = os.path.join(full_data_folder, data['issn'][i])  # Path to the journal folder\n",
    "    paper_path = os.path.join(journal_path, data['unique_id'][i] + '.xml')  # Path to the paper folder\n",
    "    sections = extract_sections_and_text_from_xml(paper_path)\n",
    "    reorganized_sections = postprocess_sections(sections)\n",
    "    for section in reorganized_sections:\n",
    "        section_title = section['title']\n",
    "        if 'data availability' in section_title.lower():\n",
    "            data.loc[i, 'is_availability_statement'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import yaml\n",
    "import json\n",
    "# Load API key from config.yaml\n",
    "from openai import OpenAI\n",
    "with open(\"/Users/junyi/Work/RR/config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "openai.api_key = config[\"openai_api_key\"]\n",
    "client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "def analyze_with_openai(data_context):\n",
    "    definition_context = \"\"\"\n",
    "                        ---------------------------\n",
    "                        Definition of data source:\n",
    "                        Real-world data is the data is collected from the real-world, such as data from sensors, surveys, or other sources.\n",
    "                        Simulation data is the data generated from simulation or synthetic data, even though the scenario is based on real-world.\n",
    "                        \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": data_context + definition_context\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"data_source_description\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"source_description\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Description of the data source.\"\n",
    "                        },\n",
    "                        \"real_world\": {\n",
    "                            \"type\": \"boolean\",\n",
    "                            \"description\": \"Is the data collected from real-world based on the definition?\"\n",
    "                        },\n",
    "                        \"simulation\": {\n",
    "                            \"type\": \"boolean\",\n",
    "                            \"description\": \"Is the data collected from simulation or synthetic data based on the definition?\"\n",
    "                        },\n",
    "                        \"details\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"dataset_size_description\": {\n",
    "                                    \"type\": \"boolean\",\n",
    "                                    \"description\": \"Indicates whether there is a description of the dataset size.\"\n",
    "                                },\n",
    "                                \"data_collection_description\": {\n",
    "                                    \"type\": \"boolean\",\n",
    "                                    \"description\": \"Indicates whether there is a description of data collection.\"\n",
    "                                },\n",
    "                                \"size_decription_detail\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"Description of the dataset size.\"\n",
    "                                },\n",
    "                                \"data_collection_detail\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"Description of the data collection.\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"dataset_size_description\",\n",
    "                                \"data_collection_description\",\n",
    "                                \"size_decription_detail\",\n",
    "                                \"data_collection_detail\"\n",
    "                            ],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"source_description\",\n",
    "                        \"real_world\",\n",
    "                        \"simulation\",\n",
    "                        \"details\"\n",
    "                    ],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        temperature=0,\n",
    "        max_completion_tokens=16383,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "data['source_description'] = ''\n",
    "data['real_world'] = ''\n",
    "data['simulation'] = ''\n",
    "data['dataset_size_description'] = ''\n",
    "data['data_collection_description'] = ''\n",
    "data['is_data_mentioned'] = 0\n",
    "if not os.path.exists(result_data_folder + '/data-description'):\n",
    "    os.makedirs(result_data_folder + '/data-description')\n",
    "for i in tqdm(range(len(data))):\n",
    "# for i in tqdm(range(10)):\n",
    "    # Initialize data context\n",
    "    data_context = ''\n",
    "    label = 0\n",
    "    try:\n",
    "        # Construct paths for journal and paper\n",
    "        journal_path = os.path.join(full_data_folder, data['issn'][i])\n",
    "        paper_path = os.path.join(journal_path, f\"{data['unique_id'][i]}.xml\")\n",
    "        \n",
    "        # get the abstract from the xml\n",
    "        abstract = extract_abstract_from_xml(paper_path)\n",
    "        if abstract:\n",
    "            # if \"data\" in abstract.lower():\n",
    "            if \"data\" in abstract.lower():\n",
    "                label = 1\n",
    "        data_context += abstract\n",
    "        # Extract and process sections from the paper XML\n",
    "        sections = extract_sections_and_text_from_xml(paper_path)\n",
    "        reorganized_sections = postprocess_sections(sections)\n",
    "\n",
    "        # Traverse the sections and collect data-related text\n",
    "        for section in reorganized_sections:\n",
    "            if 'data' in section['title'].lower():\n",
    "                data_context += section['text']\n",
    "                label = 1\n",
    "            for subsection in section.get('subsections', []):\n",
    "                if 'data' in subsection['title'].lower():\n",
    "                    data_context += subsection['text']\n",
    "                    label = 1\n",
    "                for subsubsection in subsection.get('subsubsections', []):\n",
    "                    if 'data' in subsubsection['title'].lower():\n",
    "                        data_context += subsubsection['text']\n",
    "                        label = 1\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in data: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    if len(data_context)>0:\n",
    "        response = analyze_with_openai(data_context)\n",
    "        # print(data['title'][i])\n",
    "        # print(response)\n",
    "        response = json.loads(response)\n",
    "        data.loc[i, 'source_description'] = response['source_description']\n",
    "        data.loc[i, 'real_world'] = response['real_world']\n",
    "        data.loc[i, 'simulation'] = response['simulation']\n",
    "        data.loc[i, 'dataset_size_description'] = response['details']['dataset_size_description']\n",
    "        data.loc[i, 'data_collection_description'] = response['details']['data_collection_description']\n",
    "        data.loc[i, 'is_data_mentioned'] = label\n",
    "        # print(response['source_description'])\n",
    "        # combine the response with the data context as json file\n",
    "        data_description = {\n",
    "            \"data_context\": data_context,\n",
    "            \"data_source_description\": response\n",
    "        }\n",
    "        save_json(data_description, os.path.join(result_data_folder + '/data-description', f\"{data['unique_id'][i]}.json\"))\n",
    "    else:\n",
    "        print(\"No data context found\")  \n",
    "data.to_csv(os.path.join(full_data_folder, 'full-meta-dataset-data-descriptive.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(meta_data_folder, 'full-meta-dataset-data-descriptive.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_data_mentioned_in_section_title'] = 0\n",
    "url_pattern = r'(https?://\\S+|www\\.\\S+)'  # URL pattern\n",
    "for i in range(len(data)):\n",
    "# # In case for the mini test to debug the code\n",
    "# for i in tqdm(range(1000)):\n",
    "    journal_path = os.path.join(full_data_folder, data['issn'][i])  # Path to the journal folder\n",
    "    paper_path = os.path.join(journal_path, data['unique_id'][i] + '.xml')  # Path to the paper folder\n",
    "    sections = extract_sections_and_text_from_xml(paper_path)\n",
    "    reorganized_sections = postprocess_sections(sections)\n",
    "    label = 0\n",
    "    url = []\n",
    "    for section in reorganized_sections:\n",
    "        section_title = section['title']\n",
    "        if 'data' in section_title.lower():\n",
    "            if section['text']:\n",
    "                if 'avaiable' in section['text'].lower():\n",
    "                    label = 1\n",
    "                    print(section['text'])\n",
    "        for subsection in section['subsections']:\n",
    "            subsection_title = subsection['title']\n",
    "            if 'data' in subsection_title.lower():\n",
    "                section_text = subsection['text']\n",
    "                label = 1\n",
    "                if section['text']:\n",
    "                    url.extend(re.findall(url_pattern, section['text']))\n",
    "            for subsubsection in subsection['subsubsections']:\n",
    "                subsubsection_title = subsubsection['title']\n",
    "                if 'data' in subsubsection_title.lower():\n",
    "                    label = 1\n",
    "                    if section['text']:\n",
    "                        url.extend(re.findall(url_pattern, section['text']))\n",
    "    data.loc[i, 'is_data_mentioned_in_section_title'] = label\n",
    "    # if url:\n",
    "    #     print(url)\n",
    "print(len(data[data['is_data_mentioned_in_section_title'] == 1])/(len(data)))\n",
    "data.to_csv(os.path.join(meta_data_folder, 'full-meta-dataset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_experiment_mentioned_in_section_title'] = 0\n",
    "for i in range(len(data)):\n",
    "# # In case for the mini test to debug the code\n",
    "# for i in tqdm(range(1000)):\n",
    "    journal_path = os.path.join(full_data_folder, data['issn'][i])  # Path to the journal folder\n",
    "    paper_path = os.path.join(journal_path, data['unique_id'][i] + '.xml')  # Path to the paper folder\n",
    "    sections = extract_sections_and_text_from_xml(paper_path)\n",
    "    reorganized_sections = postprocess_sections(sections)\n",
    "    label = 0\n",
    "    for section in reorganized_sections:\n",
    "        section_title = section['title']\n",
    "        if 'experiment' in section_title.lower():\n",
    "            label = 1\n",
    "        for subsection in section['subsections']:\n",
    "            subsection_title = subsection['title']\n",
    "            if 'experiment' in subsection_title.lower():\n",
    "                label = 1\n",
    "            for subsubsection in subsection['subsubsections']:\n",
    "                subsubsection_title = subsubsection['title']\n",
    "                if 'experiment' in subsubsection_title.lower():\n",
    "                    label = 1\n",
    "    data.loc[i, 'is_experiment_mentioned_in_section_title'] = label\n",
    "data.to_csv(os.path.join(meta_data_folder, 'full-meta-dataset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_link_in_avaiablity_statement'] = 0\n",
    "data['num_of_links_in_avaiablity_statement'] = 0\n",
    "url_pattern = r'(https?://\\S+|www\\.\\S+)'  # URL pattern\n",
    "# # In case for the mini test to debug the code\n",
    "url_data = []\n",
    "for i in range(len(data)):\n",
    "# for i in range(1000):\n",
    "    journal_path = os.path.join(full_data_folder, data['issn'][i])  # Path to the journal folder\n",
    "    paper_path = os.path.join(journal_path, data['unique_id'][i] + '.xml')  # Path to the paper folder\n",
    "    sections = extract_sections_and_text_from_xml(paper_path)\n",
    "    reorganized_sections = postprocess_sections(sections)\n",
    "    for section in reorganized_sections:\n",
    "        section_title = section['title']\n",
    "        if 'data availability' in section_title.lower():\n",
    "            if 'http' in section['text']:\n",
    "                data.loc[i, 'is_link_in_avaiablity_statement'] = 1\n",
    "                url = re.findall(url_pattern, section['text'])\n",
    "                unique_url = list(set(url))\n",
    "                if url:\n",
    "                    print(url)\n",
    "                url_data.append({\n",
    "                'issn': data['issn'][i],\n",
    "                'unique_id': data['unique_idf'][i],\n",
    "                'title': data['title'][i],\n",
    "                'url': unique_url\n",
    "                })\n",
    "                data.loc[i, 'num_of_links_in_avaiablity_statement'] = len(unique_url)\n",
    "data.to_csv(os.path.join(meta_data_folder, 'full-meta-dataset.csv'), index=False)\n",
    "save_json(url_data, os.path.join(meta_data_folder, 'url_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform a str to json\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_content = json.loads(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_content['real_world']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RR-measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
