{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Structure of the Dataset Folder\n",
    "```\n",
    "├── journal-meta/\n",
    "│   ├── 0968-090X.csv         # TRC\n",
    "│   ├── mini-dataset.csv      # A mini dataset for tutorial, extracted from TRC\n",
    "│   └── ... (other journal CSV files)\n",
    "├── journal-full-text/\n",
    "│   ├── 0968-090X/\n",
    "│   │   └── 10.1016_j.trc.2023.104311.xml\n",
    "│   └── ... (other DOI folders)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "# here replace the path to the folder of your downloaded dataset\n",
    "full_text_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-full-text'\n",
    "meta_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-meta'\n",
    "# List all files in the folder with csv\n",
    "journals = [f for f in os.listdir(meta_folder) if f.endswith('.csv')]\n",
    "journal_issn_list = [['TRA','0965-8564'],\n",
    "                     ['TRB','0191-2615'],\n",
    "                     ['TRC','0968-090X'],\n",
    "                     ['TRD','1361-9209'],\n",
    "                     ['TRE','1366-5545'],\n",
    "                     ['TRF','1369-8478'],\n",
    "                     ['TRIP','2590-1982'],\n",
    "                     ['mini-dataset','0968-090X']]\n",
    "journal_issn_df = pd.DataFrame(journal_issn_list, columns=['journal','issn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section demonstrates how to work with the dataset, \n",
    "# utilizing the dataframe's apply method for efficient iteration in a loop.\n",
    "# An example here is to clean up the abstract.\n",
    "def cleanup_abstract(abstract):\n",
    "    \"\"\"\n",
    "    Cleans up an abstract string by standardizing spacing.\n",
    "\n",
    "    Args:\n",
    "        abstract (str): The abstract of a journal article, which may contain irregular spacing,\n",
    "                        including multiple spaces, leading spaces, or trailing spaces.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned string where all excessive spaces are replaced with a single space,\n",
    "             and any leading or trailing spaces are removed. This is essential for preparing\n",
    "             text data for further analysis or display, ensuring uniformity in the formatting\n",
    "             of abstracts.\n",
    "\n",
    "    Example:\n",
    "        >>> cleanup_abstract(\"  This  is   an example   abstract.  \")\n",
    "        'This is an example abstract.'\n",
    "    \"\"\"\n",
    "    # Check if the input is a string\n",
    "    if not isinstance(abstract, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    \n",
    "    return re.sub(r'\\s+', ' ', abstract).strip()\n",
    "for journal in journals:\n",
    "    if journal == 'mini-dataset.csv': # to take the mini dataset for tutorial\n",
    "        # connect the journal name with the issn from the journal_issn_df\n",
    "        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['issn'] = journal_issn\n",
    "        journal_meta['abstract'] = journal_meta['abstract'].apply(cleanup_abstract) # to clean up the abstract\n",
    "        journal_meta.to_csv(os.path.join(meta_folder, journal), index=False) # at the end, save the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section demostrates how to link the doi with the full text\n",
    "def doi_to_unique_id(doi):\n",
    "    \"\"\"\n",
    "    Converts a DOI to a unique identifier by replacing slashes with underscores.\n",
    "\n",
    "    Args:\n",
    "        doi (str): The DOI of a journal article.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique identifier where slashes are replaced with underscores.\n",
    "\n",
    "    Example:\n",
    "        >>> doi_to_unique_id(\"10.1016/j.trc.2023.104311\")\n",
    "        \"10.1016_j.trc.2023_104311\"\n",
    "    \"\"\"\n",
    "    return doi.replace('/', '_')\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_sections_and_text_from_xml(file_path):\n",
    "    \"\"\"\n",
    "    Extracts sections and text from an XML file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "    \n",
    "    Example:\n",
    "        >>> extract_sections_and_text_from_xml('/path/to/file.xml')\n",
    "        [{'label': '1', 'title': 'Introduction', 'text': 'This is the introduction...', 'subsections': []}]\n",
    "    \"\"\"\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Namespace to handle XML namespaces\n",
    "    namespaces = {\n",
    "        'xocs': 'http://www.elsevier.com/xml/xocs/dtd',\n",
    "        'ce': 'http://www.elsevier.com/xml/common/dtd',\n",
    "        'ja': 'http://www.elsevier.com/xml/ja/dtd',\n",
    "        'mml': 'http://www.w3.org/1998/Math/MathML'\n",
    "    }\n",
    "\n",
    "    # Extracting the sections using the item-toc element\n",
    "    sections = []\n",
    "    for item in root.findall('.//xocs:item-toc-entry', namespaces):\n",
    "        section_title = item.find('xocs:item-toc-section-title', namespaces)\n",
    "        section_label = item.find('xocs:item-toc-label', namespaces)\n",
    "        section_text = []\n",
    "        \n",
    "        # Use the section label to find the corresponding section id in <ce:section>\n",
    "        if section_label is not None and section_title is not None:\n",
    "            label_text = section_label.text.strip()\n",
    "            section_elem = root.find(f\".//ce:section[ce:label='{label_text}']\", namespaces)\n",
    "            if section_elem is not None:\n",
    "                # Get all text under the section element, including paragraphs and other texts\n",
    "                section_text_parts = []\n",
    "                subsections = []\n",
    "                before_subsection_text = True\n",
    "\n",
    "                # Iterate over all elements within the section\n",
    "                for elem in section_elem:\n",
    "                    # Check if this element is a subsection\n",
    "                    if elem.tag == f\"{{{namespaces['ce']}}}section\":\n",
    "                        # This is a subsection, process it\n",
    "                        subsection_title_elem = elem.find(f\"ce:section-title\", namespaces)\n",
    "                        if subsection_title_elem is not None:\n",
    "                            subsection_title = subsection_title_elem.text\n",
    "                            subsection_paragraphs = []\n",
    "                            subsubsections = []\n",
    "                            \n",
    "                            for sub_elem in elem:\n",
    "                                # If this is a paragraph, append text\n",
    "                                if sub_elem.tag == f\"{{{namespaces['ce']}}}para\":\n",
    "                                    paragraph_text = ''.join(sub_elem.itertext())\n",
    "                                    subsection_paragraphs.append(paragraph_text)\n",
    "                                \n",
    "                                # If this is a sub-subsection, process it\n",
    "                                elif sub_elem.tag == f\"{{{namespaces['ce']}}}section\":\n",
    "                                    subsubsection_title_elem = sub_elem.find(f\"ce:section-title\", namespaces)\n",
    "                                    if subsubsection_title_elem is not None:\n",
    "                                        subsubsection_title = subsubsection_title_elem.text\n",
    "                                        subsubsection_paragraphs = []\n",
    "                                        for subsub_elem in sub_elem.findall('ce:para', namespaces=namespaces):\n",
    "                                            paragraph_text = ''.join(subsub_elem.itertext())\n",
    "                                            subsubsection_paragraphs.append(paragraph_text)\n",
    "                                        subsubsection_text = ' '.join(subsubsection_paragraphs)\n",
    "                                        subsubsections.append({\n",
    "                                            \"label\": sub_elem.find(f\"ce:label\", namespaces).text if sub_elem.find(f\"ce:label\", namespaces) is not None else \"\",\n",
    "                                            \"title\": subsubsection_title,\n",
    "                                            \"text\": subsubsection_text\n",
    "                                        })\n",
    "                            \n",
    "                            subsection_text = ' '.join(subsection_paragraphs)\n",
    "                            subsections.append({\n",
    "                                \"label\": elem.find(f\"ce:label\", namespaces).text if elem.find(f\"ce:label\", namespaces) is not None else \"\",\n",
    "                                \"title\": subsection_title,\n",
    "                                \"text\": subsection_text,\n",
    "                                \"subsubsections\": subsubsections\n",
    "                            })\n",
    "                    else:\n",
    "                        # Collect text before any subsection starts\n",
    "                        if before_subsection_text and elem.tag == f\"{{{namespaces['ce']}}}para\":\n",
    "                            paragraph_text = ''.join(elem.itertext())\n",
    "                            section_text_parts.append(paragraph_text)\n",
    "\n",
    "                section_text = ' '.join(section_text_parts)\n",
    "                \n",
    "                sections.append({\n",
    "                    \"label\": section_label.text,\n",
    "                    \"title\": section_title.text,\n",
    "                    \"text\": section_text,\n",
    "                    \"subsections\": subsections\n",
    "                })\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Function to postprocess sections, subsections, and subsubsections\n",
    "def postprocess_sections(data):\n",
    "    \"\"\"\n",
    "    Postprocesses sections, subsections, and subsubsections by removing duplicate labels and ensuring unique content.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "\n",
    "    Example:\n",
    "        >>> reorganized_sections = postprocess_sections(sections)\n",
    "        # Save the reorganized sections to a JSON file\n",
    "        import json\n",
    "        # Define the file path for the output\n",
    "        output_file_path = '../example.json'\n",
    "        \n",
    "        # Open the file in write mode and dump the data\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            json.dump(reorganized_sections, file, indent=4)  # Added indentation for better readability\n",
    "        \n",
    "        for section in reorganized_sections:\n",
    "            print(section['label'], section['title'])\n",
    "            for subsection in section['subsections']:\n",
    "                print(\"    \", subsection['label'], subsection['title'])\n",
    "                for subsubsection in subsection['subsubsections']:\n",
    "                    print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "        # if you want to see the original sections, subsections, and subsubsections, you can use the following code\n",
    "        # for section in sections:\n",
    "        #     print(section['label'], section['title'])\n",
    "        #     for subsection in section['subsections']:\n",
    "        #         print(\"    \", subsection['label'], subsection['title'])\n",
    "        #         for subsubsection in subsection['subsubsections']:\n",
    "        #             print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "    \"\"\"\n",
    "    reorganized_data = []\n",
    "    \n",
    "    labels_to_remove = set()\n",
    "    \n",
    "    for section in data:\n",
    "        # Skip if the section is marked for removal\n",
    "        if section[\"label\"] in labels_to_remove:\n",
    "            continue\n",
    "        \n",
    "        new_section = {\n",
    "            \"label\": section[\"label\"],\n",
    "            \"title\": section[\"title\"],\n",
    "            \"text\": section[\"text\"],\n",
    "            \"subsections\": []\n",
    "        }\n",
    "        \n",
    "        # Iterate through subsections to reorganize them\n",
    "        for subsection in data:\n",
    "            # Check if the subsection label starts with the section label and follows the x.x format\n",
    "            if subsection[\"label\"].startswith(section[\"label\"] + \".\") and len(subsection[\"label\"].split('.')) == 2:\n",
    "                new_subsection = {\n",
    "                    \"label\": subsection[\"label\"],\n",
    "                    \"title\": subsection[\"title\"],\n",
    "                    \"text\": subsection[\"text\"],\n",
    "                    \"subsubsections\": []\n",
    "                }\n",
    "                labels_to_remove.add(subsection[\"label\"])\n",
    "                \n",
    "                # Iterate through subsubsections to reorganize them under the appropriate subsection\n",
    "                for subsubsection in data:\n",
    "                    if subsubsection[\"label\"].startswith(new_subsection[\"label\"] + \".\"):\n",
    "                        new_subsubsection = {\n",
    "                            \"label\": subsubsection[\"label\"],\n",
    "                            \"title\": subsubsection[\"title\"],\n",
    "                            \"text\": subsubsection[\"text\"]\n",
    "                        }\n",
    "                        labels_to_remove.add(subsubsection[\"label\"])\n",
    "                        new_subsection[\"subsubsections\"].append(new_subsubsection)\n",
    "                \n",
    "                # Add the subsection only if it is unique or has no subsubsections\n",
    "                if new_subsection[\"subsubsections\"]:\n",
    "                    # If subsubsections exist, avoid duplicate content\n",
    "                    new_subsection[\"text\"] = \"\"\n",
    "                new_section[\"subsections\"].append(new_subsection)\n",
    "        \n",
    "        reorganized_data.append(new_section)\n",
    "    \n",
    "    return reorganized_data\n",
    "\n",
    "# search the \"github.com\" across all the text in all the sections, subsections, and subsubsections\n",
    "# and extract the full github url, like https://github.com/username/repository\n",
    "def extract_github_urls(text):\n",
    "    # Regular expression to match GitHub URLs exactly without trailing directories\n",
    "    github_url_pattern = r\"https?://github\\.com/[\\w-]+/[\\w-]+(?!/\\S)\"\n",
    "    \n",
    "    # Find all matching GitHub URLs in the text\n",
    "    github_urls = re.findall(github_url_pattern, text)\n",
    "    \n",
    "    return github_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/pabloguarda/isuelogit', 'https://github.com/pabloguarda/pesuelogit']\n",
      "['https://github.com/zhandongxu/GP_RTAP']\n",
      "['https://github.com/tjzxh/EADC']\n",
      "['https://github.com/LehmannJonas/2E-MT-VRP-PTW-Instances']\n",
      "['https://github.com/LiBiyue/MAST-GNN']\n",
      "['https://github.com/HDDL/DPRDDM']\n",
      "['https://github.com/xinychen/transdim']\n"
     ]
    }
   ],
   "source": [
    "for journal in journals:\n",
    "    if journal == 'mini-dataset.csv': # to take the mini dataset for tutorial\n",
    "        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['unique_id'] = journal_meta['doi'].apply(doi_to_unique_id) # to convert the doi to a unique id\n",
    "        # here we use the for loop to help understand how it works, it can be done in one line of code later\n",
    "        for i in range(len(journal_meta)):\n",
    "            github_urls = []\n",
    "            fulltext_path = os.path.join(full_text_folder, journal_issn, journal_meta.iloc[i]['unique_id'] + '.xml')\n",
    "            sections = extract_sections_and_text_from_xml(fulltext_path)\n",
    "            reorganized_sections = postprocess_sections(sections)\n",
    "            for section in reorganized_sections:\n",
    "                # add a preprcessing for the text here to make it more readable\n",
    "                urls  = extract_github_urls(cleanup_abstract(section['text']))\n",
    "                if urls:\n",
    "                    github_urls.extend(urls)\n",
    "                for subsection in section['subsections']:\n",
    "                    urls = github_urls.extend(extract_github_urls(cleanup_abstract(subsection['text'])))\n",
    "                    if urls:\n",
    "                        github_urls.extend(urls)\n",
    "                    for subsubsection in subsection['subsubsections']:\n",
    "                        urls = extract_github_urls(cleanup_abstract(subsubsection['text']))\n",
    "                        if urls:\n",
    "                            github_urls.extend(urls)\n",
    "            if github_urls:\n",
    "                print(github_urls)\n",
    "# write the reorganized sections to a json file\n",
    "import json\n",
    "with open('example.json', 'w') as file:\n",
    "    json.dump(reorganized_sections, file, indent=4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "2 Preliminaries\n",
      "     2.1 Notations\n",
      "     2.2 Problem definition\n",
      "3 Methodology\n",
      "     3.1 Model description\n",
      "     3.2 Computing the variable \n",
      "     3.3 Computing the variable \n",
      "     3.4 Solution algorithm\n",
      "4 Experiments\n",
      "     4.1 Traffic data sets\n",
      "     4.2 Baseline imputation models\n",
      "     4.3 Imputation results\n",
      "         4.3.1 Evaluation on PeMS-4W and PeMS-8W Data\n",
      "5 Conclusion and future directions\n",
      "Appendix A Computing sources\n"
     ]
    }
   ],
   "source": [
    "for section in reorganized_sections:\n",
    "    print(section['label'], section['title'])\n",
    "    # print(cleanup_abstract(section['text']))\n",
    "    # print('--------------------------------')\n",
    "    for subsection in section['subsections']:\n",
    "        print(\"    \", subsection['label'], subsection['title'])\n",
    "        # print(cleanup_abstract(subsection['text']))\n",
    "        # print('--------------------------------')\n",
    "        for subsubsection in subsection['subsubsections']:\n",
    "            print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "            # print(cleanup_abstract(subsubsection['text']))\n",
    "            # print('--------------------------------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/lijunsun/bgcp_imputation']\n",
      "['https://github.com/bstabler/TransportationNetwork']\n",
      "['https://github.com/davidrey123/Ridesharing']\n",
      "['https://github.com/FwDeng/ODPFM']\n",
      "['https://github.com/DanqingZ/CPS_TRC']\n",
      "['https://github.com/Lemma1/Multimodal-DUE']\n",
      "['https://github.com/sysuits/BATF']\n",
      "['https://github.com/junzis/acsmc']\n",
      "['https://github.com/stasmix/popsynth']\n",
      "['https://github.com/mbattifarano/mac-data']\n",
      "['https://github.com/ZiyuanGu/network-bi-partitioning']\n",
      "['https://github.com/adfriedm/WorkFunctionAlgorithm']\n",
      "['https://github.com/optimatorlab/mFSTSP']\n",
      "['https://github.com/rahulnair23/transfor-2019']\n",
      "['https://github.com/LiTrans/BSMD']\n",
      "['https://github.com/cjsyzwsh/ASU-DNN']\n",
      "['https://github.com/jsebanaz90/TRC_2019_867-SupplementaryMaterials']\n",
      "['https://github.com/wenbo-purdue-git/isttt-23-taxi-system-modeling-']\n",
      "['https://github.com/xchChen/CACSP_ADMM']\n",
      "['https://github.com/junzis/openap', 'https://github.com/tudelft-cns-atm/bluesky']\n",
      "['https://github.com/DrKeHan/DTD']\n",
      "['https://github.com/Charles117/resilience_shenzhen']\n",
      "['https://github.com/Jishnuns/IntegratedFixedFlexibleRouteChoiceAssignment']\n",
      "['https://github.com/zhiyongc/Seattle-Loop-Data', 'https://github.com/FelixOpolka/STGCN-PyTorch']\n",
      "['https://github.com/sinadabiri/CATT-DL-Vehicle-Classifcation-GPS']\n",
      "['https://github.com/sudatta0993/Dynamic-Congestion-Prediction']\n",
      "['https://github.com/liyaguang/DCRNN', 'https://github.com/liyaguang/DCRNN']\n",
      "['https://github.com/Kaimaoge/SUMO-DVSL']\n",
      "['https://github.com/xinychen/transdim']\n",
      "['https://github.com/TonyLiu2015/AVRLite', 'https://github.com/TonyLiu2015/AVRLite', 'https://github.com/TonyLiu2015/AVRLite']\n",
      "['https://github.com/ZiyuanGu/macroscopic-parking-modeling-and-pricing']\n",
      "['https://github.com/cjsyzwsh/dnn-for-economic-information']\n",
      "['https://github.com/zhiyongc/Seattle-Loop-Data', 'https://github.com/liyaguang/DCRNN']\n",
      "['https://github.com/VeritasYin/STGCN_IJCAI-18', 'https://github.com/liyaguang/DCRNN', 'https://github.com/nnzhan/Graph-WaveNet', 'https://github.com/xlwang233/STSeq2Seq']\n",
      "['https://github.com/amillb/pgMapMatch']\n",
      "['https://github.com/feng-jings/LDTmodel']\n",
      "['https://github.com/optimatorlab/mFSTSP-VDS', 'https://github.com/optimatorlab/mFSTSP-VDS']\n",
      "['https://github.com/DanieleGammelli/CensoredGP']\n",
      "['https://github.com/nacici/AV-preference-synthesizing']\n",
      "['https://github.com/Panchamy/3D-Partitionin']\n",
      "['https://github.com/mbc96325/Epidemic-spreading-model-on-public-transit']\n",
      "['https://github.com/meeadsaberi/dynamel']\n",
      "['https://github.com/eqasim-org/eqasim-java', 'https://github.com/matsim-org/pt2matsim']\n",
      "['https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix', 'https://github.com/andrewssobral/bgslibrary']\n",
      "['https://github.com/twintproject/twint']\n",
      "['https://github.com/gm-tools/gm-tools']\n",
      "['https://github.com/ronan-keane/havsim']\n",
      "['https://github.com/wangleicuail/EVsharingBasicData']\n",
      "['https://github.com/LiTrans/reslogit-example']\n",
      "['https://github.com/The-Ahmad/explainable_anns_for_travel_demand_analysis']\n",
      "['https://github.com/RomainLITUD/DGCN_traffic_forecasting']\n",
      "['https://github.com/jaredleekatzman/DeepSurv']\n",
      "['https://github.com/xinychen/transdim']\n",
      "['https://github.com/Vadermit/TransPAI']\n",
      "['https://github.com/bstabler/TransportationNetworks']\n",
      "['https://github.com/licit-lab/Open-SymuVia']\n",
      "['https://github.com/openplans/Leaflet']\n",
      "['https://github.com/yashpant/FlyByLogic', 'https://github.com/yashpant/FlyByLogic']\n",
      "['https://github.com/Turbo87/utm']\n",
      "['https://github.com/HaziqRazali/Pedestrian-Intention-Prediction']\n",
      "['https://github.com/mbc96325/Mixture-logit-model-for-AV-adoption']\n",
      "['https://github.com/bstabler/TransportationNetworks']\n",
      "['https://github.com/eqasim-org/ile-de-franc']\n",
      "['https://github.com/zhengyunhan/Equality-of-Opportunity-Travel-Behavior']\n",
      "['https://github.com/Amelia55/Adaptive-Control-System-for-UAM']\n",
      "['https://github.com/ycui4/Daily-Activity-Location-Schedule-Model']\n",
      "['https://github.com/bstabler/TransportationNetwork']\n",
      "['https://github.com/snu-adsl/DDP-GCN', 'https://github.com/snu-adsl/ddpgcn-dataset']\n",
      "['https://github.com/YoungYoung619/reinforcement-learning-based-driving-decision-in-Carla']\n",
      "['https://github.com/FinestStone/AV-Interactions-City-Traffic']\n",
      "['https://github.com/gsfeir/GP-LCCM', 'https://github.com/gsfeir/GBM-LCCM']\n",
      "['https://github.com/leeke2/tt-sync-scenarios']\n",
      "['https://github.com/ChengTraffic/Polynomial-Arrival-Queue-PAQ']\n",
      "['https://github.com/DanieleGammelli/variational-Poisson-rnn']\n",
      "['https://github.com/bstabler/TransportationNetworks']\n",
      "['https://github.com/HossseinMoradi/Project1', 'https://github.com/HossseinMoradi/Project2']\n",
      "['https://github.com/HaoZhouGT/openpilot']\n",
      "['https://github.com/dingjiansw101/AerialDetection']\n",
      "['https://github.com/tongnie/tensorlib']\n",
      "['https://github.com/Future-Mobility-Lab/bi-level-framework']\n",
      "['https://github.com/HaoZhouGT/openpilot', 'https://github.com/HaoZhouGT/openpilot']\n",
      "['https://github.com/salomonw/contraflow-lane-reversal']\n",
      "['https://github.com/liyaguang/DCRNN', 'https://github.com/zhiyongc/Graph-Markov-Network']\n",
      "['https://github.com/ramondalmau/Machine-learning-for-ATFM']\n",
      "['https://github.com/bstabler/TransportationNetwork']\n",
      "['https://github.com/AnirudhSubramanyam/REEVRP-Instances']\n",
      "['https://github.com/liyaguang/DCRNN', 'https://github.com/liyaguang/DCRNN', 'https://github.com/zhiyongc/Seattle-Loop-Data', 'https://github.com/lehaifeng/T-GC', 'https://github.com/lucktroy/DeepS', 'https://github.com/lehaifeng/T-GCN', 'https://github.com/fivethirtyeight/uber-tlc-foil-response', 'https://github.com/ivechan/PVCGNZ', 'https://github.com/ivechan/PVCGN', 'https://github.com/numenta/NA']\n",
      "['https://github.com/Yifanny/Generative_CF_Model_Conditioned_On_DS']\n",
      "['https://github.com/sguo28/DROP_Simulator']\n",
      "['https://github.com/jiachaol/BMUE_TRC']\n",
      "['https://github.com/MathiasNT/NRI_for_Transport', 'https://github.com/liyaguang/DCRNN', 'https://github.com/MathiasNT/NRI_for_Transport']\n",
      "['https://github.com/wn11-nyu/IRP-Drone']\n",
      "['https://github.com/joeyleehk/IrConv-LSTM']\n",
      "['https://github.com/pcbouman-eur/TSP-D-Instances']\n",
      "['https://github.com/liangchunyaobing/RCM-AIRL']\n",
      "['https://github.com/micheledsimoni/food_platform_instances']\n",
      "['https://github.com/descon-uccs/gould-trptc-2022']\n",
      "['https://github.com/danlegend5/FitFun']\n",
      "['https://github.com/nytimes/covid-19-data']\n",
      "['https://github.com/tongnie/tensor4kriging']\n",
      "['https://github.com/RadetzkyLi/3P-MSPointNet']\n",
      "['https://github.com/vita-epfl/butterflydetector', 'https://github.com/alexandre01/UltimateLabeling']\n",
      "['https://github.com/jiawlu/Traffic_State_Estimation-Computational_Graph']\n",
      "['https://github.com/VROOM-Project/vroom', 'https://github.com/graphhopper/jsprit', 'https://github.com/zephyr-data-specs/GMNS', 'https://github.com/asu-trans-ai-lab/Integrated_modeling_GMN', 'https://github.com/asu-trans-ai-lab/CAMLite', 'https://github.com/jiawlu/CAMLite']\n",
      "['https://github.com/fbohu/CensoredTGCN']\n",
      "['https://github.com/mcgill-smart-transport/circDMDsp']\n",
      "['https://github.com/Minyu-Shen/bus_berth_allocation', 'https://github.com/Minyu-Shen/bus_berth_allocation', 'https://github.com/Minyu-Shen/bus_berth_allocation', 'https://github.com/Minyu-Shen/bus_berth_allocation']\n",
      "['https://github.com/Gengmaosi/PIT-IDM']\n",
      "['https://github.com/zouguojian/STGI', 'https://github.com/guoshnBJTU/ASTGCN-r-pytorch', 'https://github.com/zouguojian/STGIN']\n",
      "['https://github.com/FeiyuYangAIOptimPlanning/DRLA-eTGM']\n",
      "['https://github.com/bstabler/TransportationNetworks']\n",
      "['https://github.com/Yiru-Jiao/DriverSpaceInference']\n",
      "['https://github.com/ljq19920202/MotionPrediction_for_bicycles']\n",
      "['https://github.com/PREDICT-EPFL/Distributed-DeeP-LCC']\n",
      "['https://github.com/epan-utbm/Moving-Object-Detection-Dataset', 'https://github.com/epan-utbm/Semantic-Aware-Object-Identification-in-Urban-Driving-Scenarios']\n",
      "['https://github.com/lpaparusso/DriVe-forecast']\n",
      "['https://github.com/JoseAngelMartinB/prediction-behavioural-analysis-ml-travel-mode-choice', 'https://github.com/JoseAngelMartinB/prediction-behavioural-analysis-ml-travel-mode-choice']\n",
      "['https://github.com/mie-lab/location-prediction']\n",
      "['https://github.com/bstabler/Transportationetwork', 'https://github.com/bstabler/TransportationNetwork', 'https://github.com/bstabler/TransportationNetwork']\n",
      "['https://github.com/tongnie/GNN4Flow']\n",
      "['https://github.com/aberke/drones-consumer-privacy', 'https://github.com/aberke/drones-consumer-privac', 'https://github.com/aberke/drones-consumer-privacy']\n",
      "['https://github.com/epournaras/EPOS']\n",
      "['https://github.com/pabloguarda/isuelogit', 'https://github.com/pabloguarda/pesuelogit']\n",
      "['https://github.com/zhandongxu/GP_RTAP']\n",
      "['https://github.com/tjzxh/EADC']\n",
      "['https://github.com/LehmannJonas/2E-MT-VRP-PTW-Instances']\n",
      "['https://github.com/LiBiyue/MAST-GNN']\n",
      "['https://github.com/HDDL/DPRDDM']\n",
      "['https://github.com/cliu01/Dataset_LSSAVDARP_ClermontFd']\n",
      "['https://github.com/ekinugurel/GPSImpute']\n",
      "['https://github.com/mie-lab/train_delay']\n",
      "['https://github.com/jcpinap/DTSP-DR']\n",
      "['https://github.com/ashishbhaskar/MCMD']\n",
      "['https://github.com/BUILTNYU/ridepooling-with-transfers']\n",
      "['https://github.com/CL2-UWaterloo/GP-MPC-of-Platooning']\n",
      "['https://github.com/Wodenxdy/TTRDCP']\n",
      "['https://github.com/RomainLITUD/UQnet-arxi']\n",
      "['https://github.com/Lucky-Fan/GP_TSE']\n",
      "['https://github.com/TMIS-Turbo/DARRL']\n",
      "['https://github.com/gjy3035/C-3-Framework']\n",
      "['https://github.com/asu-trans-ai-lab/Path4GMN', 'https://github.com/Taehooie/CGEquilibrium']\n",
      "['https://github.com/zhiyongc/Seattle-Loop-Data', 'https://github.com/peterchen96/LRTC-TMCP']\n",
      "['https://github.com/Future-Mobility-Lab/Entropy-weighting-method']\n",
      "['https://github.com/CSSEGISandData/COVID-19', 'https://github.com/geatpy-dev/geatpy', 'https://github.com/geatpy-dev/geatpy', 'https://github.com/scipy/scipy']\n",
      "['https://github.com/yonsann/GACF', 'https://github.com/yonsann/GACF']\n",
      "['https://github.com/nguyendai236/PIA']\n",
      "['https://github.com/wwb00l/CRRFNet']\n",
      "['https://github.com/siqi-feng/DNN-behavioral-regularity']\n",
      "['https://github.com/ethz-coss/calm_rl']\n",
      "['https://github.com/ZJU2021/STAN']\n",
      "['https://github.com/bstabler/TransportationNetworks']\n",
      "['https://github.com/RomainLITUD/conflict_resolution_dataset']\n",
      "['https://github.com/NicoleZlq/metro-expansion', 'https://github.com/NicoleZlq/metro-expansion']\n",
      "['https://github.com/YuanzhengLei/Weighted-the-least-square-method-for-single-regime-fundamental-diagram-models']\n",
      "['https://github.com/bstabler/TransportationNetworks']\n",
      "['https://github.com/decenter2021/SAFFRO']\n",
      "['https://github.com/bstabler/TransportationNetworks']\n",
      "['https://github.com/PascalJD/copulapopgen']\n",
      "158/2050\n"
     ]
    }
   ],
   "source": [
    "for journal in journals:\n",
    "    if journal == '0968-090X.csv': # to take the trc dataset for test\n",
    "        journal_issn = journal.replace('.csv', '')\n",
    "        # journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['unique_id'] = journal_meta['doi'].apply(doi_to_unique_id) # to convert the doi to a unique id\n",
    "        # here we use the for loop to help understand how it works, it can be done in one line of code later\n",
    "        count = 0\n",
    "        for i in range(len(journal_meta)):\n",
    "            github_urls = []\n",
    "            fulltext_path = os.path.join(full_text_folder, journal_issn, journal_meta.iloc[i]['unique_id'] + '.xml')\n",
    "            sections = extract_sections_and_text_from_xml(fulltext_path)\n",
    "            reorganized_sections = postprocess_sections(sections)\n",
    "            for section in reorganized_sections:\n",
    "                # add a preprcessing for the text here to make it more readable\n",
    "                urls  = extract_github_urls(cleanup_abstract(section['text']))\n",
    "                if urls:\n",
    "                    github_urls.extend(urls)\n",
    "                for subsection in section['subsections']:\n",
    "                    urls = github_urls.extend(extract_github_urls(cleanup_abstract(subsection['text'])))\n",
    "                    if urls:\n",
    "                        github_urls.extend(urls)\n",
    "                    for subsubsection in subsection['subsubsections']:\n",
    "                        urls = extract_github_urls(cleanup_abstract(subsubsection['text']))\n",
    "                        if urls:\n",
    "                            github_urls.extend(urls)\n",
    "            if github_urls:\n",
    "                count += 1\n",
    "                print(github_urls)\n",
    "        print(f'{count}/{len(journal_meta)}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RR-measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
