{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Structure of the Dataset Folder\n",
    "```\n",
    "├── journal-meta/\n",
    "│   ├── 0968-090X.csv         # TRC\n",
    "│   ├── mini-dataset.csv      # A mini dataset for tutorial, extracted from TRC\n",
    "│   └── ... (other journal CSV files)\n",
    "├── journal-full-text/\n",
    "│   ├── 0968-090X/\n",
    "│   │   └── 10.1016_j.trc.2023.104311.xml\n",
    "│   └── ... (other DOI folders)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "# here replace the path to the folder of your downloaded dataset\n",
    "full_text_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-full-text'\n",
    "meta_folder = '/Users/junyi/Work/RR/rr-measure-dataset/journal-meta'\n",
    "# List all files in the folder with csv\n",
    "journals = [f for f in os.listdir(meta_folder) if f.endswith('.csv')]\n",
    "journal_issn_list = [['TRA','0965-8564'],\n",
    "                     ['TRB','0191-2615'],\n",
    "                     ['TRC','0968-090X'],\n",
    "                     ['TRD','1361-9209'],\n",
    "                     ['TRE','1366-5545'],\n",
    "                     ['TRF','1369-8478'],\n",
    "                     ['TRIP','2590-1982'],\n",
    "                     ['mini-dataset','0968-090X']]\n",
    "journal_issn_df = pd.DataFrame(journal_issn_list, columns=['journal','issn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section demonstrates how to work with the dataset, \n",
    "# utilizing the dataframe's apply method for efficient iteration in a loop.\n",
    "# An example here is to clean up the abstract.\n",
    "def cleanup_abstract(abstract):\n",
    "    \"\"\"\n",
    "    Cleans up an abstract string by standardizing spacing.\n",
    "\n",
    "    Args:\n",
    "        abstract (str): The abstract of a journal article, which may contain irregular spacing,\n",
    "                        including multiple spaces, leading spaces, or trailing spaces.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned string where all excessive spaces are replaced with a single space,\n",
    "             and any leading or trailing spaces are removed. This is essential for preparing\n",
    "             text data for further analysis or display, ensuring uniformity in the formatting\n",
    "             of abstracts.\n",
    "\n",
    "    Example:\n",
    "        >>> cleanup_abstract(\"  This  is   an example   abstract.  \")\n",
    "        'This is an example abstract.'\n",
    "    \"\"\"\n",
    "    # Check if the input is a string\n",
    "    if not isinstance(abstract, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    \n",
    "    return re.sub(r'\\s+', ' ', abstract).strip()\n",
    "for journal in journals:\n",
    "    if journal == 'mini-dataset.csv': # to take the mini dataset for tutorial\n",
    "        # connect the journal name with the issn from the journal_issn_df\n",
    "        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['issn'] = journal_issn\n",
    "        journal_meta['abstract'] = journal_meta['abstract'].apply(cleanup_abstract) # to clean up the abstract\n",
    "        journal_meta.to_csv(os.path.join(meta_folder, journal), index=False) # at the end, save the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section demostrates how to link the doi with the full text\n",
    "def doi_to_unique_id(doi):\n",
    "    \"\"\"\n",
    "    Converts a DOI to a unique identifier by replacing slashes with underscores.\n",
    "\n",
    "    Args:\n",
    "        doi (str): The DOI of a journal article.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique identifier where slashes are replaced with underscores.\n",
    "\n",
    "    Example:\n",
    "        >>> doi_to_unique_id(\"10.1016/j.trc.2023.104311\")\n",
    "        \"10.1016_j.trc.2023_104311\"\n",
    "    \"\"\"\n",
    "    return doi.replace('/', '_')\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_sections_and_text_from_xml(file_path):\n",
    "    \"\"\"\n",
    "    Extracts sections and text from an XML file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "    \n",
    "    Example:\n",
    "        >>> extract_sections_and_text_from_xml('/path/to/file.xml')\n",
    "        [{'label': '1', 'title': 'Introduction', 'text': 'This is the introduction...', 'subsections': []}]\n",
    "    \"\"\"\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Namespace to handle XML namespaces\n",
    "    namespaces = {\n",
    "        'xocs': 'http://www.elsevier.com/xml/xocs/dtd',\n",
    "        'ce': 'http://www.elsevier.com/xml/common/dtd',\n",
    "        'ja': 'http://www.elsevier.com/xml/ja/dtd',\n",
    "        'mml': 'http://www.w3.org/1998/Math/MathML'\n",
    "    }\n",
    "\n",
    "    # Extracting the sections using the item-toc element\n",
    "    sections = []\n",
    "    for item in root.findall('.//xocs:item-toc-entry', namespaces):\n",
    "        section_title = item.find('xocs:item-toc-section-title', namespaces)\n",
    "        section_label = item.find('xocs:item-toc-label', namespaces)\n",
    "        section_text = []\n",
    "        \n",
    "        # Use the section label to find the corresponding section id in <ce:section>\n",
    "        if section_label is not None and section_title is not None:\n",
    "            label_text = section_label.text.strip()\n",
    "            section_elem = root.find(f\".//ce:section[ce:label='{label_text}']\", namespaces)\n",
    "            if section_elem is not None:\n",
    "                # Get all text under the section element, including paragraphs and other texts\n",
    "                section_text_parts = []\n",
    "                subsections = []\n",
    "                before_subsection_text = True\n",
    "\n",
    "                # Iterate over all elements within the section\n",
    "                for elem in section_elem:\n",
    "                    # Check if this element is a subsection\n",
    "                    if elem.tag == f\"{{{namespaces['ce']}}}section\":\n",
    "                        # This is a subsection, process it\n",
    "                        subsection_title_elem = elem.find(f\"ce:section-title\", namespaces)\n",
    "                        if subsection_title_elem is not None:\n",
    "                            subsection_title = subsection_title_elem.text\n",
    "                            subsection_paragraphs = []\n",
    "                            subsubsections = []\n",
    "                            \n",
    "                            for sub_elem in elem:\n",
    "                                # If this is a paragraph, append text\n",
    "                                if sub_elem.tag == f\"{{{namespaces['ce']}}}para\":\n",
    "                                    paragraph_text = ''.join(sub_elem.itertext())\n",
    "                                    subsection_paragraphs.append(paragraph_text)\n",
    "                                \n",
    "                                # If this is a sub-subsection, process it\n",
    "                                elif sub_elem.tag == f\"{{{namespaces['ce']}}}section\":\n",
    "                                    subsubsection_title_elem = sub_elem.find(f\"ce:section-title\", namespaces)\n",
    "                                    if subsubsection_title_elem is not None:\n",
    "                                        subsubsection_title = subsubsection_title_elem.text\n",
    "                                        subsubsection_paragraphs = []\n",
    "                                        for subsub_elem in sub_elem.findall('ce:para', namespaces=namespaces):\n",
    "                                            paragraph_text = ''.join(subsub_elem.itertext())\n",
    "                                            subsubsection_paragraphs.append(paragraph_text)\n",
    "                                        subsubsection_text = ' '.join(subsubsection_paragraphs)\n",
    "                                        subsubsections.append({\n",
    "                                            \"label\": sub_elem.find(f\"ce:label\", namespaces).text if sub_elem.find(f\"ce:label\", namespaces) is not None else \"\",\n",
    "                                            \"title\": subsubsection_title,\n",
    "                                            \"text\": subsubsection_text\n",
    "                                        })\n",
    "                            \n",
    "                            subsection_text = ' '.join(subsection_paragraphs)\n",
    "                            subsections.append({\n",
    "                                \"label\": elem.find(f\"ce:label\", namespaces).text if elem.find(f\"ce:label\", namespaces) is not None else \"\",\n",
    "                                \"title\": subsection_title,\n",
    "                                \"text\": subsection_text,\n",
    "                                \"subsubsections\": subsubsections\n",
    "                            })\n",
    "                    else:\n",
    "                        # Collect text before any subsection starts\n",
    "                        if before_subsection_text and elem.tag == f\"{{{namespaces['ce']}}}para\":\n",
    "                            paragraph_text = ''.join(elem.itertext())\n",
    "                            section_text_parts.append(paragraph_text)\n",
    "\n",
    "                section_text = ' '.join(section_text_parts)\n",
    "                \n",
    "                sections.append({\n",
    "                    \"label\": section_label.text,\n",
    "                    \"title\": section_title.text,\n",
    "                    \"text\": section_text,\n",
    "                    \"subsections\": subsections\n",
    "                })\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Function to postprocess sections, subsections, and subsubsections\n",
    "def postprocess_sections(data):\n",
    "    \"\"\"\n",
    "    Postprocesses sections, subsections, and subsubsections by removing duplicate labels and ensuring unique content.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "\n",
    "    Example:\n",
    "        >>> reorganized_sections = postprocess_sections(sections)\n",
    "        # Save the reorganized sections to a JSON file\n",
    "        import json\n",
    "        # Define the file path for the output\n",
    "        output_file_path = '../example.json'\n",
    "        \n",
    "        # Open the file in write mode and dump the data\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            json.dump(reorganized_sections, file, indent=4)  # Added indentation for better readability\n",
    "        \n",
    "        for section in reorganized_sections:\n",
    "            print(section['label'], section['title'])\n",
    "            for subsection in section['subsections']:\n",
    "                print(\"    \", subsection['label'], subsection['title'])\n",
    "                for subsubsection in subsection['subsubsections']:\n",
    "                    print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "        # if you want to see the original sections, subsections, and subsubsections, you can use the following code\n",
    "        # for section in sections:\n",
    "        #     print(section['label'], section['title'])\n",
    "        #     for subsection in section['subsections']:\n",
    "        #         print(\"    \", subsection['label'], subsection['title'])\n",
    "        #         for subsubsection in subsection['subsubsections']:\n",
    "        #             print(\"        \", subsubsection['label'], subsubsection['title'])\n",
    "    \"\"\"\n",
    "    reorganized_data = []\n",
    "    \n",
    "    labels_to_remove = set()\n",
    "    \n",
    "    for section in data:\n",
    "        # Skip if the section is marked for removal\n",
    "        if section[\"label\"] in labels_to_remove:\n",
    "            continue\n",
    "        \n",
    "        new_section = {\n",
    "            \"label\": section[\"label\"],\n",
    "            \"title\": section[\"title\"],\n",
    "            \"text\": section[\"text\"],\n",
    "            \"subsections\": []\n",
    "        }\n",
    "        \n",
    "        # Iterate through subsections to reorganize them\n",
    "        for subsection in data:\n",
    "            # Check if the subsection label starts with the section label and follows the x.x format\n",
    "            if subsection[\"label\"].startswith(section[\"label\"] + \".\") and len(subsection[\"label\"].split('.')) == 2:\n",
    "                new_subsection = {\n",
    "                    \"label\": subsection[\"label\"],\n",
    "                    \"title\": subsection[\"title\"],\n",
    "                    \"text\": subsection[\"text\"],\n",
    "                    \"subsubsections\": []\n",
    "                }\n",
    "                labels_to_remove.add(subsection[\"label\"])\n",
    "                \n",
    "                # Iterate through subsubsections to reorganize them under the appropriate subsection\n",
    "                for subsubsection in data:\n",
    "                    if subsubsection[\"label\"].startswith(new_subsection[\"label\"] + \".\"):\n",
    "                        new_subsubsection = {\n",
    "                            \"label\": subsubsection[\"label\"],\n",
    "                            \"title\": subsubsection[\"title\"],\n",
    "                            \"text\": subsubsection[\"text\"]\n",
    "                        }\n",
    "                        labels_to_remove.add(subsubsection[\"label\"])\n",
    "                        new_subsection[\"subsubsections\"].append(new_subsubsection)\n",
    "                \n",
    "                # Add the subsection only if it is unique or has no subsubsections\n",
    "                if new_subsection[\"subsubsections\"]:\n",
    "                    # If subsubsections exist, avoid duplicate content\n",
    "                    new_subsection[\"text\"] = \"\"\n",
    "                new_section[\"subsections\"].append(new_subsection)\n",
    "        \n",
    "        reorganized_data.append(new_section)\n",
    "    \n",
    "    return reorganized_data\n",
    "\n",
    "# search the \"github.com\" across all the text in all the sections, subsections, and subsubsections\n",
    "# and extract the full github url, like https://github.com/username/repository\n",
    "def extract_github_urls(text):\n",
    "    # Regular expression to match GitHub URLs\n",
    "    github_url_pattern = r\"https?://github\\.com/[\\w-]+/[\\w-]+\"\n",
    "    \n",
    "    # Find all matching GitHub URLs in the text\n",
    "    github_urls = re.findall(github_url_pattern, text)\n",
    "    \n",
    "    return github_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to postprocess sections, subsections, and subsubsections\n",
    "def postprocess_sections(data):\n",
    "    \"\"\"\n",
    "    Postprocesses sections, subsections, and subsubsections by removing duplicate labels and ensuring unique content.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing the label, title, text, subsections, and subsubsections of a section.\n",
    "\n",
    "    Example:\n",
    "        >>> reorganized_sections = postprocess_sections(sections)\n",
    "        # Save the reorganized sections to a JSON file\n",
    "        import json\n",
    "        # Define the file path for the output\n",
    "        output_file_path = '../example.json'\n",
    "        \n",
    "        # Open the file in write mode and dump the data\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            json.dump(reorganized_sections, file, indent=4)  # Added indentation for better readability\n",
    "    \"\"\"\n",
    "    reorganized_data = []\n",
    "    \n",
    "    labels_to_remove = set()\n",
    "    \n",
    "    for section in data:\n",
    "        # Skip if the section is marked for removal\n",
    "        if section[\"label\"] in labels_to_remove:\n",
    "            continue\n",
    "        \n",
    "        new_section = {\n",
    "            \"label\": section[\"label\"],\n",
    "            \"title\": section[\"title\"],\n",
    "            \"text\": section[\"text\"],\n",
    "            \"subsections\": []\n",
    "        }\n",
    "        \n",
    "        # Iterate through subsections to reorganize them\n",
    "        for subsection in data:\n",
    "            # Check if the subsection label starts with the section label and follows the x.x format\n",
    "            if subsection[\"label\"].startswith(section[\"label\"] + \".\") and len(subsection[\"label\"].split('.')) == 2:\n",
    "                new_subsection = {\n",
    "                    \"label\": subsection[\"label\"],\n",
    "                    \"title\": subsection[\"title\"],\n",
    "                    \"text\": subsection[\"text\"],\n",
    "                    \"subsubsections\": []\n",
    "                }\n",
    "                labels_to_remove.add(subsection[\"label\"])\n",
    "                \n",
    "                # Iterate through subsubsections to reorganize them under the appropriate subsection\n",
    "                for subsubsection in data:\n",
    "                    if subsubsection[\"label\"].startswith(new_subsection[\"label\"] + \".\"):\n",
    "                        new_subsubsection = {\n",
    "                            \"label\": subsubsection[\"label\"],\n",
    "                            \"title\": subsubsection[\"title\"],\n",
    "                            \"text\": subsubsection[\"text\"]\n",
    "                        }\n",
    "                        labels_to_remove.add(subsubsection[\"label\"])\n",
    "                        new_subsection[\"subsubsections\"].append(new_subsubsection)\n",
    "                \n",
    "                # Add the subsection only if it is unique or has no subsubsections\n",
    "                if new_subsection[\"subsubsections\"]:\n",
    "                    # If subsubsections exist, avoid duplicate content\n",
    "                    new_subsection[\"text\"] = \"\"\n",
    "                new_section[\"subsections\"].append(new_subsection)\n",
    "        \n",
    "        reorganized_data.append(new_section)\n",
    "    \n",
    "    return reorganized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/pabloguarda/pesuelogit']\n"
     ]
    }
   ],
   "source": [
    "for journal in journals:\n",
    "    if journal == 'mini-dataset.csv': # to take the mini dataset for tutorial\n",
    "        journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['unique_id'] = journal_meta['doi'].apply(doi_to_unique_id) # to convert the doi to a unique id\n",
    "        # here we use the for loop to help understand how it works, it can be done in one line of code later\n",
    "        for i in range(len(journal_meta)):\n",
    "            fulltext_path = os.path.join(full_text_folder, journal_issn, journal_meta.iloc[i]['unique_id'] + '.xml')\n",
    "            sections = extract_sections_and_text_from_xml(fulltext_path)\n",
    "            reorganized_sections = postprocess_sections(sections)\n",
    "            for section in reorganized_sections:\n",
    "                github_urls = extract_github_urls(section['text'])\n",
    "                for subsection in section['subsections']:\n",
    "                    github_urls.extend(extract_github_urls(subsection['text']))\n",
    "                    for subsubsection in subsection['subsubsections']:\n",
    "                        github_urls.extend(extract_github_urls(subsubsection['text']))\n",
    "            if github_urls:\n",
    "                print(github_urls)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/junzis/acsmc']\n",
      "['https://github.com/jsebanaz90/TRC_2019_867-SupplementaryMaterials']\n",
      "['https://github.com/junzis/openap', 'https://github.com/tudelft-cns-atm/bluesky']\n",
      "['https://github.com/Charles117/resilience_shenzhen']\n",
      "['https://github.com/eqasim-org/ile-de-france']\n",
      "['https://github.com/Amelia55/Adaptive-Control-System-for-UAM']\n",
      "['https://github.com/leeke2/tt-sync-scenarios']\n",
      "['https://github.com/HaoZhouGT/openpilot']\n",
      "['https://github.com/Yifanny/Generative_CF_Model_Conditioned_On_DS']\n",
      "['https://github.com/jiachaol/BMUE_TRC']\n",
      "['https://github.com/wn11-nyu/IRP-Drone']\n",
      "['https://github.com/descon-uccs/gould-trptc-2022']\n",
      "['https://github.com/Gengmaosi/PIT-IDM']\n",
      "['https://github.com/pabloguarda/pesuelogit']\n",
      "['https://github.com/geatpy-dev/geatpy', 'https://github.com/scipy/scipy']\n"
     ]
    }
   ],
   "source": [
    "for journal in journals:\n",
    "    if journal == '0968-090X.csv': # to take the trc dataset for test\n",
    "        journal_issn = journal.replace('.csv', '')\n",
    "        # journal_issn = journal_issn_df[journal_issn_df['journal'] == journal.replace('.csv', '')]['issn'].values[0]\n",
    "        journal_meta = pd.read_csv(os.path.join(meta_folder, journal))\n",
    "        journal_meta['unique_id'] = journal_meta['doi'].apply(doi_to_unique_id) # to convert the doi to a unique id\n",
    "        # here we use the for loop to help understand how it works, it can be done in one line of code later\n",
    "        for i in range(len(journal_meta)):\n",
    "            fulltext_path = os.path.join(full_text_folder, journal_issn, journal_meta.iloc[i]['unique_id'] + '.xml')\n",
    "            sections = extract_sections_and_text_from_xml(fulltext_path)\n",
    "            reorganized_sections = postprocess_sections(sections)\n",
    "            for section in reorganized_sections:\n",
    "                github_urls = extract_github_urls(section['text'])\n",
    "                for subsection in section['subsections']:\n",
    "                    github_urls.extend(extract_github_urls(subsection['text']))\n",
    "                    for subsubsection in subsection['subsubsections']:\n",
    "                        github_urls.extend(extract_github_urls(subsubsection['text']))\n",
    "            if github_urls:\n",
    "                print(github_urls)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RR-measure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
