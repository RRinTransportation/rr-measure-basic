
@article{altoeEnhancingStatisticalInference2020,
  title = {Enhancing {{Statistical Inference}} in {{Psychological Research}} via {{Prospective}} and {{Retrospective Design Analysis}}},
  author = {Altoè, Gianmarco and Bertoldo, Giulia and Zandonella Callegher, Claudio and Toffalini, Enrico and Calcagnì, Antonio and Finos, Livio and Pastore, Massimiliano},
  date = {2020},
  journaltitle = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02893},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.02893/full},
  urldate = {2020-01-29},
  abstract = {In the past two decades, psychological science has experienced an unprecedented replicability crisis, which has uncovered several issues. Among others, the use and misuse of statistical inference plays a key role in this crisis. Indeed, statistical inference is too often viewed as an isolated procedure limited to the analysis of data that have already been collected. Instead, statistical reasoning is necessary both at the planning stage and when interpreting the results of a research project. Based on these considerations, we build on and further develop an idea proposed by Gelman and Carlin (2014) termed “prospective and retrospective design analysis.” Rather than focusing only on the statistical significance of a result and on the classical control of type I and type II errors, a comprehensive design analysis involves reasoning about what can be considered a plausible effect size. Furthermore, it introduces two relevant inferential risks: the exaggeration ratio or Type M error (i.e., the predictable average overestimation of an effect that emerges as statistically significant) and the sign error or Type S error (i.e., the risk that a statistically significant effect is estimated in the wrong direction). Another important aspect of design analysis is that it can be usefully carried out both in the planning phase of a study and for the evaluation of studies that have already been conducted, thus increasing researchers’ awareness during all phases of a research project. To illustrate the benefits of a design analysis to the widest possible audience, we use a familiar example in psychology where the researcher is interested in analyzing the differences between two independent groups considering Cohen’s d as an effect size measure. We examine the case in which the plausible effect size is formalized as a single value, and we propose a method in which uncertainty concerning the magnitude of the effect is formalized via probability distributions. Through several examples and an application to a real case study, we show that, even though a design analysis requires significant effort, it has the potential to contribute to planning more robust and replicable studies. Finally, future developments in the Bayesian framework are discussed.},
  file = {/Users/claudio/MEGA/Zotero/Altoè et al_2020_Enhancing Statistical Inference in Psychological Research via Prospective and.pdf},
  langid = {english}
}

@article{andersonBestOftForgotten2019,
  title = {Best (but Oft Forgotten) Practices: Sample Size Planning for Powerful Studies},
  author = {Anderson, Samantha F},
  date = {2019},
  journaltitle = {The American Journal of Clinical Nutrition},
  shortjournal = {The American Journal of Clinical Nutrition},
  volume = {110},
  pages = {280--295},
  issn = {0002-9165},
  doi = {10.1093/ajcn/nqz058},
  url = {https://doi.org/10.1093/ajcn/nqz058},
  urldate = {2020-06-10},
  abstract = {Given recent concerns regarding replicability and trustworthiness in several areas of science, it is vital to encourage researchers to conduct statistically rigorous studies. Achieving a high level of statistical power is one particularly important domain in which researchers can improve the quality and reproducibility of their studies. Although several factors influence statistical power, appropriate sample size planning is often under the control of the researcher and can result in powerful studies. However, the process of conducting sample size planning to achieve a specified level of desired statistical power is often complex and the literature can be difficult to navigate. This article aims to provide an approachable overview of statistical power and sample size planning, with emphasis on why statistical power is important for high-quality science. Thorough examples relevant to nutrition researchers are included to illustrate the process of sample size planning. Special consideration is also given to issues that may arise when conducting sample size planning in practice. The overarching goal is to provide nutrition researchers with the tools and expertise needed to conduct effective sample size planning for future studies.},
  number = {2}
}

@article{andersonSampleSizePlanningMore2017,
  title = {Sample-{{Size Planning}} for {{More Accurate Statistical Power}}: {{A Method Adjusting Sample Effect Sizes}} for {{Publication Bias}} and {{Uncertainty}}},
  author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
  date = {2017},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {28},
  pages = {1547--1562},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797617723724},
  url = {https://doi.org/10.1177/0956797617723724},
  urldate = {2021-02-08},
  abstract = {The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.},
  number = {11}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  author = {Button, K.S. and Ioannidis, J.P.A. and Mokrysz, C. and Nosek, B.A. and Flint, J. and Robinson, E.S.J. and Munafò, M.R.},
  date = {2013},
  journaltitle = {Nature Reviews Neuroscience},
  volume = {14},
  pages = {365--376},
  doi = {10.1038/nrn3475},
  url = {https://doi.org/10.1038/nrn3475},
  langid = {english},
  number = {5}
}

@article{camererEvaluatingReplicabilityLaboratory2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  date = {2016},
  journaltitle = {Science},
  volume = {351},
  pages = {1433--1436},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aaf0918},
  url = {https://science.sciencemag.org/content/351/6280/1433},
  abstract = {Experimental economists have joined the reproducibility discussion by replicating selected published experiments from two top-tier journals in economics. Camerer et al. found that two-thirds of the 18 studies examined yielded replicable estimates of effect size and direction. This proportion is somewhat lower than unaffiliated experts were willing to bet in an associated prediction market, but roughly in line with expectations from sample sizes and P values.Science, this issue p. 1433The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs.},
  number = {6280}
}

@article{camererEvaluatingReplicabilitySocial2018,
  ids = {camerer2018a},
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  date = {2018},
  journaltitle = {Nature Human Behaviour},
  volume = {2},
  pages = {637--644},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  url = {http://www.nature.com/articles/s41562-018-0399-z},
  urldate = {2018-09-14},
  file = {/Users/claudio/MEGA/Zotero/Camerer et al_2018_Evaluating the replicability of social science experiments in Nature and.pdf},
  keywords = {Letto,Replicability},
  langid = {english},
  number = {9}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  date = {1988},
  publisher = {{Lawrence Erlbaum Associates}},
  url = {http://www.utstat.toronto.edu/~brunner/oldclass/378f16/readings/CohenPower.pdf},
  file = {/Users/claudio/MEGA/Zotero/Cohen_1988_Statistical power analysis for the behavioral sciences.pdf},
  isbn = {978-0-8058-0283-2},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  langid = {english},
  pagetotal = {567}
}

@article{cookAssessingMethodsSpecify2014,
  title = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial: {{DELTA}} ({{Difference ELicitation}} in {{TriAls}}) Review},
  author = {Cook, JA and Hislop, J and Adewuyi, TE and Harrild, K and Altman, DG and Ramsay, CR and Fraser, C and Buckley, B and Fayers, P and Harvey, I and Briggs, AH and Norrie, JD and Fergusson, D and Ford, I and Vale, LD},
  date = {2014-05-13},
  journaltitle = {Health Technol Assess},
  shortjournal = {Health Technol Assess},
  volume = {18},
  doi = {10.3310/hta18280},
  url = {http://journalslibrary.nihr.ac.uk/hta/hta18280},
  abstract = {Background: The randomised controlled trial (RCT) is widely considered to be the gold standard study for comparing the effectiveness of health interventions. Central to the design and validity of a RCT is a calculation of the number of participants needed (the sample size). The value used to determine the sample size can be considered the 'target difference'. From both a scientific and an ethical standpoint, selecting an appropriate target difference is of crucial importance. Determination of the target difference, as opposed to statistical approaches to calculating the sample size, has been greatly neglected though a variety of approaches have been proposed the current state of the evidence is unclear. The randomised controlled trial (RCT) is widely considered to be the gold standard study for comparing the effectiveness of health interventions. Central to the design and validity of a RCT is a calculation of the number of participants needed (the sample size). The value used to determine the sample size can be considered the 'target difference'. From both a scientific and an ethical standpoint, selecting an appropriate target difference is of crucial importance. Determination of the target difference, as opposed to statistical approaches to calculating the sample size, has been greatly neglected though a variety of approaches have been proposed the current state of the evidence is unclear. Objectives: The aim was to provide an overview of the current evidence regarding specifying the target difference in a RCT sample size calculation. The specific objectives were to conduct a systematic review of methods for specifying a target difference; to evaluate current practice by surveying triallists; to develop guidance on specifying the target difference in a RCT; and to identify future research needs. The aim was to provide an overview of the current evidence regarding specifying the target difference in a RCT sample size calculation. The specific objectives were to conduct a systematic review of methods for specifying a target difference; to evaluate current practice by surveying triallists; to develop guidance on specifying the target difference in a RCT; and to identify future research needs. Design: The biomedical and social science databases searched were MEDLINE, MEDLINE In-Process \& Other Non-Indexed Citations, EMBASE, Cochrane Central Register of Controlled Trials (CENTRAL), Cochrane Methodology Register, PsycINFO, Science Citation Index, EconLit, Education Resources Information Center (ERIC) and Scopus for in-press publications. All were searched from 1966 or the earliest date of the database coverage and searches were undertaken between November 2010 and January 2011. There were three interlinked components: (1) systematic review of methods for specifying a target difference for RCTs - a comprehensive search strategy involving an electronic literature search of biomedical and some non-biomedical databases and clinical trials textbooks was carried out; (2) identification of current trial practice using two surveys of triallists - members of the Society for Clinical Trials (SCT) were invited to complete an online survey and respondents were asked about their awareness and use of, and willingness to recommend, methods; one individual per triallist group [UK Clinical Research Collaboration (UKCRC)-registered Clinical Trials Units (CTUs), Medical Research Council (MRC) UK Hubs for Trials Methodology Research and National Institute for Health Research (NIHR) UK Research Design Services (RDS)] was invited to complete a survey; (3) production of a structured guidance document to aid the design of future trials - the draft guidance was developed utilising the results of the systematic review and surveys by the project steering and advisory groups. The biomedical and social science databases searched were MEDLINE, MEDLINE In-Process \& Other Non-Indexed Citations, EMBASE, Cochrane Central Register of Controlled Trials (CENTRAL), Cochrane Methodology Register, PsycINFO, Science Citation Index, EconLit, Education Resources Information Center (ERIC) and Scopus for in-press publications. All were searched from 1966 or the earliest date of the database coverage and searches were undertaken between November 2010 and January 2011. There were three interlinked components: (1) systematic review of methods for specifying a target difference for RCTs - a comprehensive search strategy involving an electronic literature search of biomedical and some non-biomedical databases and clinical trials textbooks was carried out; (2) identification of current trial practice using two surveys of triallists - members of the Society for Clinical Trials (SCT) were invited to complete an online survey and respondents were asked about their awareness and use of, and willingness to recommend, methods; one individual per triallist group [UK Clinical Research Collaboration (UKCRC)-registered Clinical Trials Units (CTUs), Medical Research Council (MRC) UK Hubs for Trials Methodology Research and National Institute for Health Research (NIHR) UK Research Design Services (RDS)] was invited to complete a survey; (3) production of a structured guidance document to aid the design of future trials - the draft guidance was developed utilising the results of the systematic review and surveys by the project steering and advisory groups. Setting: Methodological review incorporating electronic searches, review of books and guidelines, two surveys of experts (membership of an international society and UK- and Ireland-based triallists) and development of guidance. Methodological review incorporating electronic searches, review of books and guidelines, two surveys of experts (membership of an international society and UK- and Ireland-based triallists) and development of guidance. Participants: The two surveys were sent out to membership of the SCT and UK- and Ireland-based triallists. The two surveys were sent out to membership of the SCT and UK- and Ireland-based triallists. Interventions: The review focused on methods for specifying the target difference in a RCT. It was not restricted to any type of intervention or condition. The review focused on methods for specifying the target difference in a RCT. It was not restricted to any type of intervention or condition. Main outcome measures: Methods for specifying the target difference for a RCT were considered. Methods for specifying the target difference for a RCT were considered. Results: The search identified 11,485 potentially relevant studies. In total, 1434 were selected for full-text assessment and 777 were included in the review. Seven methods to specify the target difference for a RCT were identified - anchor, distribution, health economic, opinion-seeking, pilot study, review of evidence base (RoEB) and standardised effect size (SES) - each having important variations in implementation. A total of 216 of the included studies used more than one method. A total of 180 (15\%) responses to the SCT survey were received, representing 13 countries. Awareness of methods ranged from 38\% (n =69) for the health economic method to 90\% (n =162) for the pilot study. Of the 61 surveys sent out to UK triallist groups, 34 (56\%) responses were received. Awareness ranged from 97\% (n =33) for the RoEB and pilot study methods to only 41\% (n =14) for the distribution method. Based on the most recent trial, all bar three groups (91\%, n =30) used a formal method. Guidance was developed on the use of each method and the reporting of the sample size calculation in a trial protocol and results paper. The search identified 11,485 potentially relevant studies. In total, 1434 were selected for full-text assessment and 777 were included in the review. Seven methods to specify the target difference for a RCT were identified - anchor, distribution, health economic, opinion-seeking, pilot study, review of evidence base (RoEB) and standardised effect size (SES) - each having important variations in implementation. A total of 216 of the included studies used more than one method. A total of 180 (15\%) responses to the SCT survey were received, representing 13 countries. Awareness of methods ranged from 38\% (n =69) for the health economic method to 90\% (n =162) for the pilot study. Of the 61 surveys sent out to UK triallist groups, 34 (56\%) responses were received. Awareness ranged from 97\% (n =33) for the RoEB and pilot study methods to only 41\% (n =14) for the distribution method. Based on the most recent trial, all bar three groups (91\%, n =30) used a formal method. Guidance was developed on the use of each method and the reporting of the sample size calculation in a trial protocol and results paper. Conclusions: There is a clear need for greater use of formal methods to determine the target difference and better reporting of its specification. Raising the standard of RCT sample size calculations and the corresponding reporting of them would aid health professionals, patients, researchers and funders in judging the strength of the evidence and ensuring better use of scarce resources. There is a clear need for greater use of formal methods to determine the target difference and better reporting of its specification. Raising the standard of RCT sample size calculations and the corresponding reporting of them would aid health professionals, patients, researchers and funders in judging the strength of the evidence and ensuring better use of scarce resources. Funding: The Medical Research Council UK and the National Institute for Health Research Joint Methodology Research programme. The Medical Research Council UK and the National Institute for Health Research Joint Methodology Research programme.},
  number = {28}
}

@article{ebersoleManyLabsEvaluating2016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B.V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J.N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Sternglanz], R. [Weylin and Summerville, Amy and Tskhay, Konstantin O. and Allen], Zack [van and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  date = {2016},
  journaltitle = {Journal of Experimental Social Psychology},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  url = {http://www.sciencedirect.com/science/article/pii/S0022103115300123},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  keywords = {Cognitive psychology,Individual differences,Participant pool,Replication,Sampling effects,Situational effects,Social psychology}
}

@article{eisenbergerDoesRejectionHurt2003,
  title = {Does Rejection Hurt? {{An fMRI}} Study of Social Exclusion},
  author = {Eisenberger, Naomi I. and Lieberman, Matthew D. and Williams, Kipling D.},
  date = {2003},
  journaltitle = {Science},
  volume = {302},
  pages = {290--292},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1089134},
  url = {https://science.sciencemag.org/content/302/5643/290},
  abstract = {A neuroimaging study examined the neural correlates of social exclusion and tested the hypothesis that the brain bases of social pain are similar to those of physical pain. Participants were scanned while playing a virtual ball-tossing game in which they were ultimately excluded. Paralleling results from physical pain studies, the anterior cingulate cortex (ACC) was more active during exclusion than during inclusion and correlated positively with self-reported distress. Right ventral prefrontal cortex (RVPFC) was active during exclusion and correlated negatively with self-reported distress. ACC changes mediated the RVPFC-distress correlation, suggesting that RVPFC regulates the distress of social exclusion by disrupting ACC activity.},
  number = {5643}
}

@book{ellisEssentialGuideEffect2010,
  title = {The {{Essential Guide}} to {{Effect Sizes}}},
  author = {Ellis, Paul D},
  date = {2010},
  publisher = {{Cambridge University Press}},
  url = {https://doi.org/10.1017/CBO9780511761676},
  file = {/Users/claudio/MEGA/Zotero/Ellis_2010_The Essential Guide to Effect Sizes.pdf},
  keywords = {Letto Parzialmente},
  langid = {english}
}

@article{fisherFrequencyDistributionValues1915,
  title = {Frequency {{Distribution}} of the {{Values}} of the {{Correlation Coefficient}} in {{Samples}} from an {{Indefinitely Large Population}}},
  author = {Fisher, R. A.},
  date = {1915},
  journaltitle = {Biometrika},
  volume = {10},
  pages = {507},
  issn = {00063444},
  doi = {10.2307/2331838},
  eprint = {2331838},
  eprinttype = {jstor},
  file = {/Users/claudio/MEGA/Zotero/Fisher_1915_Frequency Distribution of the Values of the Correlation Coefficient in Samples.pdf},
  langid = {english},
  number = {4}
}

@online{fordAssessingTypeType2018,
  title = {Assessing {{Type S}} and {{Type M Errors}}},
  author = {Ford, Clay},
  date = {2018},
  url = {https://data.library.virginia.edu/assessing-type-s-and-type-m-errors/},
  urldate = {2020-04-28},
  file = {/Users/claudio/Zotero/storage/CU657DHD/assessing-type-s-and-type-m-errors.html},
  organization = {{University of Virginia Library}}
}

@article{francoPublicationBiasSocial2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  date = {2014},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {345},
  pages = {1502},
  doi = {10.1126/science.1255484},
  url = {http://science.sciencemag.org/content/345/6203/1502.abstract},
  abstract = {Experiments that produce null results face a higher barrier to publication than those that yield statistically significant differences. Whether this is a problem depends on how many null but otherwise valid results might be trapped in the file drawer. Franco et al. use a Time-sharing Experiments in the Social Sciences archive of nearly 250 peer-reviewed proposals of social science experiments conducted on nationally representative samples. They find that only 10 out of 48 null results were published, whereas 56 out of 91 studies with strongly significant results made it into a journal.Science, this issue p. 1502 We studied publication bias in the social sciences by analyzing a known population of conducted studies—221 in total—in which there is a full accounting of what is published and unpublished. We leveraged Time-sharing Experiments in the Social Sciences (TESS), a National Science Foundation–sponsored program in which researchers propose survey-based experiments to be run on representative samples of American adults. Because TESS proposals undergo rigorous peer review, the studies in the sample all exceed a substantial quality threshold. Strong results are 40 percentage points more likely to be published than are null results and 60 percentage points more likely to be written up. We provide direct evidence of publication bias and identify the stage of research production at which publication bias occurs: Authors do not write up and submit null findings.},
  number = {6203}
}

@article{gelmanDonCalculatePosthoc2019a,
  title = {Don’t {{Calculate Post}}-Hoc {{Power Using Observed Estimate}} of {{Effect Size}}},
  author = {Gelman, Andrew},
  date = {2019},
  journaltitle = {Annals of Surgery},
  volume = {269},
  issn = {0003-4932},
  url = {https://journals.lww.com/annalsofsurgery/Fulltext/2019/01000/Don_t_Calculate_Post_hoc_Power_Using_Observed.46.aspx},
  number = {1}
}

@unpublished{gelmanEmbracingVariationAccepting2019,
  title = {Embracing {{Variation}} and {{Accepting Uncertainty}}: {{Implications}} for {{Science}} and {{Metascience}}},
  author = {Gelman, Andrew},
  date = {2019},
  url = {https://www.metascience2019.org/presentations/andrew-gelman/},
  urldate = {2020-04-28},
  abstract = {Full video of Andrew Gelman's presentation "Embracing Variation and Accepting Uncertainty: Implications for Science and Metascience" at the Metascience 2019 Symposium at Stanford.},
  eventtitle = {Metascience: {{The Emerging Field}} of {{Research}} on the Scientific {{Process}}},
  file = {/Users/claudio/Zotero/storage/3E89279A/andrew-gelman.html},
  langid = {american},
  type = {Video}
}

@article{gelmanFailureNullHypothesis2018,
  title = {The {{Failure}} of {{Null Hypothesis Significance Testing When Studying Incremental Changes}}, and {{What}} to {{Do About It}}},
  author = {Gelman, Andrew},
  date = {2018},
  journaltitle = {Personality and Social Psychology Bulletin},
  volume = {44},
  pages = {16--23},
  issn = {0146-1672, 1552-7433},
  doi = {10.1177/0146167217729162},
  url = {http://journals.sagepub.com/doi/10.1177/0146167217729162},
  urldate = {2019-02-28},
  abstract = {A standard mode of inference in social and behavioral science is to establish stylized facts using statistical significance in quantitative studies. However, in a world in which measurements are noisy and effects are small, this will not work: selection on statistical significance leads to effect sizes which are overestimated and often in the wrong direction. After a brief discussion of two examples, one in economics and one in social psychology, we consider the procedural solution of open post-publication review, the design solution of devoting more effort to accurate measurements and within-person comparisons, and the statistical analysis solution of multilevel modeling and reporting all results rather than selection on significance. We argue that the current replication crisis in science arises in part from the ill effects of null hypothesis significance testing being used to study small effects with noisy data. In such settings, apparent success comes easy but truly replicable results require a more serious connection between theory, measurement, and data.},
  file = {/Users/claudio/MEGA/Zotero/Gelman_2018_The Failure of Null Hypothesis Significance Testing When Studying Incremental.pdf},
  keywords = {Bayesian,Letto,NHST,Replicability,Research Practice},
  langid = {english},
  number = {1}
}

@online{gelmanOverconfidenceResearchCertainty2019,
  title = {From {{Overconfidence}} in {{Research}} to {{Over Certainty}} in {{Policy Analysis}}: {{Can We Escape}} the {{Cycle}} of {{Hype}} and {{Disappointment}}?},
  shorttitle = {From {{Overconfidence}} in {{Research}} to {{Over Certainty}} in {{Policy Analysis}}},
  author = {Gelman, Andrew},
  date = {2019},
  url = {http://newamerica.org/public-interest-technology/blog/overconfidence-research-over-certainty-policy-analysis-can-we-escape-cycle-hype-and-disappointment/},
  urldate = {2020-05-29},
  file = {/Users/claudio/Zotero/storage/CRTPPN4Z/overconfidence-research-over-certainty-policy-analysis-can-we-escape-cycle-hype-and-disappointm.html},
  langid = {english},
  organization = {{New America}}
}

@article{gelmanPowerCalculationsAssessing2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  date = {2014},
  journaltitle = {Perspectives on Psychological Science},
  volume = {9},
  pages = {641--651},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614551642},
  url = {http://journals.sagepub.com/doi/10.1177/1745691614551642},
  urldate = {2018-10-25},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  file = {/Users/claudio/MEGA/Zotero/Gelman_Carlin_2014_Beyond Power Calculations.pdf},
  keywords = {Effect Size,Letto,Power Analysis,Type S and Type M errror},
  langid = {english},
  number = {6}
}

@unpublished{gelmanRetrospectiveDesignAnalysis2013,
  title = {Retrospective Design Analysis Using External Information},
  author = {Gelman, Andrew and Carlin, John},
  date = {2013},
  url = {http://www.stat.columbia.edu/~gelman/research/unpublished/retropower5.pdf},
  urldate = {2020-04-28},
  file = {/Users/claudio/MEGA/Zotero/Gelman_Carlin_2013_Retrospective design analysis using external information.pdf},
  howpublished = {Unpublished},
  type = {Unpublished}
}

@article{gelmanStatisticalCrisisScience2014,
  title = {The Statistical Crisis in Science},
  author = {Gelman, Andrew and Loken, Eric},
  date = {2014},
  journaltitle = {American scientist},
  volume = {102},
  pages = {460--466},
  publisher = {{Sigma Xi, The Scientific Research Society}},
  doi = {10.1511/2014.111.460},
  url = {https://doi.org/10.1511/2014.111.460},
  number = {6}
}

@article{gelmanTypeErrorMight2017,
  title = {Type {{M Error Might Explain Weisburd}}’s {{Paradox}}},
  author = {Gelman, Andrew and Skardhamar, Torbjørn and Aaltonen, Mikko},
  date = {2017},
  journaltitle = {Journal of Quantitative Criminology},
  issn = {0748-4518, 1573-7799},
  doi = {10.1007/s10940-017-9374-5},
  url = {http://link.springer.com/10.1007/s10940-017-9374-5},
  urldate = {2020-04-28},
  abstract = {Objectives Simple calculations seem to show that larger studies should have higher statistical power, but empirical meta-analyses of published work in criminology have found zero or weak correlations between sample size and estimated statistical power. This is ‘‘Weisburd’s paradox’’ and has been attributed by Weisburd et al. (in Crime Justice 17:337–379, 1993) to a difficulty in maintaining quality control as studies get larger, and attributed by Nelson et al. (in J Exp Criminol 11:141–163, 2015) to a negative correlation between sample sizes and the underlying sizes of the effects being measured. We argue against the necessity of both these explanations, instead suggesting that the apparent Weisburd paradox might be explainable as an artifact of systematic overestimation inherent in post-hoc power calculations, a bias that is large with small N. Methods We discuss Weisburd’s paradox in light of the concepts of type S and type M errors, and re-examine the publications used in previous studies of the so-called paradox. Results We suggest that the apparent Weisburd paradox might be explainable as an artifact of systematic overestimation inherent in post-hoc power calculations, a bias that is large with small N. Conclusions Speaking more generally, we recommend abandoning the use of statistical power as a measure of the strength of a study, because implicit in the definition of power is the bad idea of statistical significance as a research goal.},
  file = {/Users/claudio/MEGA/Zotero/Gelman et al_2017_Type M Error Might Explain Weisburd’s Paradox.pdf},
  langid = {english}
}

@article{gelmanTypeErrorRates2000,
  title = {Type {{S}} Error Rates for Classical and {{Bayesian}} Single and Multiple Comparison Procedures},
  author = {Gelman, Andrew and Tuerlinckx, Francis},
  date = {2000},
  journaltitle = {Computational Statistics},
  volume = {15},
  pages = {373--390},
  issn = {09434062},
  doi = {10.1007/s001800000040},
  url = {http://link.springer.com/10.1007/s001800000040},
  urldate = {2019-04-03},
  abstract = {In classical statistics, the signi cance of comparisons (e.g., 1 ? 2) is calibrated using the Type 1 error rate, relying on the assumption that the true di erence is zero, which makes no sense in many applications. We set up a more relevant framework in which a true comparison can be positive or negative, and, based on the data, you can state \textbackslash{} 1 {$>$} 2 with con dence," \textbackslash{} 2 {$>$} 1 with con dence," or \textbackslash no claim with con dence." We focus on the Type S (for sign) error, which occurs when you claim \textbackslash{} 1 {$>$} 2 with con dence" when 2 {$>$} 1 (or vice-versa). We compute the Type S error rates for classical and Bayesian con dence statements and nd that classical Type S error rates can be extremely high (up to 50\%). Bayesian con dence statements are conservative, in the sense that claims based on 95\% posterior intervals have Type S error rates between 0 and 2.5\%. For multiple comparison situations, the conclusions are similar.},
  file = {/Users/claudio/MEGA/Zotero/Gelman_Tuerlinckx_2000_Type S error rates for classical and Bayesian single and multiple comparison.pdf},
  langid = {english},
  number = {3}
}

@inbook{gigerenzerNullRitualWhat2004,
  title = {The {{Null Ritual}}: {{What You Always Wanted}} to {{Know About Significance Testing}} but {{Were Afraid}} to {{Ask}}},
  shorttitle = {The {{Null Ritual}}},
  booktitle = {The {{SAGE Handbook}} of {{Quantitative Methodology}} for the {{Social Sciences}}},
  author = {Gigerenzer, Gerd and Krauss, Stefan and Vitouch, Oliver},
  date = {2004},
  pages = {392--409},
  publisher = {{SAGE Publications, Inc.}},
  location = {{2455 Teller Road,~Thousand Oaks~California~91320~United States of America}},
  doi = {10.4135/9781412986311.n21},
  url = {http://methods.sagepub.com/book/the-sage-handbook-of-quantitative-methodology-for-the-social-sciences/n21.xml},
  urldate = {2019-07-23},
  bookauthor = {Kaplan, David},
  file = {/Users/claudio/MEGA/Zotero/Gigerenzer et al_2004_The Null Ritual.pdf},
  isbn = {978-0-7619-2359-6 978-1-4129-8631-1},
  langid = {english}
}

@article{goodmanUsePredictedConfidence1994a,
  title = {The Use of Predicted Confidence Intervals When Planning Experiments and the Misuse of Power When Interpreting Results},
  author = {Goodman, SN and Berlin, JA},
  date = {1994-08},
  journaltitle = {Annals of internal medicine},
  shortjournal = {Annals of internal medicine},
  volume = {121},
  pages = {200--206},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-121-3-199408010-00008},
  url = {http://europepmc.org/abstract/MED/8017747},
  abstract = {Although there is a growing understanding of the importance of statistical power considerations when designing studies and of the value of confidence intervals when interpreting data, confusion exists about the reverse arrangement: the role of confidence intervals in study design and of power in interpretation. Confidence intervals should play an important role when setting sample size, and power should play no role once the data have been collected, but exactly the opposite procedure is widely practiced. In this commentary, we present the reasons why the calculation of power after a study is over is inappropriate and how confidence intervals can be used during both study design and study interpretation.},
  eprint = {8017747},
  eprinttype = {pubmed},
  keywords = {Confidence Intervals,Data Interpretation; Statistical,Research Design},
  langid = {english},
  number = {3}
}

@article{ioannidisEmergenceLargeTreatment2013,
  title = {Emergence of {{Large Treatment Effects From Small Trials}}—{{Reply}}},
  author = {Ioannidis, John P. A. and Pereira, Tiago V. and Horwitz, Ralph I.},
  date = {2013},
  journaltitle = {JAMA},
  shortjournal = {JAMA},
  volume = {309},
  pages = {768--769},
  issn = {0098-7484},
  doi = {10.1001/jama.2012.208831},
  url = {https://doi.org/10.1001/jama.2012.208831},
  urldate = {2020-04-28},
  abstract = {In Reply: We agree with Drs Batterham and Hopkins that small studies are not necessarily inherently flawed. However, probabilistically speaking, even in the absence of biases, small-sized trials are more prone to provide overestimates (or underestimates) compared with larger trials.Thus, evidence from scattered small studies is easier to distort than evidence from large trials because analyses with the most impressive results are more likely to be published compared with studies showing underestimated treatment effects.},
  number = {8}
}

@article{ioannidisWhyMostDiscovered2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}:},
  shorttitle = {Why {{Most Discovered True Associations Are Inflated}}},
  author = {Ioannidis, John P. A.},
  date = {2008},
  journaltitle = {Epidemiology},
  volume = {19},
  pages = {640--648},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e31818131e7},
  url = {http://journals.lww.com/00001648-200809000-00002},
  urldate = {2020-04-28},
  abstract = {Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated—for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.},
  file = {/Users/claudio/MEGA/Zotero/Ioannidis_2008_Why Most Discovered True Associations Are Inflated.pdf},
  langid = {english},
  number = {5}
}

@article{kleinInvestigatingVariationReplicability2014,
  title = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and van ‘t Veer, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  date = {2014},
  journaltitle = {Social Psychology},
  shortjournal = {Social Psychology},
  volume = {45},
  pages = {142--152},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000178},
  url = {https://econtent.hogrefe.com/doi/full/10.1027/1864-9335/a000178},
  urldate = {2019-01-07},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  file = {/Users/claudio/MEGA/Zotero/Klein et al_2014_Investigating Variation in Replicability.pdf},
  number = {3},
  options = {useprefix=true}
}

@article{kleinManyLabsInvestigating2018,
  title = {Many Labs 2: {{Investigating}} Variation in Replicability across Samples and Settings},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and de Bruijn, Maaike and Schutter, Leander De and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Nichols, Austin Lee and Ocampo, Aaron and O’Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van ’t Veer, Anna Elisabeth and Echeverría, Alejandro Vásquez- and Vaughn, Leigh Ann and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  date = {2018},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  url = {https://doi.org/10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p ¡ .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p ¡ .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen’s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (¡ 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  number = {4},
  options = {useprefix=true}
}

@book{kurkiewiczDocstringProvidesDocstring2017,
  title = {Docstring: {{Provides}} Docstring Capabilities to r Functions},
  author = {Kurkiewicz, Dason},
  date = {2017},
  url = {https://CRAN.R-project.org/package=docstring}
}

@article{lakensEquivalenceTestingPsychological2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
  date = {2018-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {259--269},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  url = {https://doi.org/10.1177/2515245918770963},
  urldate = {2021-03-15},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  number = {2}
}

@article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and Gonzalez-Marquez, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and van Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavský, Jiří and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and de Oliveira, Cilene Lino and de Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and Świątkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  date = {2018},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nature Human Behaviour},
  volume = {2},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  url = {https://doi.org/10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  number = {3},
  options = {useprefix=true}
}

@report{lakensValuePreregistrationPsychological2019,
  title = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}: {{A Conceptual Analysis}}},
  shorttitle = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}},
  author = {Lakens, Daniel},
  date = {2019},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/jbh4w},
  url = {https://osf.io/jbh4w},
  urldate = {2020-03-16},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
  file = {/Users/claudio/MEGA/Zotero/Lakens_2019_The Value of Preregistration for Psychological Science.pdf},
  keywords = {Letto},
  langid = {english},
  type = {preprint}
}

@article{laneEstimatingEffectSize1978,
  title = {Estimating Effect Size: {{Bias}} Resulting from the Significance Criterion in Editorial Decisions},
  author = {Lane, David M and Dunlap, William P},
  date = {1978},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  volume = {31},
  pages = {107--112},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/j.2044-8317.1978.tb00578.x},
  url = {https://doi.org/10.1111/j.2044-8317.1978.tb00578.x},
  number = {2}
}

@article{luNoteTypeErrors2018,
  title = {A Note on {{Type S}}/{{M}} Errors in Hypothesis Testing},
  author = {Lu, Jiannan and Qiu, Yixuan and Deng, Alex},
  date = {2018},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  issn = {00071102},
  doi = {10.1111/bmsp.12132},
  url = {http://doi.wiley.com/10.1111/bmsp.12132},
  urldate = {2018-10-25},
  file = {/Users/claudio/MEGA/Zotero/Lu et al_2018_A note on Type S-M errors in hypothesis testing.pdf},
  keywords = {Effect Size,Letto,Power Analysis,Type S and Type M errror},
  langid = {english}
}

@book{mayoStatisticalInferenceSevere2018,
  title = {Statistical {{Inference}} as {{Severe Testing}}: {{How}} to {{Get Beyond}} the {{Statistics Wars}}},
  shorttitle = {Statistical {{Inference}} as {{Severe Testing}}},
  author = {Mayo, Deborah G.},
  date = {2018},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781107286184},
  url = {https://www.cambridge.org/core/product/identifier/9781107286184/type/book},
  urldate = {2020-01-27},
  file = {/Users/claudio/MEGA/Zotero/Mayo_2018_Statistical Inference as Severe Testing.pdf},
  isbn = {978-1-107-28618-4 978-1-107-05413-4 978-1-107-66464-7},
  langid = {english}
}

@article{ohaganExpertKnowledgeElicitation2019,
  title = {Expert {{Knowledge Elicitation}}: {{Subjective}} but {{Scientific}}},
  shorttitle = {Expert {{Knowledge Elicitation}}},
  author = {O’Hagan, Anthony},
  date = {2019-03-29},
  journaltitle = {The American Statistician},
  volume = {73},
  pages = {69--81},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1518265},
  url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518265},
  urldate = {2019-03-29},
  abstract = {Expert opinion and judgment enter into the practice of statistical inference and decision-making in numerous ways. Indeed, there is essentially no aspect of scientific investigation in which judgment is not required. Judgment is necessarily subjective, but should be made as carefully, as objectively, and as scientifically as possible.},
  file = {/Users/claudio/MEGA/Zotero/O’Hagan_2019_Expert Knowledge Elicitation.pdf},
  issue = {sup1},
  keywords = {Letto},
  langid = {english}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  date = {2015},
  journaltitle = {Science},
  volume = {349},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716},
  urldate = {2019-01-07},
  file = {/Users/claudio/MEGA/Zotero/Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf},
  langid = {english},
  number = {6251}
}

@article{phillipsStatisticalSignificanceSediment2001,
  title = {Statistical Significance of Sediment Toxicity Test Results: {{Threshold}} Values Derived by the Detectable Significance Approach},
  author = {Phillips, Bryn M. and Hunt, John W. and Anderson, Brian S. and Puckett, H. Max and Fairey, Russell and Wilson, Craig J. and Tjeerdema, Ron},
  date = {2001-02-01},
  journaltitle = {Environmental Toxicology and Chemistry},
  shortjournal = {Environmental Toxicology and Chemistry},
  volume = {20},
  pages = {371--373},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0730-7268},
  doi = {10.1002/etc.5620200218},
  url = {https://doi.org/10.1002/etc.5620200218},
  urldate = {2021-02-07},
  abstract = {Abstract A number of methods have been employed to determine the statistical significance of sediment toxicity test results. To allow consistency among comparisons, regardless of among-replicate variability, a protocol-specific approach has been used that considers protocol performance over a large number of comparisons. Ninetieth-percentile minimum significant difference (MSD) values were calculated to determine a critical threshold for statistically significant sample toxicity. Significant toxicity threshold values (as a percentage of laboratory control values) are presented for six species and nine endpoints based on data from as many as 720 stations. These threshold values are useful for interpreting sediment toxicity data from large studies and in eliminating cases where statistical significance is assigned in individual cases because among-replicate variability is small.},
  keywords = {Minimum significant difference,Sediment toxicity,Statistics},
  number = {2}
}

@incollection{protzkoDeclineEffectsTypes2017,
  title = {Decline Effects: {{Types}}, Mechanisms, and Personal Reflections.},
  booktitle = {Psychological Science under Scrutiny: {{Recent}} Challenges and Proposed Solutions.},
  author = {Protzko, John and Schooler, Jonathan W.},
  date = {2017},
  pages = {85--107},
  publisher = {{Wiley-Blackwell}},
  doi = {10.1002/9781119095910.ch6},
  abstract = {In this chapter, we consider four general types of declining effect sizes, each of which relates to the hypothetical true effect size of the finding in question at the time it was originally reported. False positive decline effects occur when there actually was no true effect when the research was conducted, initially reported positive findings were instead a statistical or methodological artifact. Inflated decline effects occur when a true effect did exist but the initially reported studies artificially inflated the estimate of its size. Under-specified decline effects occur when a true effect originally existed but its necessary conditions were under-specified, as a result subsequent studies faded to include those conditions and thereby observed smaller effects. Finally, genuinely decreasing decline effects occur when the true effect size was originally and accurately reported but, for some reason, the true effect genuinely declines in magnitude over time. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {978-1-118-66107-9 (Paperback); 978-1-118-66108-6 (PDF); 978-1-118-66104-8 (EPUB)},
  keywords = {*Effect Size (Statistical),*Methodology,Statistical Analysis}
}

@article{rohrerThinkingClearlyCorrelations2018,
  title = {Thinking Clearly about Correlations and Causation: {{Graphical}} Causal Models for Observational Data},
  author = {Rohrer, Julia M.},
  date = {2018},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {27--42},
  doi = {10.1177/2515245917745629},
  url = {https://doi.org/10.1177/2515245917745629},
  abstract = {Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables—colliders and mediators—should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors.},
  eprint = {https://doi.org/10.1177/2515245917745629},
  number = {1}
}

@article{schoolerTurningLensScience2014,
  title = {Turning the {{Lens}} of {{Science}} on {{Itself}}: {{Verbal Overshadowing}}, {{Replication}}, and {{Metascience}}},
  author = {Schooler, J.W.},
  date = {2014},
  journaltitle = {Perspectives on Psychological Science},
  volume = {9},
  pages = {579--584},
  doi = {10.1177/1745691614547878},
  url = {https://doi.org/10.1177/1745691614547878},
  langid = {english},
  number = {5}
}

@article{vasishthStatisticalSignificanceFilter2018,
  title = {The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability},
  author = {Vasishth, Shravan and Mertzen, Daniela and Jäger, Lena A. and Gelman, Andrew},
  date = {2018},
  journaltitle = {Journal of Memory and Language},
  volume = {103},
  pages = {151--175},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2018.07.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X18300640},
  abstract = {It is well-known in statistics (e.g., Gelman \& Carlin, 2014) that treating a result as publishable just because the p-value is less than 0.05 leads to overoptimistic expectations of replicability. These effects get published, leading to an overconfident belief in replicability. We demonstrate the adverse consequences of this statistical significance filter by conducting seven direct replication attempts (268 participants in total) of a recent paper (Levy \& Keller, 2013). We show that the published claims are so noisy that even non-significant results are fully compatible with them. We also demonstrate the contrast between such small-sample studies and a larger-sample study; the latter generally yields a less noisy estimate but also a smaller effect magnitude, which looks less compelling but is more realistic. We reiterate several suggestions from the methodology literature for improving current practices.},
  keywords = {Bayesian data analysis,Expectation,Locality,Parameter estimation,Replicability,Surprisal,Type M error}
}

@book{venablesModernAppliedStatistics2002,
  ids = {venablesModernAppliedStatistics2002a},
  title = {Modern {{Applied Statistics}} with {{S}}},
  author = {Venables, W N and Ripley, B D},
  date = {2002},
  publisher = {{Springer}},
  url = {https://cran.r-project.org/web/packages/MASS/index.html},
  file = {/Users/claudio/MEGA/Zotero/Venables_Ripley_2002_Modern Applied Statistics with S.pdf},
  langid = {english}
}

@article{vulPuzzlinglyHighCorrelations2009,
  title = {Puzzlingly High Correlations in {{fMRI}} Studies of Emotion, Personality, and Social Cognition},
  author = {Vul, Edward and Harris, Christine and Winkielman, Piotr and Pashler, Harold},
  date = {2009},
  journaltitle = {Perspectives on Psychological Science},
  volume = {4},
  pages = {274--290},
  doi = {10.1111/j.1745-6924.2009.01125.x},
  url = {https://doi.org/10.1111/j.1745-6924.2009.01125.x},
  abstract = {Functional magnetic resonance imaging (fMRI) studiesofemotion, personality, and social cognition have drawn much attention in recent years, with high-profile studies frequently reporting extremely high (e.g., ¿.8) correlations between brain activation and personality measures. We show that these correlations are higher than should be expected given the (evidently limited) reliability of both fMRI and personality measures. The high correlations are all the more puzzling because method sections rarely contain much detail about how the correlations were obtained. We surveyed authors of 55 articles that reported findings of this kind to determine a few details on how these correlations were computed. More than half acknowledged using a strategy that computes separate correlations for individual voxels and reports means of only those voxels exceeding chosen thresholds. We show how this nonindependent analysis inflates correlations while yielding reassuring-looking scattergrams. This analysis technique was used to obtain the vast majority of the implausibly high correlations in our survey sample. In addition, we argue that, in some cases, other analysis problems likely created entirely spurious correlations. We outline how the data from these studies could be reanalyzed with unbiased methods to provide accurate estimates of the correlations in question and urge authors to perform such reanalyses. The underlying problems described here appear to be common in fMRI research of many kinds—not just in studies of emotion, personality, and social cognition.},
  number = {3}
}

@incollection{vulSuspiciouslyHighCorrelations2017,
  title = {Suspiciously High Correlations in Brain Imaging Research},
  booktitle = {Psychological Science under Scrutiny},
  author = {Vul, Edward and Pashler, Harold},
  date = {2017},
  pages = {196--220},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119095910.ch11},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119095910.ch11},
  abstract = {Summary Blood-oxygen-level dependent (BOLD) activity in a small region of the brain accounted for the great majority of the variance in speed with which subjects walk out of the experiment. While trying to estimate the maximum plausible population correlation between fMRI measures and social behavior based on psychometric considerations, it seemed that the upper bound should be around 0.75. A single massively multivariate analysis in a whole-brain, across-subject fMRI experiment is analogous to a whole field carrying out many experiments. Publication bias inflates the effect sizes in a given field by filtering many executed studies to get just those that passed a significance threshold. The challenge of whole-brain fMRI parallels the challenge faced by genetics and domains in which the candidate pool of variables exceeds the number of independent measurements. In non-independent whole-brain correlation studies, instead of reporting the average correlation of a detected cluster, the investigators instead report the “peak voxel” from that cluster.},
  chapter = {11},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119095910.ch11},
  isbn = {978-1-119-09591-0},
  keywords = {blood-oxygen-level dependent activity,brain imaging research,fMRI,non-independent whole-brain correlation studies,population correlation,psychometric considerations,publication bias,social neuroscience}
}

@article{yarkoniBigCorrelationsLittle2009,
  title = {Big {{Correlations}} in {{Little Studies}}: {{Inflated fMRI Correlations Reflect Low Statistical Power}}—{{Commentary}} on {{Vul}} et al. (2009)},
  author = {Yarkoni, Tal},
  date = {2009},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {4},
  pages = {294--298},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1111/j.1745-6924.2009.01127.x},
  url = {https://doi.org/10.1111/j.1745-6924.2009.01127.x},
  urldate = {2020-04-28},
  abstract = {Vul, Harris, Winkielman, and Pashler (2009), (this issue) argue that correlations in many cognitive neuroscience studies are grossly inflated due to a widespread tendency to use nonindependent analyses. In this article, I argue that Vul et al.'s primary conclusion is correct, but for different reasons than they suggest. I demonstrate that the primary cause of grossly inflated correlations in whole-brain fMRI analyses is not nonindependence, but the pernicious combination of small sample sizes and stringent alpha-correction levels. Far from defusing Vul et al.'s conclusions, the simulations presented suggest that the level of inflation may be even worse than Vul et al.'s empirical analysis would suggest.},
  number = {3}
}

@article{youngWhyCurrentPublication2008,
  title = {Why Current Publication Practices May Distort Science},
  author = {Young, Neal S and Ioannidis, John P. A and Al-Ubaydli, Omar},
  date = {2008},
  journaltitle = {PLOS Medicine},
  volume = {5},
  pages = {1--5},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pmed.0050201},
  url = {https://doi.org/10.1371/journal.pmed.0050201},
  abstract = {John Ioannidis and colleagues argue that the current system of publication in biomedical research provides a distorted view of the reality of scientific data.},
  number = {10}
}


