{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=============================== c = 0.0\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9055\n","minRMSE =     0.2442\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 174\n","maxR2 =     0.6940\n","minRMSE =     0.3688\n","=============================== c = 0.1\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9166\n","minRMSE =     0.2281\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 174\n","maxR2 =     0.7132\n","minRMSE =     0.3559\n","=============================== c = 0.2\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9267\n","minRMSE =     0.2126\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 174\n","maxR2 =     0.7312\n","minRMSE =     0.3438\n","=============================== c = 0.3\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9358\n","minRMSE =     0.1980\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 174\n","maxR2 =     0.7481\n","minRMSE =     0.3323\n","=============================== c = 0.4\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9439\n","minRMSE =     0.1843\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 174\n","maxR2 =     0.7636\n","minRMSE =     0.3217\n","=============================== c = 0.5\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9508\n","minRMSE =     0.1720\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 178\n","maxR2 =     0.7778\n","minRMSE =     0.3121\n","=============================== c = 0.6\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9566\n","minRMSE =     0.1611\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 178\n","maxR2 =     0.7905\n","minRMSE =     0.3034\n","=============================== c = 0.7\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9611\n","minRMSE =     0.1522\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 178\n","maxR2 =     0.8017\n","minRMSE =     0.2959\n","=============================== c = 0.8\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9643\n","minRMSE =     0.1454\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 178\n","maxR2 =     0.8114\n","minRMSE =     0.2895\n","=============================== c = 0.9\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9663\n","minRMSE =     0.1413\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 199\n","maxR2 =     0.8194\n","minRMSE =     0.2844\n","=============================== c = 1.0\n","-- TRAIN: the best R^2 and RMSE values with according iteration number --\n","maxM = 200\n","maxR2 =     0.9669\n","minRMSE =     0.1398\n","-- TEST: the best R^2 and RMSE values with according iteration number --\n","maxM = 199\n","maxR2 =     0.8259\n","minRMSE =     0.2806\n"]}],"source":"# Fuzzy Gradient Boosting Regression Algorithm\n# using different defuzzification methods\n# by Resmiye Nasiboglu and Efendi Nasibov\n# August, 2022\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport seaborn as sns\nimport numpy as np\nimport random as r\nimport pandas as pd\nimport math\nfrom sklearn import preprocessing\nfrom my_datasets import *\nfrom fuzzy_operations import *\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n    \ndef c_cycle(c):\n    # c is the optimism parameter of the WABL method.\n    print(\"=============================== c = %.1f\" % (c))\n    \n    \"\"\"\n    ===== DATASETS =====\n    my_load_iris()\n    my_load_car_prices()\n    my_load_diabetes()\n    my_load_boston()\n    my_load_penguins()\n    my_load_planets()\n    my_load_diamonds()\n    my_load_mpg()\n    my_load_tips()\n    my_load_taxis()\n    \"\"\"\n    \n    # replace with the required my_load_...() function\n    X_train,X_test,y_train,y_test = my_load_iris()\n\n    r.seed(0) # initialization of the random generator.\n\n    # fuzzy number representation: A=(mode,l_width,r_width)\n\n    # in experiments the folowing forms of the FN are used\n    #     l_max_width=0.2 and r_max_witth=0.2  (symmetrical case)\n    #     l_max_width=0.2 and r_max_witth=0.0  (left skewned case)\n    #     l_max_width=0.0 and r_max_witth=0.2  (right skewned case)\n    \n    l_max_width=0.2  # left max width of fuzzyness\n    r_max_width=0.0  # right max width of fuzzyness\n\n    # reservation of empty fuzzy data \n    y_fuz_train=[[0,0,0] for _ in range(len(y_train))]\n    y_fuz_test=[[0,0,0] for _ in range(len(y_test))]\n    \n    # generation of random fuzzy data \n    for i in range(len(y_train)):\n        y_fuz_train[i] = [y_train[i],y_train[i]*(l_max_width*r.random()),y_train[i]*(r_max_width*r.random())]\n    for i in range(len(y_test)):    \n        y_fuz_test[i] = [y_test[i],y_test[i]*(l_max_width*r.random()),y_test[i]*(r_max_width*r.random())]\n    \n   \n    # Standartization of the inputs\n\n    sc = MinMaxScaler()\n    X_train_std = sc.fit_transform(X_train)\n    X_test_std = sc.transform(X_test)\n\n    # -------- Parameters ----------\n    M=201  # number of boosting iterations\n    learning_rate=0.1 \n    tree_depth=1  # depth of the stump trees\n    max_leaf=2**tree_depth  # maximum leaf number of the stump trees\n\n    F=[[[0,0,0] for i in range(len(X_train))] for j in range(M)]\n    \n    # fuzzy average of the fuzzy train outputs. c is the optimism parameter of the WABL.\n    f_ave=fuzAve(y_fuz_train,c)\n  \n    # F[i] is the fuzzy outputs of the model after i.th iteration\n    F[0]=[f_ave for _ in range(len(X_train))]\n\n    # gamma is the predicted fuzzy output (as a Fuzzy Number) according to the leaf \n    gamma=[[[0,0,0] for i in range(max_leaf)] for j in range(M)]\n    trees=[]\n\n    # boosting iterations\n    for m in range(1,M):    \n        rrr=[fuzSubtr(y_fuz_train[i],F[m-1][i],c) for i in range(len(y_fuz_train))]\n\n        # stump tree is constructed up to the defuzzified values of the FNs\n        r1=[defuz(rrr[i],c) for i in range(len(rrr))]\n\n        # constructing of the stump tree\n        tree = DecisionTreeRegressor(random_state=0,max_depth=tree_depth)\n        tree.fit(X_train_std, r1)\n        trees.append(tree)\n        \n        # h is the list of the indices of the leafs\n        h=tree.apply(X_train_std)   \n\n        # h1 is the list of the distinct leaf indices \n        h1=list(set(h))\n\n        for l in range(len(h1)):\n            leaf_l=[j for j in range(len(r1)) if h[j]==h1[l]] \n            ss=[rrr[j] for j in leaf_l]\n            ss1=np.reshape(ss,(-1,3))\n            gamma[m][l]=fuzAve(ss1,c) #for each leaf node\n            for k in leaf_l:\n                F[m][k]=fuzAdd(F[m-1][k],fuzMultBy(gamma[m][l],learning_rate,c),c) \n\n    # prediction\n    #print(\"----------- Train set R^2 and fuzRMSE -------------------\")\n    maxR2=-999999\n    minRMSE=-999999\n    maxM=-1\n\n    X1=X_train_std\n    fuzY=y_fuz_train\n\n    FM=F[0]  \n    ave=FM[0]  # average of train set\n\n    for m in range(1,M):\n        h=trees[m-1].apply(X1)\n        h1=list(set(h))\n        for l in range(len(h1)):\n            leaf_l=[j for j in range(len(X_train)) if h[j]==h1[l]] \n            for k in leaf_l:\n                FF=fuzAdd(FM[k],fuzMultBy(gamma[m][l],learning_rate,c),c)\n                FM[k]=FF    #for each xi of each leaf node \n\n        R2=fuzR2(fuzY,FM,ave,c)\n        RMSE=fuzRMSE(fuzY,FM,c)\n        if R2>maxR2:\n            maxR2=R2\n            minRMSE=RMSE\n            maxM=m\n        if m%10==0:\n            pass #print(\"%3d %10.4f %10.4f\"%(m,R2,RMSE))\n\n    print(\"-- TRAIN: the best R^2 and RMSE values with according iteration number --\")\n    print(\"maxM =\",maxM)\n    print(\"maxR2 = %10.4f\"%(maxR2))\n    print(\"minRMSE = %10.4f\"%(minRMSE))\n\n    #print(\"----------- Test set R^2 and fuzRMSE -------------------\")\n    maxR2=-999999\n    minRMSE=-999999\n    maxM=-1\n\n    X1=X_test_std\n    fuzY=y_fuz_test\n    F=[[[0,0,0] for _ in range(len(X_test))] for i in range(M)]\n    \n    # initial average of all test set with ave(train)\n    F[0]=[fuzAve(y_fuz_train,c) for _ in range(len(X_test))] \n    FM=F[0] \n    #ave=FM[0]  # average of the train set\n\n    for m in range(1,M):\n        h=trees[m-1].apply(X1)\n        h1=list(set(h))\n        for l in range(len(h1)):\n            leaf_l=[j for j in range(len(X1)) if h[j]==h1[l]] \n            for k in leaf_l:\n                FF=fuzAdd(FM[k],fuzMultBy(gamma[m][l],learning_rate,c),c)\n                FM[k]=FF    #for each xi of each leaf node \n\n        R2=fuzR2(fuzY,FM,ave,c)\n        RMSE=fuzRMSE(fuzY,FM,c)\n        if R2>maxR2:\n            maxR2=R2\n            minRMSE=RMSE\n            maxM=m\n        if m%10==0:\n            pass #print(\"%10.4f %10.4f\"%(R2,RMSE))   \n\n    print(\"-- TEST: the best R^2 and RMSE values with according iteration number --\")\n    print(\"maxM =\",maxM)\n    print(\"maxR2 = %10.4f\"%(maxR2))\n    print(\"minRMSE = %10.4f\"%(minRMSE))\n\n# in case of the WABL, calculations are made for all values of the optimism parameter - c\n# in case of the other defuzzification methods, this cycle shold be commented\nfor c in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n    c_cycle(c)\n    \n# in case of the other defuzzification methods, c does not matter. If should be fixed, for example, c = 0.5 \n# and the following two lines should be uncommented.\n#c=0.5\n#c_cycle(c)"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}