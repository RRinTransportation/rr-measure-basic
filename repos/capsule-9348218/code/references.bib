
@article{Rocca2021,
	title = {Putting {Psychology} to the {Test}: {Rethinking} {Model} {Evaluation} {Through} {Benchmarking} and {Prediction}},
	volume = {4},
	issn = {2515-2459, 2515-2467},
	shorttitle = {Putting {Psychology} to the {Test}},
	url = {http://journals.sagepub.com/doi/10.1177/25152459211026864},
	doi = {10.1177/25152459211026864},
	abstract = {Consensus on standards for evaluating models and theories is an integral part of every science. Nonetheless, in psychology, relatively little focus has been placed on defining reliable communal metrics to assess model performance. Evaluation practices are often idiosyncratic and are affected by a number of shortcomings (e.g., failure to assess models’ ability to generalize to unseen data) that make it difficult to discriminate between good and bad models. Drawing inspiration from fields such as machine learning and statistical genetics, we argue in favor of introducing common benchmarks as a means of overcoming the lack of reliable model evaluation criteria currently observed in psychology. We discuss a number of principles benchmarks should satisfy to achieve maximal utility, identify concrete steps the community could take to promote the development of such benchmarks, and address a number of potential pitfalls and concerns that may arise in the course of implementation. We argue that reaching consensus on common evaluation benchmarks will foster cumulative progress in psychology and encourage researchers to place heavier emphasis on the practical utility of scientific models.},
	language = {en},
	number = {3},
	urldate = {2023-02-07},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Rocca, Roberta and Yarkoni, Tal},
	month = jul,
	year = {2021},
	pages = {251524592110268},
}


@article{Fife2022,
	title = {Common, uncommon, and novel applications of random forest in psychological research},
	issn = {1554-3528},
	url = {https://link.springer.com/10.3758/s13428-022-01901-9},
	doi = {10.3758/s13428-022-01901-9},
	language = {en},
	urldate = {2023-02-07},
	journal = {Behavior Research Methods},
	author = {Fife, Dustin A. and D’Onofrio, Juliana},
	year = {2022},
}

@book{mlr3book2022,
 author = {Becker, Marc and Biecek, Przemyslaw and Binder, Martin and Bischl, Bernd and Burk, Lukas and Casalicchio, Giuseppe and Fischer, Sebastian and Foss, Natalie and Kotthoff, Lars and Lang, Michel and Pfisterer, Florian and Pulatov, Damir and Schneider, Lennart and Schratz, Patrick and Sonabend, Raphael and Wright, Marvin},
 title = {Flexible and Robust Machine Learning Using mlr3 in R},
 year = {2022},
 url = {https://mlr3book.mlr-org.com/}
}

@article{Kosinski2016,
	title = {Mining big data to extract patterns and predict real-life outcomes.},
	volume = {21},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000105},
	doi = {10.1037/met0000105},
	language = {en},
	number = {4},
	urldate = {2022-11-22},
	journal = {Psychological Methods},
	author = {Kosinski, Michal and Wang, Yilun and Lakkaraju, Himabindu and Leskovec, Jure},
	month = dec,
	year = {2016},
	pages = {493--506},
}

@book{Murphy2022,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: An introduction",
 publisher = "MIT Press",
 year = 2022,
 url = "probml.ai"
}

@misc{Grinsztajn2022,
  doi = {10.48550/ARXIV.2207.08815},
  url = {https://arxiv.org/abs/2207.08815},
  author = {Grinsztajn, Léo and Oyallon, Edouard and Varoquaux, Gaël},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Methodology (stat.ME), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Why do tree-based models still outperform deep learning on tabular data?},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Jacobucci2020,
author = {Ross Jacobucci and Kevin J. Grimm},
title ={Machine Learning and Psychological Research: The Unexplored Effect of Measurement},
journal = {Perspectives on Psychological Science},
volume = {15},
number = {3},
pages = {809-816},
year = {2020},
doi = {10.1177/1745691620902467},
    note ={PMID: 32348703},
URL = {https://doi.org/10.1177/1745691620902467
},
eprint = {https://doi.org/10.1177/1745691620902467
},
    abstract = { Machine learning (i.e., data mining, artificial intelligence, big data) has been increasingly applied in psychological science. Although some areas of research have benefited tremendously from a new set of statistical tools, most often in the use of biological or genetic variables, the hype has not been substantiated in more traditional areas of research. We argue that this phenomenon results from measurement errors that prevent machine-learning algorithms from accurately modeling nonlinear relationships, if indeed they exist. This shortcoming is showcased across a set of simulated examples, demonstrating that model selection between a machine-learning algorithm and regression depends on the measurement quality, regardless of sample size. We conclude with a set of recommendations and a discussion of ways to better integrate machine learning with statistics as traditionally practiced in psychological science. }
}


@InProceedings{Buolamwini2018,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}

@inproceedings{Mitchell2019,
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {ethical considerations, fairness evaluation, datasheets, documentation, ML model evaluation, model cards, disaggregated evaluation},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{Janitza2018,
	title = {A computationally fast variable importance test for random forests for high-dimensional data},
	volume = {12},
	issn = {1862-5355},
	url = {https://doi.org/10.1007/s11634-016-0276-4},
	doi = {10.1007/s11634-016-0276-4},
	abstract = {Random forests are a commonly used tool for classification and for ranking candidate predictors based on the so-called variable importance measures. These measures attribute scores to the variables reflecting their importance. A drawback of variable importance measures is that there is no natural cutoff that can be used to discriminate between important and non-important variables. Several approaches, for example approaches based on hypothesis testing, were developed for addressing this problem. The existing testing approaches require the repeated computation of random forests. While for low-dimensional settings those approaches might be computationally tractable, for high-dimensional settings typically including thousands of candidate predictors, computing time is enormous. In this article a computationally fast heuristic variable importance test is proposed that is appropriate for high-dimensional data where many variables do not carry any information. The testing approach is based on a modified version of the permutation variable importance, which is inspired by cross-validation procedures. The new approach is tested and compared to the approach of Altmann and colleagues using simulation studies, which are based on real data from high-dimensional binary classification settings. The new approach controls the type I error and has at least comparable power at a substantially smaller computation time in the studies. Thus, it might be used as a computationally fast alternative to existing procedures for high-dimensional data settings where many variables do not carry any information. The new approach is implemented in the R package vita.},
	number = {4},
	journal = {Advances in Data Analysis and Classification},
	author = {Janitza, Silke and Celik, Ender and Boulesteix, Anne-Laure},
	month = dec,
	year = {2018},
	pages = {885--915},
}


@article{Fisher2019,
  author  = {Aaron Fisher and Cynthia Rudin and Francesca Dominici},
  title   = {All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {177},
  pages   = {1--81},
  url     = {http://jmlr.org/papers/v20/18-760.html}
}

@article{Plecko2022,
  title={Causal Fairness Analysis},
  author={Plecko, Drago and Bareinboim, Elias},
  journal={arXiv preprint arXiv:2207.11385},
  year={2022}
}

@article{Deffner2022,
author = {Dominik Deffner and Julia M. Rohrer and Richard McElreath},
title ={A Causal Framework for Cross-Cultural Generalizability},
journal = {Advances in Methods and Practices in Psychological Science},
volume = {5},
number = {3},
pages = {},
year = {2022},
doi = {10.1177/25152459221106366},
URL = { 
        https://doi.org/10.1177/25152459221106366
},
eprint = { 
        https://doi.org/10.1177/25152459221106366
},
    abstract = { Behavioral researchers increasingly recognize the need for more diverse samples that capture the breadth of human experience. Current attempts to establish generalizability across populations focus on threats to validity, constraints on generalization, and the accumulation of large, cross-cultural data sets. But for continued progress, we also require a framework that lets us determine which inferences can be drawn and how to make informative cross-cultural comparisons. We describe a generative causal-modeling framework and outline simple graphical criteria to derive analytic strategies and implied generalizations. Using both simulated and real data, we demonstrate how to project and compare estimates across populations and further show how to formally represent measurement equivalence or inequivalence across societies. We conclude with a discussion of how a formal framework for generalizability can assist researchers in designing more informative cross-cultural studies and thus provides a more solid foundation for cumulative and generalizable behavioral research. }
}

@article{Wysocki2022,
author = {Anna C. Wysocki and Katherine M. Lawson and Mijke Rhemtulla},
title ={Statistical Control Requires Causal Justification},
journal = {Advances in Methods and Practices in Psychological Science},
volume = {5},
number = {2},
pages = {},
year = {2022},
doi = {10.1177/25152459221095823},
URL = { 
        https://doi.org/10.1177/25152459221095823
},
eprint = { 
        https://doi.org/10.1177/25152459221095823
}
,
    abstract = { It is common practice in correlational or quasiexperimental studies to use statistical control to remove confounding effects from a regression coefficient. Controlling for relevant confounders can debias the estimated causal effect of a predictor on an outcome; that is, it can bring the estimated regression coefficient closer to the value of the true causal effect. But statistical control works only under ideal circumstances. When the selected control variables are inappropriate, controlling can result in estimates that are more biased than uncontrolled estimates. Despite the ubiquity of statistical control in published regression analyses and the consequences of controlling for inappropriate third variables, the selection of control variables is rarely explicitly justified in print. We argue that to carefully select appropriate control variables, researchers must propose and defend a causal structure that includes the outcome, predictors, and plausible confounders. We underscore the importance of causality when selecting control variables by demonstrating how regression coefficients are affected by controlling for appropriate and inappropriate variables. Finally, we provide practical recommendations for applied researchers who wish to use statistical control. }
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Wicherts2016,
	title = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}: {A} {Checklist} to {Avoid} p-{Hacking}},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Degrees of {Freedom} in {Planning}, {Running}, {Analyzing}, and {Reporting} {Psychological} {Studies}},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01832/full},
	doi = {10.3389/fpsyg.2016.01832},
	urldate = {2022-04-05},
	journal = {Frontiers in Psychology},
	author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and van Aert, Robbie C. M. and van Assen, Marcel A. L. M.},
	month = nov,
	year = {2016},
}

@article{Au2021,
   author = {Quay Au and Julia Herbinger and Clemens Stachl and Bernd Bischl and Giuseppe Casalicchio and Q Au and · J Herbinger and · B Bischl and · G Casalicchio and C Stachl},
   doi = {10.48550/arxiv.2104.11688},
   keywords = {Formal analysis and investigation: 1, 2, 4,Funding acquisition: 3, 5,Grouped Feature Importance · Combined Features Effects · Dimension Reduction · Interpretable Machine Learning · CRediT taxonomy: Conceptualization: 1, 2, 3, 4,Investigation: 1, 2,Methodology: 1, 2, 4,Software: 1, 2,Supervision: 3, 4,Validation: 1, 2, 4,Visualization: 1, 2,Writing-original draft preparation: 1, 2,Writing-review and editing: 3, 4, 5},
   month = {4},
   title = {Grouped Feature Importance and Combined Features Effect Plot},
   url = {https://arxiv.org/abs/2104.11688v1},
   year = {2021},
}


@article{Schuwerk2019,
   author = {Tobias Schuwerk and Larissa J. Kaltefleiter and Jiew Quay Au and Axel Hoesl and Clemens Stachl},
   doi = {10.1007/s10803-019-04134-6},
   issn = {15733432},
   issue = {10},
   journal = {Journal of Autism and Developmental Disorders},
   keywords = {Autism,Experience sampling method,Mentalizing,Mobile sensing,Theory of mind},
   month = {7},
   pages = {4193-4208},
   pmid = {31273579},
   publisher = {Springer US},
   title = {Enter the Wild: Autistic Traits and Their Relationship to Mentalizing and Social Interaction in Everyday Life},
   volume = {49},
   url = {http://link.springer.com/10.1007/s10803-019-04134-6},
   year = {2019},
}


@article{Sust2022,
   author = {Larissa Sust and Gayatri Kudchadker and Ramona Schoedel and Tobias Schuwerk and Markus Bühner and Clemens Stachl},
   year = {2022},
   doi = {10.31234/OSF.IO/XNV2Q},
   keywords = {Computational Modeling,Individual Differences,Music,Quantitative Methods,Social and Behavioral Sciences,Social and Personality Psychology,audio features,behavior,machine learning,mobile,music preferences,natural language processing,personality,sensing,smarpthones,song lyrics},
   publisher = {PsyArXiv},
   title = {Personality Computing With Naturalistic Music Listening Behavior},
   url = {https://psyarxiv.com/xnv2q/},
}


@article{Harari2019,
   author = {Gabriella M. Harari and Sandrine R. Müller and Clemens Stachl and Rui Wang and Weichen Wang and Markus Bühner and Peter J. Rentfrow and Andrew T. Campbell and Samuel D. Gosling},
   doi = {10.1037/pspp0000245},
   issn = {00223514},
   journal = {Journal of Personality and Social Psychology},
   keywords = {Big Five personality traits,Mobile sensing,Naturalistic observation,Smartphones,Social behavior},
   month = {5},
   pmid = {31107054},
   title = {Sensing Sociability: Individual Differences in Young Adults' Conversation, Calling, Texting, and App Use Behaviors in Daily Life},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/31107054 http://doi.apa.org/getdoi.cfm?doi=10.1037/pspp0000245},
   year = {2019},
}



@article{Schoedel2020,
   author = {Ramona Schoedel and Florian Pargent and Quay Au and Sarah Theres Völkel and Tobias Schuwerk and Markus Bühner and Clemens Stachl},
   doi = {10.1002/per.2258},
   editor = {John Rauthmann},
   issn = {10990984},
   issue = {5},
   journal = {European Journal of Personality},
   keywords = {chronotype,day–night behaviour patterns,diurnal activity,personality,smartphone sensing data},
   month = {5},
   pages = {733-752},
   publisher = {John Wiley & Sons, Ltd},
   title = {To Challenge the Morning Lark and the Night Owl: Using Smartphone Sensing Data to Investigate Day–Night Behaviour Patterns},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/per.2258},
   year = {2020},
}


@article{Masters1982,
	title = {A rasch model for partial credit scoring},
	volume = {47},
	issn = {0033-3123},
	doi = {10.1007/BF02296272},
	number = {2},
	journal = {Psychometrika},
	author = {Masters, Geoff N.},
	year = {1982},
	note = {Publisher: Springer-Verlag},
	pages = {149--174},
}


@article{Shmueli2010,
	title = {To {Explain} or to {Predict}?},
	volume = {25},
	url = {https://doi.org/10.1214/10-STS330},
	doi = {10.1214/10-STS330},
	number = {3},
	journal = {Statistical Science},
	author = {Shmueli, Galit},
	year = {2010},
	note = {Publisher: The Institute of Mathematical Statistics},
	keywords = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
	pages = {289--310},
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{scikitlearn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@Manual{tidymodels,
    title = {Tidymodels: a collection of packages for modeling and
      machine learning using tidyverse principles.},
    author = {Max Kuhn and Hadley Wickham},
    url = {https://www.tidymodels.org},
    year = {2020},
}

@book{Pearl2009,
	address = {Cambridge, U.K. ; New York},
	title = {Causality: models, reasoning, and inference},
	isbn = {978-0-521-89560-6},
	shorttitle = {Causality},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2009},
	keywords = {Causation, Probabilities},
}

@article{Rohrer2018,
	title = {Thinking {Clearly} {About} {Correlations} and {Causation}: {Graphical} {Causal} {Models} for {Observational} {Data}},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	shorttitle = {Thinking {Clearly} {About} {Correlations} and {Causation}},
	url = {http://journals.sagepub.com/doi/10.1177/2515245917745629},
	doi = {10.1177/2515245917745629},
	abstract = {Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables—colliders and mediators—should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors.},
	language = {en},
	number = {1},
	urldate = {2022-03-18},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Rohrer, Julia M.},
	year = {2018},
	pages = {27--42},
}


@article{Roberts2017,
	title = {Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure},
	volume = {40},
	issn = {09067590},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881},
	doi = {10.1111/ecog.02881},
	language = {en},
	number = {8},
	urldate = {2022-03-18},
	journal = {Ecography},
	author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, José J. and Schröder, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
	year = {2017},
	pages = {913--929},
}


@article{Varma2006,
	title = {Bias in error estimation when using cross-validation for model selection},
	volume = {7},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-7-91},
	doi = {10.1186/1471-2105-7-91},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Varma, Sudhir and Simon, Richard},
	month = feb,
	year = {2006},
	pages = {91},
}


@article{Monsted2018,
	title = {Phone-based metric as a predictor for basic personality traits},
	volume = {74},
	issn = {00926566},
	doi = {10.1016/j.jrp.2017.12.004},
	language = {en},
	urldate = {2022-03-18},
	journal = {Journal of Research in Personality},
	author = {Mønsted, Bjarke and Mollgaard, Anders and Mathiesen, Joachim},
	month = jun,
	year = {2018},
	pages = {16--22},
}


@article{Wolff2019,
  title={PROBAST: a tool to assess the risk of bias and applicability of prediction model studies},
  author={Wolff, Robert F and Moons, Karel GM and Riley, Richard D and Whiting, Penny F and Westwood, Marie and Collins, Gary S and Reitsma, Johannes B and Kleijnen, Jos and Mallett, Sue},
  journal={Annals of internal medicine},
  volume={170},
  number={1},
  pages={51--58},
  year={2019},
  publisher={American College of Physicians},
  doi={https://doi.org/10.7326/M18-1376}
}

@article{Moons2015,
  title={Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis (TRIPOD): explanation and elaboration},
  author={Moons, Karel GM and Altman, Douglas G and Reitsma, Johannes B and Ioannidis, John PA and Macaskill, Petra and Steyerberg, Ewout W and Vickers, Andrew J and Ransohoff, David F and Collins, Gary S},
  journal={Annals of internal medicine},
  volume={162},
  number={1},
  pages={W1--W73},
  year={2015},
  publisher={American College of Physicians},
  doi={https://doi.org/10.7326/M14-0698}
}


@incollection{Molnar2020,
	address = {Cham},
	title = {Interpretable {Machine} {Learning} – {A} {Brief} {History}, {State}-of-the-{Art} and {Challenges}},
	volume = {1323},
	isbn = {978-3-030-65964-6 978-3-030-65965-3},
	url = {https://link.springer.com/10.1007/978-3-030-65965-3_28},
	language = {en},
	urldate = {2022-03-17},
	booktitle = {{ECML} {PKDD} 2020 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
	editor = {Koprinska, Irena and Kamp, Michael and Appice, Annalisa and Loglisci, Corrado and Antonie, Luiza and Zimmermann, Albrecht and Guidotti, Riccardo and Özgöbek, Özlem and Ribeiro, Rita P. and Gavaldà, Ricard and Gama, João and Adilova, Linara and Krishnamurthy, Yamuna and Ferreira, Pedro M. and Malerba, Donato and Medeiros, Ibéria and Ceci, Michelangelo and Manco, Giuseppe and Masciari, Elio and Ras, Zbigniew W. and Christen, Peter and Ntoutsi, Eirini and Schubert, Erich and Zimek, Arthur and Monreale, Anna and Biecek, Przemyslaw and Rinzivillo, Salvatore and Kille, Benjamin and Lommatzsch, Andreas and Gulla, Jon Atle},
	year = {2020},
	doi = {10.1007/978-3-030-65965-3_28},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {417--431},
}

@article{Rudin2022,
	title = {Interpretable machine learning: {Fundamental} principles and 10 grand challenges},
	volume = {16},
	issn = {1935-7516},
	shorttitle = {Interpretable machine learning},
	url = {https://projecteuclid.org/journals/statistics-surveys/volume-16/issue-none/Interpretable-machine-learning-Fundamental-principles-and-10-grand-challenges/10.1214/21-SS133.full},
	doi = {10.1214/21-SS133},
	number = {none},
	urldate = {2022-03-17},
	journal = {Statistics Surveys},
	author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
	month = jan,
	year = {2022},
}

@article{Jacobucci2016,
	title = {Regularized {Structural} {Equation} {Modeling}},
	volume = {23},
	issn = {1070-5511, 1532-8007},
	url = {http://www.tandfonline.com/doi/full/10.1080/10705511.2016.1154793},
	doi = {10.1080/10705511.2016.1154793},
	language = {en},
	number = {4},
	urldate = {2022-03-17},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Jacobucci, Ross and Grimm, Kevin J. and McArdle, John J.},
	month = jul,
	year = {2016},
	pages = {555--566},
	file = {Akzeptierte Version:/Users/florianpargent/Zotero/storage/46SZQST5/Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf:application/pdf},
}

@article{Brandmaier2013,
	title = {Structural equation model trees.},
	volume = {18},
	doi = {https://doi.org/10.1037/a0030001},
	number = {1},
	journal = {Psychological methods},
	author = {Brandmaier, Andreas M and von Oertzen, Timo and McArdle, John J and Lindenberger, Ulman},
	year = {2013},
	note = {Publisher: American Psychological Association},
	pages = {71},
}

@article{Strobl2009,
	title = {An introduction to recursive partitioning: {Rationale}, application, and characteristics of classification and regression trees, bagging, and random forests.},
	volume = {14},
	issn = {1939-1463(Electronic),1082-989X(Print)},
	doi = {10.1037/a0016973},
	number = {4},
	journal = {Psychological Methods},
	author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
	year = {2009},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {*Prediction, *Statistical Analysis, *Statistical Regression, Statistical Variables},
	pages = {323--348},
}

@misc{Henninger2022,
 title={Interpretable machine learning for psychological research: Opportunities and pitfalls},
 url={psyarxiv.com/xe83y},
 DOI={10.31234/osf.io/xe83y},
 publisher={PsyArXiv},
 author={Henninger, Mirka and Debelak, Rudolf and Rothacher, Yannick and Strobl, Carolin},
 year={2022},
 month={Feb}
}

@article{Jobin2019,
   author = {Anna Jobin and Marcello Ienca and Effy Vayena},
   doi = {10.1038/s42256-019-0088-2},
   issn = {2522-5839},
   issue = {9},
   journal = {Nature Machine Intelligence 2019 1:9},
   keywords = {Ethics,Information systems and information technology,Information technology,Science,technology and society},
   month = {9},
   pages = {389-399},
   publisher = {Nature Publishing Group},
   title = {The global landscape of AI ethics guidelines},
   volume = {1},
   url = {https://www.nature.com/articles/s42256-019-0088-2},
   year = {2019},
}



@article{Yarkoni2022,
    author = {Tal Yarkoni},
   doi = {10.1017/S0140525X20001685},
   issn = {0140-525X},
   journal = {Behavioral and Brain Sciences},
   keywords = {generalization,inference,philosophy of science,psychology,random effects,statistics},
   month = {12},
   pages = {e1},
   publisher = {Cambridge University Press},
   title = {The generalizability crisis},
   volume = {45},
   url = {https://www.cambridge.org/core/product/identifier/S0140525X20001685/type/journal_article},
   year = {2022},
}


@article{Shaw2022,
   author = {Heather Shaw and Paul J. Taylor and David A. Ellis and Stacey M. Conchie},
   doi = {10.1177/09567976211040491},
   issn = {0956-7976},
   journal = {Psychological Science},
   keywords = {behavioral consistency,digital footprint,intraindividual,open data,personality,preregistered},
   month = {2},
   pages = {095679762110404},
   publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
   title = {Behavioral Consistency in the Digital Age},
   url = {http://journals.sagepub.com/doi/10.1177/09567976211040491},
   year = {2022},
}


@Manual{rpartplot_2021,
    title = {rpart.plot: Plot 'rpart' Models: An Enhanced Version of 'plot.rpart'},
    author = {Stephen Milborrow},
    year = {2021},
    note = {R package version 3.1.0},
    url = {https://CRAN.R-project.org/package=rpart.plot},
  }
  
@Manual{ameshousing_2020,
    title = {AmesHousing: The Ames Iowa Housing Data},
    author = {Max Kuhn},
    year = {2020},
    note = {R package version 0.0.4},
    url = {https://CRAN.R-project.org/package=AmesHousing},
  }

@article{bommert_2020,
title = {Benchmark for filter methods for feature selection in high-dimensional classification data},
journal = {Computational Statistics & Data Analysis},
volume = {143},
pages = {106839},
year = {2020},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2019.106839},
url = {https://www.sciencedirect.com/science/article/pii/S016794731930194X},
author = {Andrea Bommert and Xudong Sun and Bernd Bischl and Jörg Rahnenführer and Michel Lang},
keywords = {Feature selection, Filter methods, High-dimensional data, Benchmark},
abstract = {Feature selection is one of the most fundamental problems in machine learning and has drawn increasing attention due to high-dimensional data sets emerging from different fields like bioinformatics. For feature selection, filter methods play an important role, since they can be combined with any machine learning model and can heavily reduce run time of machine learning algorithms. The aim of the analyses is to review how different filter methods work, to compare their performance with respect to both run time and predictive accuracy, and to provide guidance for applications. Based on 16 high-dimensional classification data sets, 22 filter methods are analyzed with respect to run time and accuracy when combined with a classification method. It is concluded that there is no group of filter methods that always outperforms all other methods, but recommendations on filter methods that perform well on many of the data sets are made. Also, groups of filters that are similar with respect to the order in which they rank the features are found. For the analyses, the R machine learning package mlr is used. It provides a uniform programming API and therefore is a convenient tool to conduct feature selection using filter methods.}
}

@article{pargent_2021,
  title={Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features},
  author={Pargent, Florian and Pfisterer, Florian and Thomas, Janek and Bischl, Bernd},
  journal={arXiv preprint arXiv:2104.00629},
  year={2021}
}

@article{wright_2019,
  title={Splitting on categorical predictors in random forests},
  author={Wright, Marvin N. and König, Inke R.},
  journal={PeerJ},
  volume={7},
  year={2019},
  publisher={PeerJ, Inc},
  doi={10.7717/peerj.6339}
}

@inproceedings{chen2016,
	title = {{XGBoost}: {A} scalable tree boosting system},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	pages = {785--794}
}

@misc{sterner2021,
 title={Everything has its Price: Foundations of Cost-Sensitive Learning and its Application in Psychology},
 url={psyarxiv.com/7asgz},
 DOI={10.31234/osf.io/7asgz},
 publisher={PsyArXiv},
 author={Sterner, Philipp and Goretzko, David and Pargent, Florian},
 year={2021},
 month={Dec}
}

@article{probst2019,
author = {Probst, Philipp and Wright, Marvin N. and Boulesteix, Anne-Laure},
title = {Hyperparameters and tuning strategies for random forest},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
year = {2019},
volume = {0},
number = {0},
pages = {e1301},
keywords = {ensemble, literature review, out-of-bag, performance evaluation, ranger, sequential model-based optimization, tuning parameter},
doi = {10.1002/widm.1301},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1301},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1301},
abstract = {The random forest (RF) algorithm has several hyperparameters that have to be set by the user, for example, the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain, and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures. It is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a presenting brief overview of tuning strategies, we demonstrate the application of one of the most established tuning strategies, model-based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters. This article is categorized under: Algorithmic Development > Biological Data Mining Algorithmic Development > Statistics Algorithmic Development > Hierarchies and Trees Technologies > Machine Learning}
}

@InProceedings{bernard2009,
author="Bernard, Simon
and Heutte, Laurent
and Adam, S{\'e}bastien",
editor="Benediktsson, J{\'o}n Atli
and Kittler, Josef
and Roli, Fabio",
title="Influence of Hyperparameters on Random Forest Accuracy",
booktitle="Multiple Classifier Systems",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="171--180",
abstract="In this paper we present our work on the Random Forest (RF) family of classification methods. Our goal is to go one step further in the understanding of RF mechanisms by studying the parametrization of the reference algorithm Forest-RI. In this algorithm, a randomization principle is used during the tree induction process, that randomly selects K features at each node, among which the best split is chosen. The strength of randomization in the tree induction is thus led by the hyperparameter K which plays an important role for building accurate RF classifiers. We have decided to focus our experimental study on this hyperparameter and on its influence on classification accuracy. For that purpose, we have evaluated the Forest-RI algorithm on several machine learning problems and with different settings of K in order to understand the way it acts on RF performance. We show that default values of K traditionally used in the literature are globally near-optimal, except for some cases for which they are all significatively sub-optimal. Thus additional experiments have been led on those datasets, that highlight the crucial role played by feature relevancy in finding the optimal setting of K.",
isbn="978-3-642-02326-2"
}


@article{breiman_bagging_1996,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer},
  doi={10.1007/BF00058655}
}

@Article{friedman2010,
    title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
    author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
    journal = {Journal of Statistical Software},
    year = {2010},
    volume = {33},
    number = {1},
    pages = {1--22},
    url = {https://www.jstatsoft.org/v33/i01/},
  }

@article{ferri2009,
title = {An experimental comparison of performance measures for classification},
journal = {Pattern Recognition Letters},
volume = {30},
number = {1},
pages = {27-38},
year = {2009},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2008.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167865508002687},
author = {C. Ferri and J. Hernández-Orallo and R. Modroiu},
keywords = {Classification, Performance measures, Ranking, Calibration},
abstract = {Performance metrics in classification are fundamental in assessing the quality of learning methods and learned models. However, many different measures have been defined in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In this work, we analyse experimentally the behaviour of 18 different performance metrics in several scenarios, identifying clusters and relationships between measures. We also perform a sensitivity analysis for all of them in terms of several traits: class threshold choice, separability/ranking quality, calibration performance and sensitivity to changes in prior class distribution. From the definitions and experiments, we make a comprehensive analysis of the relationships between metrics, and a taxonomy and arrangement of them according to the previous traits. This can be useful for choosing the most adequate measure (or set of measures) for a specific application. Additionally, the study also highlights some niches in which new measures might be defined and also shows that some supposedly innovative measures make the same choices (or almost) as existing ones. Finally, this work can also be used as a reference for comparing experimental results in pattern recognition and machine learning literature, when using different measures.}
}

@article{nembrini2018,
    author = {Nembrini, Stefano and König, Inke R and Wright, Marvin N},
    title = "{The revival of the Gini importance?}",
    journal = {Bioinformatics},
    volume = {34},
    number = {21},
    pages = {3711-3718},
    year = {2018},
    month = {05},
    abstract = "{Random forests are fast, flexible and represent a robust approach to analyze high dimensional data. A key advantage over alternative machine learning algorithms are variable importance measures, which can be used to identify relevant features or perform variable selection. Measures based on the impurity reduction of splits, such as the Gini importance, are popular because they are simple and fast to compute. However, they are biased in favor of variables with many possible split points and high minor allele frequency.We set up a fast approach to debias impurity-based variable importance measures for classification, regression and survival forests. We show that it creates a variable importance measure which is unbiased with regard to the number of categories and minor allele frequency and almost as fast as the standard impurity importance. As a result, it is now possible to compute reliable importance estimates without the extra computing cost of permutations. Further, we combine the importance measure with a fast testing procedure, producing p-values for variable importance with almost no computational overhead to the creation of the random forest. Applications to gene expression and genome-wide association data show that the proposed method is powerful and computationally efficient.The procedure is included in the ranger package, available at https://cran.r-project.org/package=ranger and https://github.com/imbs-hl/ranger.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bty373},
    url = {https://doi.org/10.1093/bioinformatics/bty373},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/34/21/3711/26146978/bty373.pdf},
}

@article{debeer2020,
  author = {D. Debeer and C. Strobl},
  title = {Conditional Permutation Importance Revisited},
  journal = {BMC Bioinformatics},
  year = {2020},
  volume = {21},
  number = {1},
  pages = {307},
  url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03622-2},
  doi = {10.1186/s12859-020-03622-2}
}


@book{barocas2019,
  title = {Fairness and Machine Learning},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}

@article{apley2020,
author = {Apley, Daniel W. and Zhu, Jingyu},
title = {Visualizing the effects of predictor variables in black box supervised learning models},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {82},
number = {4},
pages = {1059-1086},
keywords = {Functional analysis of variance, Marginal plots, Partial dependence plots, Supervised learning, Visualization},
doi = {https://doi.org/10.1111/rssb.12377},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12377},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12377},
abstract = {Summary In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
year = {2020}
}

@article{friedman2001,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}

@article{goldstein2015,
  title={Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation},
  author={Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  journal={journal of Computational and Graphical Statistics},
  volume={24},
  number={1},
  pages={44--65},
  year={2015},
  publisher={Taylor \& Francis},
  doi={10.1080/10618600.2014.907095}
}

@book{molnar2019, 
  title = {Interpretable Machine Learning}, 
  author = {Christoph Molnar}, 
  year = {2019}, 
  subtitle = {A Guide for Making Black Box Models Explainable},
  url = {https://christophm.github.io/interpretable-ml-book/}
}

@Book{biecek2021,
  author = {Przemyslaw Biecek and Tomasz Burzykowski},
  title = {{Explanatory Model Analysis}},
  publisher = {Chapman and Hall/CRC, New York},
  year = {2021},
  isbn = {9780367135591},
  url = {https://pbiecek.github.io/ema/},
}

@article{bentsson2021,
  author = {Henrik Bengtsson},
  title = {{A Unifying Framework for Parallel and Distributed Processing
          in R using Futures}},
  year = {2021},
  journal = {{The R Journal}},
  doi = {10.32614/RJ-2021-048},
  url = {https://journal.r-project.org/archive/2021/RJ-2021-048/index.html}
}

@inproceedings{kohavi1995,
  title={A study of cross-validation and bootstrap for accuracy estimation and model selection},
  author={Kohavi, Ron and others},
  booktitle={Ijcai},
  volume={14},
  number={2},
  pages={1137--1145},
  year={1995},
  organization={Montreal, Canada}
}

@inproceedings{philipp2016,
  author = {Michel Philipp and Achim Zeileis and Carolin Strobl},
  editor = {In Ana Colubi and Angela Blanco and Cristian Gatu},
  title = {A Toolkit for Stability Assessment of Tree-Based Learners},
  booktitle = {Proceedings of COMPSTAT 2016 – 22nd International Conference on Computational Statistics},
  year = {2016},
  pages = {315--325}
}

@book{james2021,
	title = {An {Introduction} to {Statistical} {Learning}},
	subtitle = {with {Applications} in {R}},
	publisher = {New York: Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2021}
}

@article{binder2021,
  title={mlr3pipelines-flexible machine learning pipelines in r},
  author={Binder, Martin and Pfisterer, Florian and Lang, Michel and Schneider, Lennart and Kotthoff, Lars and Bischl, Bernd},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={184},
  pages={1--7},
  year={2021}
}

@article{Hilbert2021,
  title={Machine Learning for the Educational Sciences},
  author={Hilbert, Sven and Coors, Stefan and Kraus, Elisabeth Barbara and Bischl, Bernd and Frei, Mario and Lindl, Alfred and Wild, Johannes and Krauss, Stefan and Goretzko, David and Stachl, Clemens},
  year={2021},
  publisher={PsyArXiv}
}


 @misc{Koch2020,
  title={Predicting Age and Gender from Language, Emoji, and Emoticon Use in WhatsApp Instant Messages},
  url={osf.io/ymx26},
  publisher={OSF},
  author={Koch, Timo and Romero, Peter and Stachl, Clemens},
  year={2020},
  month={Nov}
}


@article{Stachl2020ML,
author = {Stachl, Clemens and Pargent, Florian and Hilbert, Sven and Harari, Gabriella M. and Schoedel, Ramona and Vaid, Sumer and Gosling, Samuel D. and B{\"{u}}hner, Markus},
doi = {10.1002/per.2257},
issn = {10990984},
journal = {European Journal of Personality},
keywords = {assessment,interpretability,machine learning,overfitting,personality},
month = {sep},
number = {5},
pages = {613--631},
publisher = {John Wiley and Sons Ltd},
title = {{Personality Research and Assessment in the Era of Machine Learning}},
url = {https://osf.io/j9yrw/,},
volume = {34},
year = {2020}
}


@misc{mlr3book,
  title = {mlr3 book},
  author = {Marc Becker and Martin Binder and Bernd Bischl and Michel Lang and Florian Pfisterer and Nicholas G. Reich and Jakob Richter and Patrick Schratz and Raphael Sonabend},
  url = {https://mlr3book.mlr-org.com},
  year = {2021},
  month = {03},
  day = {04},
}

@article{Zhao2021,
author = {Qingyuan Zhao and Trevor Hastie},
title = {Causal Interpretations of Black-Box Models},
journal = {Journal of Business \& Economic Statistics},
volume = {39},
number = {1},
pages = {272-281},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/07350015.2019.1624293},
URL = {https://doi.org/10.1080/07350015.2019.1624293},
eprint = {https://doi.org/10.1080/07350015.2019.1624293}
}

@Article{dalex,
    title = {DALEX: Explainers for Complex Predictive Models in R},
    author = {Przemyslaw Biecek},
    journal = {Journal of Machine Learning Research},
    year = {2018},
    volume = {19},
    pages = {1-5},
    number = {84},
    url = {https://jmlr.org/papers/v19/18-416.html},
}

@misc{Zweck2019,
 title={Prediction of Health-Related Outcomes and Turnover Intention with the Munich Employee Health Questionnaire (MEHQ)},
 url={psyarxiv.com/bgu6m},
 DOI={10.31234/osf.io/bgu6m},
 publisher={PsyArXiv},
 author={Zweck, Bettina M and Pargent, Florian and Bühner, Markus},
 year={2019},
 month={Jun}
}

@article{Stachl2020,
author = {Stachl, Clemens and Au, Quay and Schoedel, Ramona and Gosling, Samuel D and Harari, Gabriella M and Buschek, Daniel and V{\"{o}}lkel, Sarah Theres and Schuwerk, Tobias and Oldemeier, Michelle and Ullmann, Theresa and Hussmann, Heinrich and Bischl, Bernd and B{\"{u}}hner, Markus},
doi = {10.1073/pnas.1920484117},
file = {:Users/cstachl/Library/Application Support/Mendeley Desktop/Downloaded/Stachl et al. - 2020 - Predicting personality from patterns of behavior collected with smartphones(2).pdf:pdf;:Users/cstachl/Library/Application Support/Mendeley Desktop/Downloaded/Stachl et al. - 2020 - Predicting personality from patterns of behavior collected with smartphones(3).pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Behavior,Machine learning,Mobile sensing,Personality,Privacy},
month = {jul},
number = {30},
pages = {17680--17687},
pmid = {32665436},
publisher = {National Academy of Sciences},
title = {{Predicting personality from patterns of behavior collected with smartphones}},
url = {www.pnas.org/cgi/doi/10.1073/pnas.1920484117},
volume = {117},
year = {2020}
}

@article{delgado_do_2014,
  author  = {Manuel Fern\'{a}ndez-Delgado and Eva Cernadas and Sen\'{e}n Barro and Dinani Amorim},
  title   = {Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {3133-3181},
  url     = {http://jmlr.org/papers/v15/delgado14a.html}
}

@article{tibshirani_regression_1996,
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}

@article{stachl_personality_2017,
  author = {Stachl, Clemens and Hilbert, Sven and Au, Jiew-Quay and Buschek, Daniel and De Luca, Alexander and Bischl, Bernd and Hussmann, Heinrich and Bühner, Markus},
  title = {Personality Traits Predict Smartphone Usage},
  journal = {European Journal of Personality},
  year = {2017},
  volume = {31},
  number = {6},
  pages = {701-722},
  doi = {10.1002/per.2113},
  url = {https://doi.org/10.1002/per.2113},
}

@Article{wright_ranger_2017,
  title = {{ranger}: A Fast Implementation of Random Forests for High Dimensional Data in {C++} and {R}},
  author = {Marvin N Wright and Andreas Ziegler},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {77},
  number = {1},
  pages = {1--17},
  doi = {10.18637/jss.v077.i01},
}

@Article{strobl_bias_2007,
  author="Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten",
  title="Bias in random forest variable importance measures: Illustrations, sources and a solution",
  journal="BMC Bioinformatics",
  year="2007",
  month="Jan",
  day="25",
  volume="8",
  number="1",
  pages="25",
  doi="10.1186/1471-2105-8-25",
  url="https://doi.org/10.1186/1471-2105-8-25"
}

@Article{strobl_conditional_2008,
  author="Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim",
  title="Conditional variable importance for random forests",
  journal="BMC Bioinformatics",
  year="2008",
  month="Jul",
  day="11",
  volume="9",
  number="1",
  pages="307",
  doi="10.1186/1471-2105-9-307",
  url="https://doi.org/10.1186/1471-2105-9-307"
}

@article{molnar_iml_2018,
  title = {iml: An R package for Interpretable Machine Learning},
  author = {Molnar, Christoph and Casalicchio, Guiseppe and Bischl, Bernd},
  journaltitle = {Journal of Open Source Software},
  volume = {3},
  issue = {26},
  pages = {786},
  year = {2018},
  doi = {10.21105/joss.00786},
  url = {https://doi.org/10.21105/joss.00786}
}

@article{pargent_predictive_2018,
author = {Pargent, Florian and {Albert-Von Der G{\"{o}}nna}, Johannes},
doi = {10.1027/2151-2604/a000343},
issn = {21512604},
journal = {Zeitschrift fur Psychologie / Journal of Psychology},
keywords = {elastic net,machine learning,panel data,predictive modeling,random forest},
month = {oct},
number = {4},
pages = {246--258},
publisher = {Hogrefe Publishing},
title = {{Predictive Modeling with Psychological Panel Data}},
url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000343},
volume = {226},
year = {2018}
}


@Manual{kuhn_tidymodels_2018,
    title = {tidymodels: Easily Install and Load the 'Tidymodels' Packages},
    author = {Kuhn, Max and Wickham, Hadley},
    year = {2018},
    note = {R package version 0.0.1},
    url = {https://CRAN.R-project.org/package=tidymodels},
  }


@article{schoedel_digital_2018,
  title = {Digital Footprints of Sensation Seeking: A Traditional concept in the Big Data Era},
  author = {Schoedel, Ramona and Au, Quay and Völkel, Sarah Theres and Lehmann, 
  Florian and Becker, Daniela and Bühner, Markus and Bischl, Bernd and 
  Hussmann, Heinrich and Stachl, Clemens},
  year = {2018},
  url = {http://dx.doi.org/10.23668/psycharchives.846}
}

@article{youyou_computer-based_2015,
  title = {Computer-Based Personality Judgments Are More Accurate than Those Made by Humans},
  volume = {112},
  number = {4},
  journaltitle = {Proceedings of the National Academy of Sciences},
  author = {Youyou, Wu and Kosinski, Michal and Stillwell, David},
  date = {2015},
  pages = {1036--1040}
}

 @article{Kosinski2013, 
 title={Private traits and attributes are predictable from digital records of human behavior.}, 
 volume={110}, ISSN={1091-6490}, 
 DOI={10.1073/pnas.1218772110}, 
 number={15}, 
 journal={Proceedings of the National Academy of Sciences of the United States of America}, 
 author={Kosinski, Michal and Stillwell, David and Graepel, Thore}, 
 year={2013}, 
 month={Apr}, 
 pages={5802–5} 
}


@article{yarkoni_choosing_2017,
  author = {Tal Yarkoni and Jacob Westfall},
  title ={Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning},
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {6},
  pages = {1100-1122},
  year = {2017},
  doi = {10.1177/1745691617693393}
}


@book{hastie_statistical_2015,
  title = {Statistical Learning with Sparsity},
  publisher = {{CRC press}},
  author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
  date = {2015}
}

@article{breiman_random_2001,
  title = {Random Forests},
  volume = {45},
  number = {1},
  journaltitle = {Machine learning},
  author = {Breiman, Leo},
  date = {2001},
  pages = {5--32}
}

@book{hastie_elements_2009,
  title = {The Elements of Statistical Learning 2nd Edition},
  publisher = {{New York: Springer}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009}
}

@book{breiman_classification_1984,
  title = {Classification and Regression Trees},
  publisher = {{CRC press}},
  author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J. and Olshen, Richard A.},
  date = {1984}
}

@book{kuhn_applied_2013,
  title = {Applied Predictive Modeling},
  volume = {26},
  publisher = {{Springer}},
  author = {Kuhn, Max and Johnson, Kjell},
  date = {2013}
}

@article{breiman_statistical_2001,
  title = {Statistical Modeling: {{The}} Two Cultures (with Comments and a Rejoinder by the Author)},
  volume = {16},
  shorttitle = {Statistical Modeling},
  number = {3},
  journaltitle = {Statistical science},
  author = {Breiman, Leo and {others}},
  date = {2001},
  pages = {199--231}
}

@article{stachl_show_2015,
  title = {Show Me How You {{Drive}} and {{I}}’ll {{Tell}} You Who You Are {{Recognizing Gender Using Automotive Driving Parameters}}},
  volume = {3},
  journaltitle = {Procedia Manufacturing},
  series = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
  author = {Stachl, Clemens and Bühner, Markus},
  date = {2015},
  pages = {5587--5594}
}

@article{arendasy_manual_2011,
  title = {Manual {{Big}}-{{Five Structure Inventory BFSI}}},
  journaltitle = {Schuhfried Gmbh, Mödling},
  author = {Arendasy, M. and Sommer, M. and Feldhammer, M.},
  date = {2011}
}

@article{bischl_resampling_2012,
  title = {Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation},
  volume = {20},
  number = {2},
  journaltitle = {Evolutionary computation},
  author = {Bischl, Bernd and Mersmann, Olaf and Trautmann, Heike and Weihs, Claus},
  date = {2012},
  pages = {249--275}
}

@book{james_introduction_2013,
  title = {An Introduction to Statistical Learning},
  publisher = {{Springer}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2013}
}

@Article{mlr3,
    title = {{mlr3}: A modern object-oriented machine learning framework in {R}},
    author = {Michel Lang and Martin Binder and Jakob Richter and Patrick Schratz and Florian Pfisterer and Stefan Coors and Quay Au and Giuseppe Casalicchio and Lars Kotthoff and Bernd Bischl},
    journal = {Journal of Open Source Software},
    year = {2019},
    month = {dec},
    doi = {10.21105/joss.01903},
    url = {https://joss.theoj.org/papers/10.21105/joss.01903},
  }


@article{Eisenberg2019,
   author = {Ian W. Eisenberg and Patrick G. Bissett and A. Zeynep Enkavi and Jamie Li and David P. MacKinnon and Lisa A. Marsch and Russell A. Poldrack},
   doi = {10.1038/s41467-019-10301-1},
   issn = {2041-1723},
   issue = {1},
   journal = {Nature Communications},
   keywords = {Human behaviour},
   month = {12},
   pages = {2319},
   publisher = {Nature Publishing Group},
   title = {Uncovering the structure of self-regulation through data-driven ontology discovery},
   volume = {10},
   url = {http://www.nature.com/articles/s41467-019-10301-1},
   year = {2019},
}

